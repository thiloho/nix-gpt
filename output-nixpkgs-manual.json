[
  {
    "title": "Nixpkgs 23.11 manual | Nix & NixOS",
    "url": "https://nixos.org/manual/nixpkgs/unstable/",
    "html": "Table of Contents\n\nPreface\nUsing Nixpkgs\nPlatform Support\nGlobal configuration\nOverlays\nOverriding\nNixpkgs lib\nFunctions reference\nModule System\nStandard environment\nThe Standard Environment\nMeta-attributes\nMultiple-output packages\nCross-compilation\nPlatform Notes\nBuild helpers\nFetchers\nTrivial build helpers\nTesters\nSpecial build helpers\nImages\nHooks reference\nLanguages and frameworks\nPackages\nDevelopment of Nixpkgs\nOpening issues\nContributing to Nixpkgs\nQuick Start to Adding a Package\nCoding conventions\nSubmitting changes\nVulnerability Roundup\nReviewing contributions\nContributing to Nixpkgs documentation\nPreface \n\nTable of Contents\n\nOverview of Nixpkgs\n\nThe Nix Packages collection (Nixpkgs) is a set of thousands of packages for the Nix package manager, released under a permissive MIT license. Packages are available for several platforms, and can be used with the Nix package manager on most GNU/Linux distributions as well as NixOS.\n\nThis manual primarily describes how to write packages for the Nix Packages collection (Nixpkgs). Thus it’s mainly for packagers and developers who want to add packages to Nixpkgs. If you like to learn more about the Nix package manager and the Nix expression language, then you are kindly referred to the Nix manual. The NixOS distribution is documented in the NixOS manual.\n\nOverview of Nixpkgs \n\nNix expressions describe how to build packages from source and are collected in the nixpkgs repository. Also included in the collection are Nix expressions for NixOS modules. With these expressions the Nix package manager can build binary packages.\n\nPackages, including the Nix packages collection, are distributed through channels. The collection is distributed for users of Nix on non-NixOS distributions through the channel nixpkgs. Users of NixOS generally use one of the nixos-* channels, e.g. nixos-22.11, which includes all packages and modules for the stable NixOS 22.11. Stable NixOS releases are generally only given security updates. More up to date packages and modules are available via the nixos-unstable channel.\n\nBoth nixos-unstable and nixpkgs follow the master branch of the Nixpkgs repository, although both do lag the master branch by generally a couple of days. Updates to a channel are distributed as soon as all tests for that channel pass, e.g. this table shows the status of tests for the nixpkgs channel.\n\nThe tests are conducted by a cluster called Hydra, which also builds binary packages from the Nix expressions in Nixpkgs for x86_64-linux, i686-linux and x86_64-darwin. The binaries are made available via a binary cache.\n\nThe current Nix expressions of the channels are available in the nixpkgs repository in branches that correspond to the channel names (e.g. nixos-22.11-small).\n\nUsing Nixpkgs \n\nTable of Contents\n\nPlatform Support\nGlobal configuration\nOverlays\nOverriding\nPlatform Support \n\nPackages receive varying degrees of support, both in terms of maintainer attention and available computation resources for continuous integration (CI).\n\nBelow is the list of the best supported platforms:\n\nx86_64-linux: Highest level of support.\n\naarch64-linux: Well supported, with most packages building successfully in CI.\n\naarch64-darwin: Receives better support than x86_64-darwin.\n\nx86_64-darwin: Receives some support.\n\nThere are many other platforms with varying levels of support. The provisional platform list in Appendix A of RFC046, while not up to date, can be used as guidance.\n\nA more formal definition of the platform support tiers is provided in RFC046, but has not been fully implemented yet.\n\nGlobal configuration \n\nTable of Contents\n\nInstalling broken packages\nInstalling packages on unsupported systems\nInstalling unfree packages\nInstalling insecure packages\nModify packages via packageOverrides\nconfig Options Reference\nDeclarative Package Management\n\nNix comes with certain defaults about what packages can and cannot be installed, based on a package’s metadata. By default, Nix will prevent installation if any of the following criteria are true:\n\nThe package is thought to be broken, and has had its meta.broken set to true.\n\nThe package isn’t intended to run on the given system, as none of its meta.platforms match the given system.\n\nThe package’s meta.license is set to a license which is considered to be unfree.\n\nThe package has known security vulnerabilities but has not or can not be updated for some reason, and a list of issues has been entered in to the package’s meta.knownVulnerabilities.\n\nNote that all this is checked during evaluation already, and the check includes any package that is evaluated. In particular, all build-time dependencies are checked. nix-env -qa will (attempt to) hide any packages that would be refused.\n\nEach of these criteria can be altered in the nixpkgs configuration.\n\nThe nixpkgs configuration for a NixOS system is set in the configuration.nix, as in the following example:\n\n{\n  nixpkgs.config = {\n    allowUnfree = true;\n  };\n}\n\n\nHowever, this does not allow unfree software for individual users. Their configurations are managed separately.\n\nA user’s nixpkgs configuration is stored in a user-specific configuration file located at ~/.config/nixpkgs/config.nix. For example:\n\n{\n  allowUnfree = true;\n}\n\n\nNote that we are not able to test or build unfree software on Hydra due to policy. Most unfree licenses prohibit us from either executing or distributing the software.\n\nInstalling broken packages \n\nThere are two ways to try compiling a package which has been marked as broken.\n\nFor allowing the build of a broken package once, you can use an environment variable for a single invocation of the nix tools:\n\n$ export NIXPKGS_ALLOW_BROKEN=1\n\n\nFor permanently allowing broken packages to be built, you may add allowBroken = true; to your user’s configuration file, like this:\n\n{\n  allowBroken = true;\n}\n\nInstalling packages on unsupported systems \n\nThere are also two ways to try compiling a package which has been marked as unsupported for the given system.\n\nFor allowing the build of an unsupported package once, you can use an environment variable for a single invocation of the nix tools:\n\n$ export NIXPKGS_ALLOW_UNSUPPORTED_SYSTEM=1\n\n\nFor permanently allowing unsupported packages to be built, you may add allowUnsupportedSystem = true; to your user’s configuration file, like this:\n\n{\n  allowUnsupportedSystem = true;\n}\n\n\nThe difference between a package being unsupported on some system and being broken is admittedly a bit fuzzy. If a program ought to work on a certain platform, but doesn’t, the platform should be included in meta.platforms, but marked as broken with e.g. meta.broken = !hostPlatform.isWindows. Of course, this begs the question of what “ought” means exactly. That is left to the package maintainer.\n\nInstalling unfree packages \n\nAll users of Nixpkgs are free software users, and many users (and developers) of Nixpkgs want to limit and tightly control their exposure to unfree software. At the same time, many users need (or want) to run some specific pieces of proprietary software. Nixpkgs includes some expressions for unfree software packages. By default unfree software cannot be installed and doesn’t show up in searches.\n\nThere are several ways to tweak how Nix handles a package which has been marked as unfree.\n\nTo temporarily allow all unfree packages, you can use an environment variable for a single invocation of the nix tools:\n\n$ export NIXPKGS_ALLOW_UNFREE=1\n\n\nIt is possible to permanently allow individual unfree packages, while still blocking unfree packages by default using the allowUnfreePredicate configuration option in the user configuration file.\n\nThis option is a function which accepts a package as a parameter, and returns a boolean. The following example configuration accepts a package and always returns false:\n\n{\n  allowUnfreePredicate = (pkg: false);\n}\n\n\nFor a more useful example, try the following. This configuration only allows unfree packages named roon-server and visual studio code:\n\n{\n  allowUnfreePredicate = pkg: builtins.elem (lib.getName pkg) [\n    \"roon-server\"\n    \"vscode\"\n  ];\n}\n\n\nIt is also possible to allow and block licenses that are specifically acceptable or not acceptable, using allowlistedLicenses and blocklistedLicenses, respectively.\n\nThe following example configuration allowlists the licenses amd and wtfpl:\n\n{\n  allowlistedLicenses = with lib.licenses; [ amd wtfpl ];\n}\n\n\nThe following example configuration blocklists the gpl3Only and agpl3Only licenses:\n\n{\n  blocklistedLicenses = with lib.licenses; [ agpl3Only gpl3Only ];\n}\n\n\nNote that allowlistedLicenses only applies to unfree licenses unless allowUnfree is enabled. It is not a generic allowlist for all types of licenses. blocklistedLicenses applies to all licenses.\n\nA complete list of licenses can be found in the file lib/licenses.nix of the nixpkgs tree.\n\nInstalling insecure packages \n\nThere are several ways to tweak how Nix handles a package which has been marked as insecure.\n\nTo temporarily allow all insecure packages, you can use an environment variable for a single invocation of the nix tools:\n\n$ export NIXPKGS_ALLOW_INSECURE=1\n\n\nIt is possible to permanently allow individual insecure packages, while still blocking other insecure packages by default using the permittedInsecurePackages configuration option in the user configuration file.\n\nThe following example configuration permits the installation of the hypothetically insecure package hello, version 1.2.3:\n\n{\n  permittedInsecurePackages = [\n    \"hello-1.2.3\"\n  ];\n}\n\n\nIt is also possible to create a custom policy around which insecure packages to allow and deny, by overriding the allowInsecurePredicate configuration option.\n\nThe allowInsecurePredicate option is a function which accepts a package and returns a boolean, much like allowUnfreePredicate.\n\nThe following configuration example only allows insecure packages with very short names:\n\n{\n  allowInsecurePredicate = pkg: builtins.stringLength (lib.getName pkg) <= 5;\n}\n\n\nNote that permittedInsecurePackages is only checked if allowInsecurePredicate is not specified.\n\nModify packages via packageOverrides \n\nYou can define a function called packageOverrides in your local ~/.config/nixpkgs/config.nix to override Nix packages. It must be a function that takes pkgs as an argument and returns a modified set of packages.\n\n{\n  packageOverrides = pkgs: rec {\n    foo = pkgs.foo.override { ... };\n  };\n}\n\nconfig Options Reference \n\nThe following attributes can be passed in config.\n\nenableParallelBuildingByDefault\n\nWhether to set enableParallelBuilding to true by default while building nixpkgs packages. Changing the default may cause a mass rebuild.\n\nType: boolean\n\nDefault: false\n\nDeclared by:\n\npkgs/top-level/config.nix\nallowAliases\n\nWhether to expose old attribute names for compatibility.\n\nThe recommended setting is to enable this, as it improves backward compatibity, easing updates.\n\nThe only reason to disable aliases is for continuous integration purposes. For instance, Nixpkgs should not depend on aliases in its internal code. Projects that aren’t Nixpkgs should be cautious of instantly removing all usages of aliases, as migrating too soon can break compatibility with the stable Nixpkgs releases.\n\nType: boolean\n\nDefault: true\n\nDeclared by:\n\npkgs/top-level/config.nix\nallowBroken\n\nWhether to allow broken packages.\n\nSee Installing broken packages in the NixOS manual.\n\nType: boolean\n\nDefault: false || builtins.getEnv \"NIXPKGS_ALLOW_BROKEN\" == \"1\"\n\nDeclared by:\n\npkgs/top-level/config.nix\nallowUnfree\n\nWhether to allow unfree packages.\n\nSee Installing unfree packages in the NixOS manual.\n\nType: boolean\n\nDefault: false || builtins.getEnv \"NIXPKGS_ALLOW_UNFREE\" == \"1\"\n\nDeclared by:\n\npkgs/top-level/config.nix\nallowUnsupportedSystem\n\nWhether to allow unsupported packages.\n\nSee Installing packages on unsupported systems in the NixOS manual.\n\nType: boolean\n\nDefault: false || builtins.getEnv \"NIXPKGS_ALLOW_UNSUPPORTED_SYSTEM\" == \"1\"\n\nDeclared by:\n\npkgs/top-level/config.nix\ncheckMeta\n\nWhether to check that the meta attribute of derivations are correct during evaluation time.\n\nType: boolean\n\nDefault: false\n\nDeclared by:\n\npkgs/top-level/config.nix\nconfigurePlatformsByDefault\n\nWhether to set configurePlatforms to [\"build\" \"host\"] by default while building nixpkgs packages. Changing the default may cause a mass rebuild.\n\nType: boolean\n\nDefault: false\n\nDeclared by:\n\npkgs/top-level/config.nix\ncontentAddressedByDefault\n\nWhether to set __contentAddressed to true by default while building nixpkgs packages. Changing the default may cause a mass rebuild.\n\nType: boolean\n\nDefault: false\n\nDeclared by:\n\npkgs/top-level/config.nix\ncudaSupport\n\nWhether to build packages with CUDA support by default while building nixpkgs packages. Changing the default may cause a mass rebuild.\n\nType: boolean\n\nDefault: false\n\nDeclared by:\n\npkgs/top-level/config.nix\ndoCheckByDefault\n\nWhether to run checkPhase by default while building nixpkgs packages. Changing the default may cause a mass rebuild.\n\nType: boolean\n\nDefault: false\n\nDeclared by:\n\npkgs/top-level/config.nix\nshowDerivationWarnings\n\nWhich warnings to display for potentially dangerous or deprecated values passed into stdenv.mkDerivation.\n\nA list of warnings can be found in /pkgs/stdenv/generic/check-meta.nix.\n\nThis is not a stable interface; warnings may be added, changed or removed without prior notice.\n\nType: list of value “maintainerless” (singular enum)\n\nDefault: [ ]\n\nDeclared by:\n\npkgs/top-level/config.nix\nstrictDepsByDefault\n\nWhether to set strictDeps to true by default while building nixpkgs packages. Changing the default may cause a mass rebuild.\n\nType: boolean\n\nDefault: false\n\nDeclared by:\n\npkgs/top-level/config.nix\nstructuredAttrsByDefault\n\nWhether to set __structuredAttrs to true by default while building nixpkgs packages. Changing the default may cause a mass rebuild.\n\nType: boolean\n\nDefault: false\n\nDeclared by:\n\npkgs/top-level/config.nix\nwarnUndeclaredOptions\n\nWhether to warn when config contains an unrecognized attribute.\n\nType: boolean\n\nDefault: false\n\nDeclared by:\n\npkgs/top-level/config.nix\nDeclarative Package Management \nBuild an environment\nGetting documentation\nGNU info setup\nBuild an environment \n\nUsing packageOverrides, it is possible to manage packages declaratively. This means that we can list all of our desired packages within a declarative Nix expression. For example, to have aspell, bc, ffmpeg, coreutils, gdb, nixUnstable, emscripten, jq, nox, and silver-searcher, we could use the following in ~/.config/nixpkgs/config.nix:\n\n{\n  packageOverrides = pkgs: with pkgs; {\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        aspell\n        bc\n        coreutils\n        gdb\n        ffmpeg\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n    };\n  };\n}\n\n\nTo install it into our environment, you can just run nix-env -iA nixpkgs.myPackages. If you want to load the packages to be built from a working copy of nixpkgs you just run nix-env -f. -iA myPackages. To explore what’s been installed, just look through ~/.nix-profile/. You can see that a lot of stuff has been installed. Some of this stuff is useful some of it isn’t. Let’s tell Nixpkgs to only link the stuff that we want:\n\n{\n  packageOverrides = pkgs: with pkgs; {\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        aspell\n        bc\n        coreutils\n        gdb\n        ffmpeg\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n      pathsToLink = [ \"/share\" \"/bin\" ];\n    };\n  };\n}\n\n\npathsToLink tells Nixpkgs to only link the paths listed which gets rid of the extra stuff in the profile. /bin and /share are good defaults for a user environment, getting rid of the clutter. If you are running on Nix on MacOS, you may want to add another path as well, /Applications, that makes GUI apps available.\n\nGetting documentation \n\nAfter building that new environment, look through ~/.nix-profile to make sure everything is there that we wanted. Discerning readers will note that some files are missing. Look inside ~/.nix-profile/share/man/man1/ to verify this. There are no man pages for any of the Nix tools! This is because some packages like Nix have multiple outputs for things like documentation (see section 4). Let’s make Nix install those as well.\n\n{\n  packageOverrides = pkgs: with pkgs; {\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        aspell\n        bc\n        coreutils\n        ffmpeg\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n      pathsToLink = [ \"/share/man\" \"/share/doc\" \"/bin\" ];\n      extraOutputsToInstall = [ \"man\" \"doc\" ];\n    };\n  };\n}\n\n\nThis provides us with some useful documentation for using our packages. However, if we actually want those manpages to be detected by man, we need to set up our environment. This can also be managed within Nix expressions.\n\n{\n  packageOverrides = pkgs: with pkgs; rec {\n    myProfile = writeText \"my-profile\" ''\n      export PATH=$HOME/.nix-profile/bin:/nix/var/nix/profiles/default/bin:/sbin:/bin:/usr/sbin:/usr/bin\n      export MANPATH=$HOME/.nix-profile/share/man:/nix/var/nix/profiles/default/share/man:/usr/share/man\n    '';\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        (runCommand \"profile\" {} ''\n          mkdir -p $out/etc/profile.d\n          cp ${myProfile} $out/etc/profile.d/my-profile.sh\n        '')\n        aspell\n        bc\n        coreutils\n        ffmpeg\n        man\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n      ];\n      pathsToLink = [ \"/share/man\" \"/share/doc\" \"/bin\" \"/etc\" ];\n      extraOutputsToInstall = [ \"man\" \"doc\" ];\n    };\n  };\n}\n\n\nFor this to work fully, you must also have this script sourced when you are logged in. Try adding something like this to your ~/.profile file:\n\n#!/bin/sh\nif [ -d \"${HOME}/.nix-profile/etc/profile.d\" ]; then\n  for i in \"${HOME}/.nix-profile/etc/profile.d/\"*.sh; do\n    if [ -r \"$i\" ]; then\n      . \"$i\"\n    fi\n  done\nfi\n\n\nNow just run . \"${HOME}/.profile\" and you can start loading man pages from your environment.\n\nGNU info setup \n\nConfiguring GNU info is a little bit trickier than man pages. To work correctly, info needs a database to be generated. This can be done with some small modifications to our environment scripts.\n\n{\n  packageOverrides = pkgs: with pkgs; rec {\n    myProfile = writeText \"my-profile\" ''\n      export PATH=$HOME/.nix-profile/bin:/nix/var/nix/profiles/default/bin:/sbin:/bin:/usr/sbin:/usr/bin\n      export MANPATH=$HOME/.nix-profile/share/man:/nix/var/nix/profiles/default/share/man:/usr/share/man\n      export INFOPATH=$HOME/.nix-profile/share/info:/nix/var/nix/profiles/default/share/info:/usr/share/info\n    '';\n    myPackages = pkgs.buildEnv {\n      name = \"my-packages\";\n      paths = [\n        (runCommand \"profile\" {} ''\n          mkdir -p $out/etc/profile.d\n          cp ${myProfile} $out/etc/profile.d/my-profile.sh\n        '')\n        aspell\n        bc\n        coreutils\n        ffmpeg\n        man\n        nixUnstable\n        emscripten\n        jq\n        nox\n        silver-searcher\n        texinfoInteractive\n      ];\n      pathsToLink = [ \"/share/man\" \"/share/doc\" \"/share/info\" \"/bin\" \"/etc\" ];\n      extraOutputsToInstall = [ \"man\" \"doc\" \"info\" ];\n      postBuild = ''\n        if [ -x $out/bin/install-info -a -w $out/share/info ]; then\n          shopt -s nullglob\n          for i in $out/share/info/*.info $out/share/info/*.info.gz; do\n              $out/bin/install-info $i $out/share/info/dir\n          done\n        fi\n      '';\n    };\n  };\n}\n\n\npostBuild tells Nixpkgs to run a command after building the environment. In this case, install-info adds the installed info pages to dir which is GNU info’s default root node. Note that texinfoInteractive is added to the environment to give the install-info command.\n\nOverlays \n\nTable of Contents\n\nInstalling overlays\nDefining overlays\nUsing overlays to configure alternatives\n\nThis chapter describes how to extend and change Nixpkgs using overlays. Overlays are used to add layers in the fixed-point used by Nixpkgs to compose the set of all packages.\n\nNixpkgs can be configured with a list of overlays, which are applied in order. This means that the order of the overlays can be significant if multiple layers override the same package.\n\nInstalling overlays \nSet overlays in NixOS or Nix expressions\nInstall overlays via configuration lookup\n\nThe list of overlays can be set either explicitly in a Nix expression, or through <nixpkgs-overlays> or user configuration files.\n\nSet overlays in NixOS or Nix expressions \n\nOn a NixOS system the value of the nixpkgs.overlays option, if present, is passed to the system Nixpkgs directly as an argument. Note that this does not affect the overlays for non-NixOS operations (e.g. nix-env), which are looked up independently.\n\nThe list of overlays can be passed explicitly when importing nixpkgs, for example import <nixpkgs> { overlays = [ overlay1 overlay2 ]; }.\n\nNOTE: DO NOT USE THIS in nixpkgs. Further overlays can be added by calling the pkgs.extend or pkgs.appendOverlays, although it is often preferable to avoid these functions, because they recompute the Nixpkgs fixpoint, which is somewhat expensive to do.\n\nInstall overlays via configuration lookup \n\nThe list of overlays is determined as follows.\n\nFirst, if an overlays argument to the Nixpkgs function itself is given, then that is used and no path lookup will be performed.\n\nOtherwise, if the Nix path entry <nixpkgs-overlays> exists, we look for overlays at that path, as described below.\n\nSee the section on NIX_PATH in the Nix manual for more details on how to set a value for <nixpkgs-overlays>.\n\nIf one of ~/.config/nixpkgs/overlays.nix and ~/.config/nixpkgs/overlays/ exists, then we look for overlays at that path, as described below. It is an error if both exist.\n\nIf we are looking for overlays at a path, then there are two cases:\n\nIf the path is a file, then the file is imported as a Nix expression and used as the list of overlays.\n\nIf the path is a directory, then we take the content of the directory, order it lexicographically, and attempt to interpret each as an overlay by:\n\nImporting the file, if it is a .nix file.\n\nImporting a top-level default.nix file, if it is a directory.\n\nBecause overlays that are set in NixOS configuration do not affect non-NixOS operations such as nix-env, the overlays.nix option provides a convenient way to use the same overlays for a NixOS system configuration and user configuration: the same file can be used as overlays.nix and imported as the value of nixpkgs.overlays.\n\nDefining overlays \n\nOverlays are Nix functions which accept two arguments, conventionally called self and super, and return a set of packages. For example, the following is a valid overlay.\n\nself: super:\n\n{\n  boost = super.boost.override {\n    python = self.python3;\n  };\n  rr = super.callPackage ./pkgs/rr {\n    stdenv = self.stdenv_32bit;\n  };\n}\n\n\nThe first argument (self) corresponds to the final package set. You should use this set for the dependencies of all packages specified in your overlay. For example, all the dependencies of rr in the example above come from self, as well as the overridden dependencies used in the boost override.\n\nThe second argument (super) corresponds to the result of the evaluation of the previous stages of Nixpkgs. It does not contain any of the packages added by the current overlay, nor any of the following overlays. This set should be used either to refer to packages you wish to override, or to access functions defined in Nixpkgs. For example, the original recipe of boost in the above example, comes from super, as well as the callPackage function.\n\nThe value returned by this function should be a set similar to pkgs/top-level/all-packages.nix, containing overridden and/or new packages.\n\nOverlays are similar to other methods for customizing Nixpkgs, in particular the packageOverrides attribute described in the section called “Modify packages via packageOverrides”. Indeed, packageOverrides acts as an overlay with only the super argument. It is therefore appropriate for basic use, but overlays are more powerful and easier to distribute.\n\nUsing overlays to configure alternatives \nBLAS/LAPACK\nSwitching the MPI implementation\n\nCertain software packages have different implementations of the same interface. Other distributions have functionality to switch between these. For example, Debian provides DebianAlternatives. Nixpkgs has what we call alternatives, which are configured through overlays.\n\nBLAS/LAPACK \n\nIn Nixpkgs, we have multiple implementations of the BLAS/LAPACK numerical linear algebra interfaces. They are:\n\nOpenBLAS\n\nThe Nixpkgs attribute is openblas for ILP64 (integer width = 64 bits) and openblasCompat for LP64 (integer width = 32 bits). openblasCompat is the default.\n\nLAPACK reference (also provides BLAS and CBLAS)\n\nThe Nixpkgs attribute is lapack-reference.\n\nIntel MKL (only works on the x86_64 architecture, unfree)\n\nThe Nixpkgs attribute is mkl.\n\nBLIS\n\nBLIS, available through the attribute blis, is a framework for linear algebra kernels. In addition, it implements the BLAS interface.\n\nAMD BLIS/LIBFLAME (optimized for modern AMD x86_64 CPUs)\n\nThe AMD fork of the BLIS library, with attribute amd-blis, extends BLIS with optimizations for modern AMD CPUs. The changes are usually submitted to the upstream BLIS project after some time. However, AMD BLIS typically provides some performance improvements on AMD Zen CPUs. The complementary AMD LIBFLAME library, with attribute amd-libflame, provides a LAPACK implementation.\n\nIntroduced in PR #83888, we are able to override the blas and lapack packages to use different implementations, through the blasProvider and lapackProvider argument. This can be used to select a different provider. BLAS providers will have symlinks in $out/lib/libblas.so.3 and $out/lib/libcblas.so.3 to their respective BLAS libraries. Likewise, LAPACK providers will have symlinks in $out/lib/liblapack.so.3 and $out/lib/liblapacke.so.3 to their respective LAPACK libraries. For example, Intel MKL is both a BLAS and LAPACK provider. An overlay can be created to use Intel MKL that looks like:\n\nself: super:\n\n{\n  blas = super.blas.override {\n    blasProvider = self.mkl;\n  };\n\n  lapack = super.lapack.override {\n    lapackProvider = self.mkl;\n  };\n}\n\n\nThis overlay uses Intel’s MKL library for both BLAS and LAPACK interfaces. Note that the same can be accomplished at runtime using LD_LIBRARY_PATH of libblas.so.3 and liblapack.so.3. For instance:\n\n$ LD_LIBRARY_PATH=$(nix-build -A mkl)/lib${LD_LIBRARY_PATH:+:}$LD_LIBRARY_PATH nix-shell -p octave --run octave\n\n\nIntel MKL requires an openmp implementation when running with multiple processors. By default, mkl will use Intel’s iomp implementation if no other is specified, but this is a runtime-only dependency and binary compatible with the LLVM implementation. To use that one instead, Intel recommends users set it with LD_PRELOAD. Note that mkl is only available on x86_64-linux and x86_64-darwin. Moreover, Hydra is not building and distributing pre-compiled binaries using it.\n\nTo override blas and lapack with its reference implementations (i.e. for development purposes), one can use the following overlay:\n\nself: super:\n\n{\n  blas = super.blas.override {\n    blasProvider = self.lapack-reference;\n  };\n\n  lapack = super.lapack.override {\n    lapackProvider = self.lapack-reference;\n  };\n}\n\n\nFor BLAS/LAPACK switching to work correctly, all packages must depend on blas or lapack. This ensures that only one BLAS/LAPACK library is used at one time. There are two versions of BLAS/LAPACK currently in the wild, LP64 (integer size = 32 bits) and ILP64 (integer size = 64 bits). The attributes blas and lapack are LP64 by default. Their ILP64 version are provided through the attributes blas-ilp64 and lapack-ilp64. Some software needs special flags or patches to work with ILP64. You can check if ILP64 is used in Nixpkgs with blas.isILP64 and lapack.isILP64. Some software does NOT work with ILP64, and derivations need to specify an assertion to prevent this. You can prevent ILP64 from being used with the following:\n\n{ stdenv, blas, lapack, ... }:\n\nassert (!blas.isILP64) && (!lapack.isILP64);\n\nstdenv.mkDerivation {\n  ...\n}\n\nSwitching the MPI implementation \n\nAll programs that are built with MPI support use the generic attribute mpi as an input. At the moment Nixpkgs natively provides two different MPI implementations:\n\nOpen MPI (default), attribute name openmpi\n\nMPICH, attribute name mpich\n\nMVAPICH, attribute name mvapich\n\nTo provide MPI enabled applications that use MPICH, instead of the default Open MPI, use the following overlay:\n\nself: super:\n\n{\n  mpi = self.mpich;\n}\n\nOverriding \n\nTable of Contents\n\n<pkg>.override\n<pkg>.overrideAttrs\n<pkg>.overrideDerivation\nlib.makeOverridable\n\nSometimes one wants to override parts of nixpkgs, e.g. derivation attributes, the results of derivations.\n\nThese functions are used to make changes to packages, returning only single packages. Overlays, on the other hand, can be used to combine the overridden packages across the entire package set of Nixpkgs.\n\n<pkg>.override \n\nThe function override is usually available for all the derivations in the nixpkgs expression (pkgs).\n\nIt is used to override the arguments passed to a function.\n\nExample usages:\n\npkgs.foo.override { arg1 = val1; arg2 = val2; ... }\n\n\nIt’s also possible to access the previous arguments.\n\npkgs.foo.override (previous: { arg1 = previous.arg1; ... })\n\nimport pkgs.path { overlays = [ (self: super: {\n  foo = super.foo.override { barSupport = true ; };\n  })]};\n\nmypkg = pkgs.callPackage ./mypkg.nix {\n  mydep = pkgs.mydep.override { ... };\n  }\n\n\nIn the first example, pkgs.foo is the result of a function call with some default arguments, usually a derivation. Using pkgs.foo.override will call the same function with the given new arguments.\n\n<pkg>.overrideAttrs \n\nThe function overrideAttrs allows overriding the attribute set passed to a stdenv.mkDerivation call, producing a new derivation based on the original one. This function is available on all derivations produced by the stdenv.mkDerivation function, which is most packages in the nixpkgs expression pkgs.\n\nExample usages:\n\nhelloBar = pkgs.hello.overrideAttrs (finalAttrs: previousAttrs: {\n  pname = previousAttrs.pname + \"-bar\";\n});\n\n\nIn the above example, “-bar” is appended to the pname attribute, while all other attributes will be retained from the original hello package.\n\nThe argument previousAttrs is conventionally used to refer to the attr set originally passed to stdenv.mkDerivation.\n\nThe argument finalAttrs refers to the final attributes passed to mkDerivation, plus the finalPackage attribute which is equal to the result of mkDerivation or subsequent overrideAttrs calls.\n\nIf only a one-argument function is written, the argument has the meaning of previousAttrs.\n\nFunction arguments can be omitted entirely if there is no need to access previousAttrs or finalAttrs.\n\nhelloWithDebug = pkgs.hello.overrideAttrs {\n  separateDebugInfo = true;\n};\n\n\nIn the above example, the separateDebugInfo attribute is overridden to be true, thus building debug info for helloWithDebug.\n\nNote\n\nNote that separateDebugInfo is processed only by the stdenv.mkDerivation function, not the generated, raw Nix derivation. Thus, using overrideDerivation will not work in this case, as it overrides only the attributes of the final derivation. It is for this reason that overrideAttrs should be preferred in (almost) all cases to overrideDerivation, i.e. to allow using stdenv.mkDerivation to process input arguments, as well as the fact that it is easier to use (you can use the same attribute names you see in your Nix code, instead of the ones generated (e.g. buildInputs vs nativeBuildInputs), and it involves less typing).\n\n<pkg>.overrideDerivation \nWarning\n\nYou should prefer overrideAttrs in almost all cases, see its documentation for the reasons why. overrideDerivation is not deprecated and will continue to work, but is less nice to use and does not have as many abilities as overrideAttrs.\n\nWarning\n\nDo not use this function in Nixpkgs as it evaluates a derivation before modifying it, which breaks package abstraction. In addition, this evaluation-per-function application incurs a performance penalty, which can become a problem if many overrides are used. It is only intended for ad-hoc customisation, such as in ~/.config/nixpkgs/config.nix.\n\nThe function overrideDerivation creates a new derivation based on an existing one by overriding the original’s attributes with the attribute set produced by the specified function. This function is available on all derivations defined using the makeOverridable function. Most standard derivation-producing functions, such as stdenv.mkDerivation, are defined using this function, which means most packages in the nixpkgs expression, pkgs, have this function.\n\nExample usage:\n\nmySed = pkgs.gnused.overrideDerivation (oldAttrs: {\n  name = \"sed-4.2.2-pre\";\n  src = fetchurl {\n    url = \"ftp://alpha.gnu.org/gnu/sed/sed-4.2.2-pre.tar.bz2\";\n    hash = \"sha256-MxBJRcM2rYzQYwJ5XKxhXTQByvSg5jZc5cSHEZoB2IY=\";\n  };\n  patches = [];\n});\n\n\nIn the above example, the name, src, and patches of the derivation will be overridden, while all other attributes will be retained from the original derivation.\n\nThe argument oldAttrs is used to refer to the attribute set of the original derivation.\n\nNote\n\nA package’s attributes are evaluated before being modified by the overrideDerivation function. For example, the name attribute reference in url = \"mirror://gnu/hello/${name}.tar.gz\"; is filled-in before the overrideDerivation function modifies the attribute set. This means that overriding the name attribute, in this example, will not change the value of the url attribute. Instead, we need to override both the name and url attributes.\n\nlib.makeOverridable \n\nThe function lib.makeOverridable is used to make the result of a function easily customizable. This utility only makes sense for functions that accept an argument set and return an attribute set.\n\nExample usage:\n\nf = { a, b }: { result = a+b; };\nc = lib.makeOverridable f { a = 1; b = 2; };\n\n\nThe variable c is the value of the f function applied with some default arguments. Hence the value of c.result is 3, in this example.\n\nThe variable c however also has some additional functions, like c.override which can be used to override the default arguments. In this example the value of (c.override { a = 4; }).result is 6.\n\nNixpkgs lib \n\nTable of Contents\n\nFunctions reference\nModule System\nFunctions reference \n\nTable of Contents\n\nNixpkgs Library Functions\nGenerators\nDebugging Nix Expressions\nprefer-remote-fetch overlay\npkgs.nix-gitignore\nFile sets\n\nThe nixpkgs repository has several utility functions to manipulate Nix expressions.\n\nNixpkgs Library Functions \nlib.asserts: assertion functions\nlib.attrsets: attribute set functions\nlib.strings: string manipulation functions\nlib.versions: version string functions\nlib.trivial: miscellaneous functions\nlib.fixedPoints: explicit recursion functions\nlib.lists: list manipulation functions\nlib.debug: debugging functions\nlib.options: NixOS / nixpkgs option handling\nlib.path: path functions\nlib.filesystem: filesystem functions\nlib.fileset: file set functions\nlib.sources: source filtering functions\nlib.cli: command-line serialization functions\nlib.gvariant: GVariant formatted string serialization functions\nlib.customisation: Functions to customise (derivation-related) functions, derivatons, or attribute sets\n\nNixpkgs provides a standard library at pkgs.lib, or through import <nixpkgs/lib>.\n\nlib.asserts: assertion functions \nlib.asserts.assertMsg\n\nType: assertMsg :: Bool -> String -> Bool\n\nThrow if pred is false, else return pred. Intended to be used to augment asserts with helpful error messages.\n\npred\n\nPredicate that needs to succeed, otherwise msg is thrown\n\nmsg\n\nMessage to throw in case pred fails\n\nExample 1. lib.asserts.assertMsg usage example\n\nassertMsg false \"nope\"\nstderr> error: nope\n\nassert assertMsg (\"foo\" == \"bar\") \"foo is not bar, silly\"; \"\"\nstderr> error: foo is not bar, silly\n\n\n\n\nLocated at lib/asserts.nix:19 in <nixpkgs>.\n\nlib.asserts.assertOneOf\n\nType: assertOneOf :: String -> ComparableVal -> List ComparableVal -> Bool\n\nSpecialized assertMsg for checking if val is one of the elements of the list xs. Useful for checking enums.\n\nname\n\nThe name of the variable the user entered val into, for inclusion in the error message\n\nval\n\nThe value of what the user provided, to be compared against the values in xs\n\nxs\n\nThe list of valid values\n\nExample 2. lib.asserts.assertOneOf usage example\n\nlet sslLibrary = \"libressl\";\nin assertOneOf \"sslLibrary\" sslLibrary [ \"openssl\" \"bearssl\" ]\nstderr> error: sslLibrary must be one of [\nstderr>   \"openssl\"\nstderr>   \"bearssl\"\nstderr> ], but is: \"libressl\"\n\n\n\n\nLocated at lib/asserts.nix:40 in <nixpkgs>.\n\nlib.asserts.assertEachOneOf\n\nType: assertEachOneOf :: String -> List ComparableVal -> List ComparableVal -> Bool\n\nSpecialized assertMsg for checking if every one of vals is one of the elements of the list xs. Useful for checking lists of supported attributes.\n\nname\n\nThe name of the variable the user entered val into, for inclusion in the error message\n\nvals\n\nThe list of values of what the user provided, to be compared against the values in xs\n\nxs\n\nThe list of valid values\n\nExample 3. lib.asserts.assertEachOneOf usage example\n\nlet sslLibraries = [ \"libressl\" \"bearssl\" ];\nin assertEachOneOf \"sslLibraries\" sslLibraries [ \"openssl\" \"bearssl\" ]\nstderr> error: each element in sslLibraries must be one of [\nstderr>   \"openssl\"\nstderr>   \"bearssl\"\nstderr> ], but is: [\nstderr>   \"libressl\"\nstderr>   \"bearssl\"\nstderr> ]\n\n\n\n\nLocated at lib/asserts.nix:70 in <nixpkgs>.\n\nlib.attrsets: attribute set functions \nlib.attrsets.attrByPath\n\nType: attrByPath :: [String] -> Any -> AttrSet -> Any\n\nReturn an attribute from nested attribute sets.\n\nattrPath\n\nA list of strings representing the attribute path to return from set\n\ndefault\n\nDefault value if attrPath does not resolve to an existing value\n\nset\n\nThe nested attribute set to select values from\n\nExample 4. lib.attrsets.attrByPath usage example\n\nx = { a = { b = 3; }; }\n# [\"a\" \"b\"] is equivalent to x.a.b\n# 6 is a default value to return if the path does not exist in attrset\nattrByPath [\"a\" \"b\"] 6 x\n=> 3\nattrByPath [\"z\" \"z\"] 6 x\n=> 6\n\n\n\n\nLocated at lib/attrsets.nix:30 in <nixpkgs>.\n\nlib.attrsets.hasAttrByPath\n\nType: hasAttrByPath :: [String] -> AttrSet -> Bool\n\nReturn if an attribute from nested attribute set exists.\n\nattrPath\n\nA list of strings representing the attribute path to check from set\n\ne\n\nThe nested attribute set to check\n\nExample 5. lib.attrsets.hasAttrByPath usage example\n\nx = { a = { b = 3; }; }\nhasAttrByPath [\"a\" \"b\"] x\n=> true\nhasAttrByPath [\"z\" \"z\"] x\n=> false\n\n\n\n\nLocated at lib/attrsets.nix:56 in <nixpkgs>.\n\nlib.attrsets.setAttrByPath\n\nType: setAttrByPath :: [String] -> Any -> AttrSet\n\nCreate a new attribute set with value set at the nested attribute location specified in attrPath.\n\nattrPath\n\nA list of strings representing the attribute path to set\n\nvalue\n\nThe value to set at the location described by attrPath\n\nExample 6. lib.attrsets.setAttrByPath usage example\n\nsetAttrByPath [\"a\" \"b\"] 3\n=> { a = { b = 3; }; }\n\n\n\n\nLocated at lib/attrsets.nix:78 in <nixpkgs>.\n\nlib.attrsets.getAttrFromPath\n\nType: getAttrFromPath :: [String] -> AttrSet -> Any\n\nLike attrByPath, but without a default value. If it doesn’t find the path it will throw an error.\n\nattrPath\n\nA list of strings representing the attribute path to get from set\n\nset\n\nThe nested attribute set to find the value in.\n\nExample 7. lib.attrsets.getAttrFromPath usage example\n\nx = { a = { b = 3; }; }\ngetAttrFromPath [\"a\" \"b\"] x\n=> 3\ngetAttrFromPath [\"z\" \"z\"] x\n=> error: cannot find attribute `z.z'\n\n\n\n\nLocated at lib/attrsets.nix:104 in <nixpkgs>.\n\nlib.attrsets.concatMapAttrs\n\nType: concatMapAttrs :: (String -> a -> AttrSet) -> AttrSet -> AttrSet\n\nMap each attribute in the given set and merge them into a new attribute set.\n\nf\n\nFunction argument\n\nv\n\nFunction argument\n\nExample 8. lib.attrsets.concatMapAttrs usage example\n\nconcatMapAttrs\n  (name: value: {\n    ${name} = value;\n    ${name + value} = value;\n  })\n  { x = \"a\"; y = \"b\"; }\n=> { x = \"a\"; xa = \"a\"; y = \"b\"; yb = \"b\"; }\n\n\n\n\nLocated at lib/attrsets.nix:126 in <nixpkgs>.\n\nlib.attrsets.updateManyAttrsByPath\n\nType: updateManyAttrsByPath :: [{ path :: [String]; update :: (Any -> Any); }] -> AttrSet -> AttrSet\n\nUpdate or set specific paths of an attribute set.\n\nTakes a list of updates to apply and an attribute set to apply them to, and returns the attribute set with the updates applied. Updates are represented as { path = ...; update = ...; } values, where path is a list of strings representing the attribute path that should be updated, and update is a function that takes the old value at that attribute path as an argument and returns the new value it should be.\n\nProperties:\n\nUpdates to deeper attribute paths are applied before updates to more shallow attribute paths\n\nMultiple updates to the same attribute path are applied in the order they appear in the update list\n\nIf any but the last path element leads into a value that is not an attribute set, an error is thrown\n\nIf there is an update for an attribute path that doesn’t exist, accessing the argument in the update function causes an error, but intermediate attribute sets are implicitly created as needed\n\nExample 9. lib.attrsets.updateManyAttrsByPath usage example\n\nupdateManyAttrsByPath [\n  {\n    path = [ \"a\" \"b\" ];\n    update = old: { d = old.c; };\n  }\n  {\n    path = [ \"a\" \"b\" \"c\" ];\n    update = old: old + 1;\n  }\n  {\n    path = [ \"x\" \"y\" ];\n    update = old: \"xy\";\n  }\n] { a.b.c = 0; }\n=> { a = { b = { d = 1; }; }; x = { y = \"xy\"; }; }\n\n\n\n\nLocated at lib/attrsets.nix:177 in <nixpkgs>.\n\nlib.attrsets.attrVals\n\nType: attrVals :: [String] -> AttrSet -> [Any]\n\nReturn the specified attributes from a set.\n\nnameList\n\nThe list of attributes to fetch from set. Each attribute name must exist on the attrbitue set\n\nset\n\nThe set to get attribute values from\n\nExample 10. lib.attrsets.attrVals usage example\n\nattrVals [\"a\" \"b\" \"c\"] as\n=> [as.a as.b as.c]\n\n\n\n\nLocated at lib/attrsets.nix:245 in <nixpkgs>.\n\nlib.attrsets.attrValues\n\nType: attrValues :: AttrSet -> [Any]\n\nReturn the values of all attributes in the given set, sorted by attribute name.\n\nExample 11. lib.attrsets.attrValues usage example\n\nattrValues {c = 3; a = 1; b = 2;}\n=> [1 2 3]\n\n\n\n\nLocated at lib/attrsets.nix:262 in <nixpkgs>.\n\nlib.attrsets.getAttrs\n\nType: getAttrs :: [String] -> AttrSet -> AttrSet\n\nGiven a set of attribute names, return the set of the corresponding attributes from the given set.\n\nnames\n\nA list of attribute names to get out of set\n\nattrs\n\nThe set to get the named attributes from\n\nExample 12. lib.attrsets.getAttrs usage example\n\ngetAttrs [ \"a\" \"b\" ] { a = 1; b = 2; c = 3; }\n=> { a = 1; b = 2; }\n\n\n\n\nLocated at lib/attrsets.nix:275 in <nixpkgs>.\n\nlib.attrsets.catAttrs\n\nType: catAttrs :: String -> [AttrSet] -> [Any]\n\nCollect each attribute named attr from a list of attribute sets. Sets that don’t contain the named attribute are ignored.\n\nExample 13. lib.attrsets.catAttrs usage example\n\ncatAttrs \"a\" [{a = 1;} {b = 0;} {a = 2;}]\n=> [1 2]\n\n\n\n\nLocated at lib/attrsets.nix:291 in <nixpkgs>.\n\nlib.attrsets.filterAttrs\n\nType: filterAttrs :: (String -> Any -> Bool) -> AttrSet -> AttrSet\n\nFilter an attribute set by removing all attributes for which the given predicate return false.\n\npred\n\nPredicate taking an attribute name and an attribute value, which returns true to include the attribute, or false to exclude the attribute.\n\nset\n\nThe attribute set to filter\n\nExample 14. lib.attrsets.filterAttrs usage example\n\nfilterAttrs (n: v: n == \"foo\") { foo = 1; bar = 2; }\n=> { foo = 1; }\n\n\n\n\nLocated at lib/attrsets.nix:305 in <nixpkgs>.\n\nlib.attrsets.filterAttrsRecursive\n\nType: filterAttrsRecursive :: (String -> Any -> Bool) -> AttrSet -> AttrSet\n\nFilter an attribute set recursively by removing all attributes for which the given predicate return false.\n\npred\n\nPredicate taking an attribute name and an attribute value, which returns true to include the attribute, or false to exclude the attribute.\n\nset\n\nThe attribute set to filter\n\nExample 15. lib.attrsets.filterAttrsRecursive usage example\n\nfilterAttrsRecursive (n: v: v != null) { foo = { bar = null; }; }\n=> { foo = {}; }\n\n\n\n\nLocated at lib/attrsets.nix:323 in <nixpkgs>.\n\nlib.attrsets.foldlAttrs\n\nType: foldlAttrs :: ( a -> String -> b -> a ) -> a -> { ... :: b } -> a\n\nLike lib.lists.foldl' but for attribute sets. Iterates over every name-value pair in the given attribute set. The result of the callback function is often called acc for accumulator. It is passed between callbacks from left to right and the final acc is the return value of foldlAttrs.\n\nAttention: There is a completely different function lib.foldAttrs which has nothing to do with this function, despite the similar name.\n\nf\n\nFunction argument\n\ninit\n\nFunction argument\n\nset\n\nFunction argument\n\nExample 16. lib.attrsets.foldlAttrs usage example\n\nfoldlAttrs\n  (acc: name: value: {\n    sum = acc.sum + value;\n    names = acc.names ++ [name];\n  })\n  { sum = 0; names = []; }\n  {\n    foo = 1;\n    bar = 10;\n  }\n->\n  {\n    sum = 11;\n    names = [\"bar\" \"foo\"];\n  }\n\nfoldlAttrs\n  (throw \"function not needed\")\n  123\n  {};\n->\n  123\n\nfoldlAttrs\n  (acc: _: _: acc)\n  3\n  { z = throw \"value not needed\"; a = throw \"value not needed\"; };\n->\n  3\n\nThe accumulator doesn't have to be an attrset.\nIt can be as simple as a number or string.\n\nfoldlAttrs\n  (acc: _: v: acc * 10 + v)\n  1\n  { z = 1; a = 2; };\n->\n  121\n\n\n\n\nLocated at lib/attrsets.nix:394 in <nixpkgs>.\n\nlib.attrsets.foldAttrs\n\nType: foldAttrs :: (Any -> Any -> Any) -> Any -> [AttrSets] -> Any\n\nApply fold functions to values grouped by key.\n\nop\n\nA function, given a value and a collector combines the two.\n\nnul\n\nThe starting value.\n\nlist_of_attrs\n\nA list of attribute sets to fold together by key.\n\nExample 17. lib.attrsets.foldAttrs usage example\n\nfoldAttrs (item: acc: [item] ++ acc) [] [{ a = 2; } { a = 3; }]\n=> { a = [ 2 3 ]; }\n\n\n\n\nLocated at lib/attrsets.nix:410 in <nixpkgs>.\n\nlib.attrsets.collect\n\nType: collect :: (AttrSet -> Bool) -> AttrSet -> [x]\n\nRecursively collect sets that verify a given predicate named pred from the set attrs. The recursion is stopped when the predicate is verified.\n\npred\n\nGiven an attribute’s value, determine if recursion should stop.\n\nattrs\n\nThe attribute set to recursively collect.\n\nExample 18. lib.attrsets.collect usage example\n\ncollect isList { a = { b = [\"b\"]; }; c = [1]; }\n=> [[\"b\"] [1]]\n\ncollect (x: x ? outPath)\n   { a = { outPath = \"a/\"; }; b = { outPath = \"b/\"; }; }\n=> [{ outPath = \"a/\"; } { outPath = \"b/\"; }]\n\n\n\n\nLocated at lib/attrsets.nix:439 in <nixpkgs>.\n\nlib.attrsets.cartesianProductOfSets\n\nType: cartesianProductOfSets :: AttrSet -> [AttrSet]\n\nReturn the cartesian product of attribute set value combinations.\n\nattrsOfLists\n\nAttribute set with attributes that are lists of values\n\nExample 19. lib.attrsets.cartesianProductOfSets usage example\n\ncartesianProductOfSets { a = [ 1 2 ]; b = [ 10 20 ]; }\n=> [\n     { a = 1; b = 10; }\n     { a = 1; b = 20; }\n     { a = 2; b = 10; }\n     { a = 2; b = 20; }\n   ]\n\n\n\n\nLocated at lib/attrsets.nix:464 in <nixpkgs>.\n\nlib.attrsets.nameValuePair\n\nType: nameValuePair :: String -> Any -> { name :: String; value :: Any; }\n\nUtility function that creates a {name, value} pair as expected by builtins.listToAttrs.\n\nname\n\nAttribute name\n\nvalue\n\nAttribute value\n\nExample 20. lib.attrsets.nameValuePair usage example\n\nnameValuePair \"some\" 6\n=> { name = \"some\"; value = 6; }\n\n\n\n\nLocated at lib/attrsets.nix:483 in <nixpkgs>.\n\nlib.attrsets.mapAttrs\n\nType: mapAttrs :: (String -> Any -> Any) -> AttrSet -> AttrSet\n\nApply a function to each element in an attribute set, creating a new attribute set.\n\nExample 21. lib.attrsets.mapAttrs usage example\n\nmapAttrs (name: value: name + \"-\" + value)\n   { x = \"foo\"; y = \"bar\"; }\n=> { x = \"x-foo\"; y = \"y-bar\"; }\n\n\n\n\nLocated at lib/attrsets.nix:501 in <nixpkgs>.\n\nlib.attrsets.mapAttrs'\n\nType: mapAttrs' :: (String -> Any -> { name :: String; value :: Any; }) -> AttrSet -> AttrSet\n\nLike mapAttrs, but allows the name of each attribute to be changed in addition to the value. The applied function should return both the new name and value as a nameValuePair.\n\nf\n\nA function, given an attribute’s name and value, returns a new nameValuePair.\n\nset\n\nAttribute set to map over.\n\nExample 22. lib.attrsets.mapAttrs' usage example\n\nmapAttrs' (name: value: nameValuePair (\"foo_\" + name) (\"bar-\" + value))\n   { x = \"a\"; y = \"b\"; }\n=> { foo_x = \"bar-a\"; foo_y = \"bar-b\"; }\n\n\n\n\nLocated at lib/attrsets.nix:518 in <nixpkgs>.\n\nlib.attrsets.mapAttrsToList\n\nType: mapAttrsToList :: (String -> a -> b) -> AttrSet -> [b]\n\nCall a function for each attribute in the given set and return the result in a list.\n\nf\n\nA function, given an attribute’s name and value, returns a new value.\n\nattrs\n\nAttribute set to map over.\n\nExample 23. lib.attrsets.mapAttrsToList usage example\n\nmapAttrsToList (name: value: name + value)\n   { x = \"a\"; y = \"b\"; }\n=> [ \"xa\" \"yb\" ]\n\n\n\n\nLocated at lib/attrsets.nix:538 in <nixpkgs>.\n\nlib.attrsets.attrsToList\n\nType: attrsToList :: AttrSet -> [ { name :: String; value :: Any; } ]\n\nDeconstruct an attrset to a list of name-value pairs as expected by builtins.listToAttrs. Each element of the resulting list is an attribute set with these attributes:\n\nname (string): The name of the attribute\n\nvalue (any): The value of the attribute\n\nThe following is always true:\n\nbuiltins.listToAttrs (attrsToList attrs) == attrs\n\nWarning\n\nThe opposite is not always true. In general expect that\n\nattrsToList (builtins.listToAttrs list) != list\n\n\nThis is because the listToAttrs removes duplicate names and doesn’t preserve the order of the list.\n\nExample 24. lib.attrsets.attrsToList usage example\n\nattrsToList { foo = 1; bar = \"asdf\"; }\n=> [ { name = \"bar\"; value = \"asdf\"; } { name = \"foo\"; value = 1; } ]\n\n\n\n\nLocated at lib/attrsets.nix:573 in <nixpkgs>.\n\nlib.attrsets.mapAttrsRecursive\n\nType: mapAttrsRecursive :: ([String] -> a -> b) -> AttrSet -> AttrSet\n\nLike mapAttrs, except that it recursively applies itself to the leaf attributes of a potentially-nested attribute set: the second argument of the function will never be an attrset. Also, the first argument of the argument function is a list of the attribute names that form the path to the leaf attribute.\n\nFor a function that gives you control over what counts as a leaf, see mapAttrsRecursiveCond.\n\nf\n\nA function, given a list of attribute names and a value, returns a new value.\n\nset\n\nSet to recursively map over.\n\nExample 25. lib.attrsets.mapAttrsRecursive usage example\n\nmapAttrsRecursive (path: value: concatStringsSep \"-\" (path ++ [value]))\n  { n = { a = \"A\"; m = { b = \"B\"; c = \"C\"; }; }; d = \"D\"; }\n=> { n = { a = \"n-a-A\"; m = { b = \"n-m-b-B\"; c = \"n-m-c-C\"; }; }; d = \"d-D\"; }\n\n\n\n\nLocated at lib/attrsets.nix:593 in <nixpkgs>.\n\nlib.attrsets.mapAttrsRecursiveCond\n\nType: mapAttrsRecursiveCond :: (AttrSet -> Bool) -> ([String] -> a -> b) -> AttrSet -> AttrSet\n\nLike mapAttrsRecursive, but it takes an additional predicate function that tells it whether to recurse into an attribute set. If it returns false, mapAttrsRecursiveCond does not recurse, but does apply the map function. If it returns true, it does recurse, and does not apply the map function.\n\ncond\n\nA function, given the attribute set the recursion is currently at, determine if to recurse deeper into that attribute set.\n\nf\n\nA function, given a list of attribute names and a value, returns a new value.\n\nset\n\nAttribute set to recursively map over.\n\nExample 26. lib.attrsets.mapAttrsRecursiveCond usage example\n\n# To prevent recursing into derivations (which are attribute\n# sets with the attribute \"type\" equal to \"derivation\"):\nmapAttrsRecursiveCond\n  (as: !(as ? \"type\" && as.type == \"derivation\"))\n  (x: ... do something ...)\n  attrs\n\n\n\n\nLocated at lib/attrsets.nix:618 in <nixpkgs>.\n\nlib.attrsets.genAttrs\n\nType: genAttrs :: [ String ] -> (String -> Any) -> AttrSet\n\nGenerate an attribute set by mapping a function over a list of attribute names.\n\nnames\n\nNames of values in the resulting attribute set.\n\nf\n\nA function, given the name of the attribute, returns the attribute’s value.\n\nExample 27. lib.attrsets.genAttrs usage example\n\ngenAttrs [ \"foo\" \"bar\" ] (name: \"x_\" + name)\n=> { foo = \"x_foo\"; bar = \"x_bar\"; }\n\n\n\n\nLocated at lib/attrsets.nix:647 in <nixpkgs>.\n\nlib.attrsets.isDerivation\n\nType: isDerivation :: Any -> Bool\n\nCheck whether the argument is a derivation. Any set with { type = \"derivation\"; } counts as a derivation.\n\nvalue\n\nValue to check.\n\nExample 28. lib.attrsets.isDerivation usage example\n\nnixpkgs = import <nixpkgs> {}\nisDerivation nixpkgs.ruby\n=> true\nisDerivation \"foobar\"\n=> false\n\n\n\n\nLocated at lib/attrsets.nix:668 in <nixpkgs>.\n\nlib.attrsets.toDerivation\n\nType: toDerivation :: Path -> Derivation\n\nConverts a store path to a fake derivation.\n\npath\n\nA store path to convert to a derivation.\n\nLocated at lib/attrsets.nix:677 in <nixpkgs>.\n\nlib.attrsets.optionalAttrs\n\nType: optionalAttrs :: Bool -> AttrSet -> AttrSet\n\nIf cond is true, return the attribute set as, otherwise an empty attribute set.\n\ncond\n\nCondition under which the as attribute set is returned.\n\nas\n\nThe attribute set to return if cond is true.\n\nExample 29. lib.attrsets.optionalAttrs usage example\n\noptionalAttrs (true) { my = \"set\"; }\n=> { my = \"set\"; }\noptionalAttrs (false) { my = \"set\"; }\n=> { }\n\n\n\n\nLocated at lib/attrsets.nix:705 in <nixpkgs>.\n\nlib.attrsets.zipAttrsWithNames\n\nType: zipAttrsWithNames :: [ String ] -> (String -> [ Any ] -> Any) -> [ AttrSet ] -> AttrSet\n\nMerge sets of attributes and use the function f to merge attributes values.\n\nnames\n\nList of attribute names to zip.\n\nf\n\nA function, accepts an attribute name, all the values, and returns a combined value.\n\nsets\n\nList of values from the list of attribute sets.\n\nExample 30. lib.attrsets.zipAttrsWithNames usage example\n\nzipAttrsWithNames [\"a\"] (name: vs: vs) [{a = \"x\";} {a = \"y\"; b = \"z\";}]\n=> { a = [\"x\" \"y\"]; }\n\n\n\n\nLocated at lib/attrsets.nix:723 in <nixpkgs>.\n\nlib.attrsets.zipAttrsWith\n\nType: zipAttrsWith :: (String -> [ Any ] -> Any) -> [ AttrSet ] -> AttrSet\n\nMerge sets of attributes and use the function f to merge attribute values. Like lib.attrsets.zipAttrsWithNames with all key names are passed for names.\n\nImplementation note: Common names appear multiple times in the list of names, hopefully this does not affect the system because the maximal laziness avoid computing twice the same expression and listToAttrs does not care about duplicated attribute names.\n\nExample 31. lib.attrsets.zipAttrsWith usage example\n\nzipAttrsWith (name: values: values) [{a = \"x\";} {a = \"y\"; b = \"z\";}]\n=> { a = [\"x\" \"y\"]; b = [\"z\"]; }\n\n\n\n\nLocated at lib/attrsets.nix:751 in <nixpkgs>.\n\nlib.attrsets.zipAttrs\n\nType: zipAttrs :: [ AttrSet ] -> AttrSet\n\nMerge sets of attributes and combine each attribute value in to a list.\n\nLike lib.attrsets.zipAttrsWith with (name: values: values) as the function.\n\nsets\n\nList of attribute sets to zip together.\n\nExample 32. lib.attrsets.zipAttrs usage example\n\nzipAttrs [{a = \"x\";} {a = \"y\"; b = \"z\";}]\n=> { a = [\"x\" \"y\"]; b = [\"z\"]; }\n\n\n\n\nLocated at lib/attrsets.nix:766 in <nixpkgs>.\n\nlib.attrsets.mergeAttrsList\n\nType: mergeAttrsList :: [ Attrs ] -> Attrs\n\nMerge a list of attribute sets together using the // operator. In case of duplicate attributes, values from later list elements take precedence over earlier ones. The result is the same as foldl mergeAttrs { }, but the performance is better for large inputs. For n list elements, each with an attribute set containing m unique attributes, the complexity of this operation is O(nm log n).\n\nlist\n\nFunction argument\n\nExample 33. lib.attrsets.mergeAttrsList usage example\n\nmergeAttrsList [ { a = 0; b = 1; } { c = 2; d = 3; } ]\n=> { a = 0; b = 1; c = 2; d = 3; }\nmergeAttrsList [ { a = 0; } { a = 1; } ]\n=> { a = 1; }\n\n\n\n\nLocated at lib/attrsets.nix:786 in <nixpkgs>.\n\nlib.attrsets.recursiveUpdateUntil\n\nType: recursiveUpdateUntil :: ( [ String ] -> AttrSet -> AttrSet -> Bool ) -> AttrSet -> AttrSet -> AttrSet\n\nDoes the same as the update operator ‘//’ except that attributes are merged until the given predicate is verified. The predicate should accept 3 arguments which are the path to reach the attribute, a part of the first attribute set and a part of the second attribute set. When the predicate is satisfied, the value of the first attribute set is replaced by the value of the second attribute set.\n\npred\n\nPredicate, taking the path to the current attribute as a list of strings for attribute names, and the two values at that path from the original arguments.\n\nlhs\n\nLeft attribute set of the merge.\n\nrhs\n\nRight attribute set of the merge.\n\nExample 34. lib.attrsets.recursiveUpdateUntil usage example\n\nrecursiveUpdateUntil (path: l: r: path == [\"foo\"]) {\n  # first attribute set\n  foo.bar = 1;\n  foo.baz = 2;\n  bar = 3;\n} {\n  #second attribute set\n  foo.bar = 1;\n  foo.quz = 2;\n  baz = 4;\n}\n\n=> {\n  foo.bar = 1; # 'foo.*' from the second set\n  foo.quz = 2; #\n  bar = 3;     # 'bar' from the first set\n  baz = 4;     # 'baz' from the second set\n}\n\n\n\n\nLocated at lib/attrsets.nix:838 in <nixpkgs>.\n\nlib.attrsets.recursiveUpdate\n\nType: recursiveUpdate :: AttrSet -> AttrSet -> AttrSet\n\nA recursive variant of the update operator ‘//’. The recursion stops when one of the attribute values is not an attribute set, in which case the right hand side value takes precedence over the left hand side value.\n\nlhs\n\nLeft attribute set of the merge.\n\nrhs\n\nRight attribute set of the merge.\n\nExample 35. lib.attrsets.recursiveUpdate usage example\n\nrecursiveUpdate {\n  boot.loader.grub.enable = true;\n  boot.loader.grub.device = \"/dev/hda\";\n} {\n  boot.loader.grub.device = \"\";\n}\n\nreturns: {\n  boot.loader.grub.enable = true;\n  boot.loader.grub.device = \"\";\n}\n\n\n\n\nLocated at lib/attrsets.nix:878 in <nixpkgs>.\n\nlib.attrsets.matchAttrs\n\nType: matchAttrs :: AttrSet -> AttrSet -> Bool\n\nReturns true if the pattern is contained in the set. False otherwise.\n\npattern\n\nAttribute set structure to match\n\nattrs\n\nAttribute set to find patterns in\n\nExample 36. lib.attrsets.matchAttrs usage example\n\nmatchAttrs { cpu = {}; } { cpu = { bits = 64; }; }\n=> true\n\n\n\n\nLocated at lib/attrsets.nix:895 in <nixpkgs>.\n\nlib.attrsets.overrideExisting\n\nType: overrideExisting :: AttrSet -> AttrSet -> AttrSet\n\nOverride only the attributes that are already present in the old set useful for deep-overriding.\n\nold\n\nOriginal attribute set\n\nnew\n\nAttribute set with attributes to override in old.\n\nExample 37. lib.attrsets.overrideExisting usage example\n\noverrideExisting {} { a = 1; }\n=> {}\noverrideExisting { b = 2; } { a = 1; }\n=> { b = 2; }\noverrideExisting { a = 3; b = 2; } { a = 1; }\n=> { a = 1; b = 2; }\n\n\n\n\nLocated at lib/attrsets.nix:923 in <nixpkgs>.\n\nlib.attrsets.showAttrPath\n\nType: showAttrPath :: [String] -> String\n\nTurns a list of strings into a human-readable description of those strings represented as an attribute path. The result of this function is not intended to be machine-readable. Create a new attribute set with value set at the nested attribute location specified in attrPath.\n\npath\n\nAttribute path to render to a string\n\nExample 38. lib.attrsets.showAttrPath usage example\n\nshowAttrPath [ \"foo\" \"10\" \"bar\" ]\n=> \"foo.\\\"10\\\".bar\"\nshowAttrPath []\n=> \"<root attribute path>\"\n\n\n\n\nLocated at lib/attrsets.nix:945 in <nixpkgs>.\n\nlib.attrsets.getOutput\n\nType: getOutput :: String -> Derivation -> String\n\nGet a package output. If no output is found, fallback to .out and then to the default.\n\noutput\n\nFunction argument\n\npkg\n\nFunction argument\n\nExample 39. lib.attrsets.getOutput usage example\n\ngetOutput \"dev\" pkgs.openssl\n=> \"/nix/store/9rz8gxhzf8sw4kf2j2f1grr49w8zx5vj-openssl-1.0.1r-dev\"\n\n\n\n\nLocated at lib/attrsets.nix:962 in <nixpkgs>.\n\nlib.attrsets.getBin\n\nType: getBin :: Derivation -> String\n\nGet a package’s bin output. If the output does not exist, fallback to .out and then to the default.\n\nExample 40. lib.attrsets.getBin usage example\n\ngetBin pkgs.openssl\n=> \"/nix/store/9rz8gxhzf8sw4kf2j2f1grr49w8zx5vj-openssl-1.0.1r\"\n\n\n\n\nLocated at lib/attrsets.nix:977 in <nixpkgs>.\n\nlib.attrsets.getLib\n\nType: getLib :: Derivation -> String\n\nGet a package’s lib output. If the output does not exist, fallback to .out and then to the default.\n\nExample 41. lib.attrsets.getLib usage example\n\ngetLib pkgs.openssl\n=> \"/nix/store/9rz8gxhzf8sw4kf2j2f1grr49w8zx5vj-openssl-1.0.1r-lib\"\n\n\n\n\nLocated at lib/attrsets.nix:990 in <nixpkgs>.\n\nlib.attrsets.getDev\n\nType: getDev :: Derivation -> String\n\nGet a package’s dev output. If the output does not exist, fallback to .out and then to the default.\n\nExample 42. lib.attrsets.getDev usage example\n\ngetDev pkgs.openssl\n=> \"/nix/store/9rz8gxhzf8sw4kf2j2f1grr49w8zx5vj-openssl-1.0.1r-dev\"\n\n\n\n\nLocated at lib/attrsets.nix:1003 in <nixpkgs>.\n\nlib.attrsets.getMan\n\nType: getMan :: Derivation -> String\n\nGet a package’s man output. If the output does not exist, fallback to .out and then to the default.\n\nExample 43. lib.attrsets.getMan usage example\n\ngetMan pkgs.openssl\n=> \"/nix/store/9rz8gxhzf8sw4kf2j2f1grr49w8zx5vj-openssl-1.0.1r-man\"\n\n\n\n\nLocated at lib/attrsets.nix:1016 in <nixpkgs>.\n\nlib.attrsets.chooseDevOutputs\n\nType: chooseDevOutputs :: [Derivation] -> [String]\n\nPick the outputs of packages to place in buildInputs\n\ndrvs\n\nList of packages to pick dev outputs from\n\nLocated at lib/attrsets.nix:1023 in <nixpkgs>.\n\nlib.attrsets.recurseIntoAttrs\n\nType: recurseIntoAttrs :: AttrSet -> AttrSet\n\nMake various Nix tools consider the contents of the resulting attribute set when looking for what to build, find, etc.\n\nThis function only affects a single attribute set; it does not apply itself recursively for nested attribute sets.\n\nattrs\n\nAn attribute set to scan for derivations.\n\nExample 44. lib.attrsets.recurseIntoAttrs usage example\n\n{ pkgs ? import <nixpkgs> {} }:\n{\n  myTools = pkgs.lib.recurseIntoAttrs {\n    inherit (pkgs) hello figlet;\n  };\n}\n\n\n\n\nLocated at lib/attrsets.nix:1046 in <nixpkgs>.\n\nlib.attrsets.dontRecurseIntoAttrs\n\nType: dontRecurseIntoAttrs :: AttrSet -> AttrSet\n\nUndo the effect of recurseIntoAttrs.\n\nattrs\n\nAn attribute set to not scan for derivations.\n\nLocated at lib/attrsets.nix:1056 in <nixpkgs>.\n\nlib.attrsets.unionOfDisjoint\n\nType: unionOfDisjoint :: AttrSet -> AttrSet -> AttrSet\n\nunionOfDisjoint x y is equal to x // y // z where the attrnames in z are the intersection of the attrnames in x and y, and all values assert with an error message. This operator is commutative, unlike (//).\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nLocated at lib/attrsets.nix:1068 in <nixpkgs>.\n\nlib.strings: string manipulation functions \nlib.strings.concatStrings\n\nType: concatStrings :: [string] -> string\n\nConcatenate a list of strings.\n\nExample 45. lib.strings.concatStrings usage example\n\nconcatStrings [\"foo\" \"bar\"]\n=> \"foobar\"\n\n\n\n\nLocated at lib/strings.nix:50 in <nixpkgs>.\n\nlib.strings.concatMapStrings\n\nType: concatMapStrings :: (a -> string) -> [a] -> string\n\nMap a function over a list and concatenate the resulting strings.\n\nf\n\nFunction argument\n\nlist\n\nFunction argument\n\nExample 46. lib.strings.concatMapStrings usage example\n\nconcatMapStrings (x: \"a\" + x) [\"foo\" \"bar\"]\n=> \"afooabar\"\n\n\n\n\nLocated at lib/strings.nix:60 in <nixpkgs>.\n\nlib.strings.concatImapStrings\n\nType: concatImapStrings :: (int -> a -> string) -> [a] -> string\n\nLike concatMapStrings except that the f functions also gets the position as a parameter.\n\nf\n\nFunction argument\n\nlist\n\nFunction argument\n\nExample 47. lib.strings.concatImapStrings usage example\n\nconcatImapStrings (pos: x: \"${toString pos}-${x}\") [\"foo\" \"bar\"]\n=> \"1-foo2-bar\"\n\n\n\n\nLocated at lib/strings.nix:71 in <nixpkgs>.\n\nlib.strings.intersperse\n\nType: intersperse :: a -> [a] -> [a]\n\nPlace an element between each element of a list\n\nseparator\n\nSeparator to add between elements\n\nlist\n\nInput list\n\nExample 48. lib.strings.intersperse usage example\n\nintersperse \"/\" [\"usr\" \"local\" \"bin\"]\n=> [\"usr\" \"/\" \"local\" \"/\" \"bin\"].\n\n\n\n\nLocated at lib/strings.nix:81 in <nixpkgs>.\n\nlib.strings.concatStringsSep\n\nType: concatStringsSep :: string -> [string] -> string\n\nConcatenate a list of strings with a separator between each element\n\nExample 49. lib.strings.concatStringsSep usage example\n\nconcatStringsSep \"/\" [\"usr\" \"local\" \"bin\"]\n=> \"usr/local/bin\"\n\n\n\n\nLocated at lib/strings.nix:98 in <nixpkgs>.\n\nlib.strings.concatMapStringsSep\n\nType: concatMapStringsSep :: string -> (a -> string) -> [a] -> string\n\nMaps a function over a list of strings and then concatenates the result with the specified separator interspersed between elements.\n\nsep\n\nSeparator to add between elements\n\nf\n\nFunction to map over the list\n\nlist\n\nList of input strings\n\nExample 50. lib.strings.concatMapStringsSep usage example\n\nconcatMapStringsSep \"-\" (x: toUpper x)  [\"foo\" \"bar\" \"baz\"]\n=> \"FOO-BAR-BAZ\"\n\n\n\n\nLocated at lib/strings.nix:111 in <nixpkgs>.\n\nlib.strings.concatImapStringsSep\n\nType: concatIMapStringsSep :: string -> (int -> a -> string) -> [a] -> string\n\nSame as concatMapStringsSep, but the mapping function additionally receives the position of its argument.\n\nsep\n\nSeparator to add between elements\n\nf\n\nFunction that receives elements and their positions\n\nlist\n\nList of input strings\n\nExample 51. lib.strings.concatImapStringsSep usage example\n\nconcatImapStringsSep \"-\" (pos: x: toString (x / pos)) [ 6 6 6 ]\n=> \"6-3-2\"\n\n\n\n\nLocated at lib/strings.nix:128 in <nixpkgs>.\n\nlib.strings.concatLines\n\nType: concatLines :: [string] -> string\n\nConcatenate a list of strings, adding a newline at the end of each one. Defined as concatMapStrings (s: s + \"\\n\").\n\nExample 52. lib.strings.concatLines usage example\n\nconcatLines [ \"foo\" \"bar\" ]\n=> \"foo\\nbar\\n\"\n\n\n\n\nLocated at lib/strings.nix:145 in <nixpkgs>.\n\nlib.strings.replicate\n\nType: replicate :: int -> string -> string\n\nReplicate a string n times, and concatenate the parts into a new string.\n\nn\n\nFunction argument\n\ns\n\nFunction argument\n\nExample 53. lib.strings.replicate usage example\n\nreplicate 3 \"v\"\n=> \"vvv\"\nreplicate 5 \"hello\"\n=> \"hellohellohellohellohello\"\n\n\n\n\nLocated at lib/strings.nix:159 in <nixpkgs>.\n\nlib.strings.makeSearchPath\n\nType: makeSearchPath :: string -> [string] -> string\n\nConstruct a Unix-style, colon-separated search path consisting of the given subDir appended to each of the given paths.\n\nsubDir\n\nDirectory name to append\n\npaths\n\nList of base paths\n\nExample 54. lib.strings.makeSearchPath usage example\n\nmakeSearchPath \"bin\" [\"/root\" \"/usr\" \"/usr/local\"]\n=> \"/root/bin:/usr/bin:/usr/local/bin\"\nmakeSearchPath \"bin\" [\"\"]\n=> \"/bin\"\n\n\n\n\nLocated at lib/strings.nix:172 in <nixpkgs>.\n\nlib.strings.makeSearchPathOutput\n\nType: string -> string -> [package] -> string\n\nConstruct a Unix-style search path by appending the given subDir to the specified output of each of the packages. If no output by the given name is found, fallback to .out and then to the default.\n\noutput\n\nPackage output to use\n\nsubDir\n\nDirectory name to append\n\npkgs\n\nList of packages\n\nExample 55. lib.strings.makeSearchPathOutput usage example\n\nmakeSearchPathOutput \"dev\" \"bin\" [ pkgs.openssl pkgs.zlib ]\n=> \"/nix/store/9rz8gxhzf8sw4kf2j2f1grr49w8zx5vj-openssl-1.0.1r-dev/bin:/nix/store/wwh7mhwh269sfjkm6k5665b5kgp7jrk2-zlib-1.2.8/bin\"\n\n\n\n\nLocated at lib/strings.nix:190 in <nixpkgs>.\n\nlib.strings.makeLibraryPath\n\nConstruct a library search path (such as RPATH) containing the libraries for a set of packages\n\nExample 56. lib.strings.makeLibraryPath usage example\n\nmakeLibraryPath [ \"/usr\" \"/usr/local\" ]\n=> \"/usr/lib:/usr/local/lib\"\npkgs = import <nixpkgs> { }\nmakeLibraryPath [ pkgs.openssl pkgs.zlib ]\n=> \"/nix/store/9rz8gxhzf8sw4kf2j2f1grr49w8zx5vj-openssl-1.0.1r/lib:/nix/store/wwh7mhwh269sfjkm6k5665b5kgp7jrk2-zlib-1.2.8/lib\"\n\n\n\n\nLocated at lib/strings.nix:208 in <nixpkgs>.\n\nlib.strings.makeBinPath\n\nConstruct a binary search path (such as $PATH) containing the binaries for a set of packages.\n\nExample 57. lib.strings.makeBinPath usage example\n\nmakeBinPath [\"/root\" \"/usr\" \"/usr/local\"]\n=> \"/root/bin:/usr/bin:/usr/local/bin\"\n\n\n\n\nLocated at lib/strings.nix:217 in <nixpkgs>.\n\nlib.strings.normalizePath\n\nType: normalizePath :: string -> string\n\nNormalize path, removing extraneous /s\n\ns\n\nFunction argument\n\nExample 58. lib.strings.normalizePath usage example\n\nnormalizePath \"/a//b///c/\"\n=> \"/a/b/c/\"\n\n\n\n\nLocated at lib/strings.nix:227 in <nixpkgs>.\n\nlib.strings.optionalString\n\nType: optionalString :: bool -> string -> string\n\nDepending on the boolean `cond’, return either the given string or the empty string. Useful to concatenate against a bigger string.\n\ncond\n\nCondition\n\nstring\n\nString to return if condition is true\n\nExample 59. lib.strings.optionalString usage example\n\noptionalString true \"some-string\"\n=> \"some-string\"\noptionalString false \"some-string\"\n=> \"\"\n\n\n\n\nLocated at lib/strings.nix:253 in <nixpkgs>.\n\nlib.strings.hasPrefix\n\nType: hasPrefix :: string -> string -> bool\n\nDetermine whether a string has given prefix.\n\npref\n\nPrefix to check for\n\nstr\n\nInput string\n\nExample 60. lib.strings.hasPrefix usage example\n\nhasPrefix \"foo\" \"foobar\"\n=> true\nhasPrefix \"foo\" \"barfoo\"\n=> false\n\n\n\n\nLocated at lib/strings.nix:269 in <nixpkgs>.\n\nlib.strings.hasSuffix\n\nType: hasSuffix :: string -> string -> bool\n\nDetermine whether a string has given suffix.\n\nsuffix\n\nSuffix to check for\n\ncontent\n\nInput string\n\nExample 61. lib.strings.hasSuffix usage example\n\nhasSuffix \"foo\" \"foobar\"\n=> false\nhasSuffix \"foo\" \"barfoo\"\n=> true\n\n\n\n\nLocated at lib/strings.nix:296 in <nixpkgs>.\n\nlib.strings.hasInfix\n\nType: hasInfix :: string -> string -> bool\n\nDetermine whether a string contains the given infix\n\ninfix\n\nFunction argument\n\ncontent\n\nFunction argument\n\nExample 62. lib.strings.hasInfix usage example\n\nhasInfix \"bc\" \"abcd\"\n=> true\nhasInfix \"ab\" \"abcd\"\n=> true\nhasInfix \"cd\" \"abcd\"\n=> true\nhasInfix \"foo\" \"abcd\"\n=> false\n\n\n\n\nLocated at lib/strings.nix:333 in <nixpkgs>.\n\nlib.strings.stringToCharacters\n\nType: stringToCharacters :: string -> [string]\n\nConvert a string to a list of characters (i.e. singleton strings). This allows you to, e.g., map a function over each character. However, note that this will likely be horribly inefficient; Nix is not a general purpose programming language. Complex string manipulations should, if appropriate, be done in a derivation. Also note that Nix treats strings as a list of bytes and thus doesn’t handle unicode.\n\ns\n\nFunction argument\n\nExample 63. lib.strings.stringToCharacters usage example\n\nstringToCharacters \"\"\n=> [ ]\nstringToCharacters \"abc\"\n=> [ \"a\" \"b\" \"c\" ]\nstringToCharacters \"🦄\"\n=> [ \"�\" \"�\" \"�\" \"�\" ]\n\n\n\n\nLocated at lib/strings.nix:363 in <nixpkgs>.\n\nlib.strings.stringAsChars\n\nType: stringAsChars :: (string -> string) -> string -> string\n\nManipulate a string character by character and replace them by strings before concatenating the results.\n\nf\n\nFunction to map over each individual character\n\ns\n\nInput string\n\nExample 64. lib.strings.stringAsChars usage example\n\nstringAsChars (x: if x == \"a\" then \"i\" else x) \"nax\"\n=> \"nix\"\n\n\n\n\nLocated at lib/strings.nix:375 in <nixpkgs>.\n\nlib.strings.charToInt\n\nType: charToInt :: string -> int\n\nConvert char to ascii value, must be in printable range\n\nc\n\nFunction argument\n\nExample 65. lib.strings.charToInt usage example\n\ncharToInt \"A\"\n=> 65\ncharToInt \"(\"\n=> 40\n\n\n\n\nLocated at lib/strings.nix:394 in <nixpkgs>.\n\nlib.strings.escape\n\nType: escape :: [string] -> string -> string\n\nEscape occurrence of the elements of list in string by prefixing it with a backslash.\n\nlist\n\nFunction argument\n\nExample 66. lib.strings.escape usage example\n\nescape [\"(\" \")\"] \"(foo)\"\n=> \"\\\\(foo\\\\)\"\n\n\n\n\nLocated at lib/strings.nix:405 in <nixpkgs>.\n\nlib.strings.escapeC\n\nType: escapeC = [string] -> string -> string\n\nEscape occurrence of the element of list in string by converting to its ASCII value and prefixing it with \\x. Only works for printable ascii characters.\n\nlist\n\nFunction argument\n\nExample 67. lib.strings.escapeC usage example\n\nescapeC [\" \"] \"foo bar\"\n=> \"foo\\\\x20bar\"\n\n\n\n\nLocated at lib/strings.nix:418 in <nixpkgs>.\n\nlib.strings.escapeURL\n\nType: escapeURL :: string -> string\n\nEscape the string so it can be safely placed inside a URL query.\n\nExample 68. lib.strings.escapeURL usage example\n\nescapeURL \"foo/bar baz\"\n=> \"foo%2Fbar%20baz\"\n\n\n\n\nLocated at lib/strings.nix:429 in <nixpkgs>.\n\nlib.strings.escapeShellArg\n\nType: escapeShellArg :: string -> string\n\nQuote string to be used safely within the Bourne shell.\n\narg\n\nFunction argument\n\nExample 69. lib.strings.escapeShellArg usage example\n\nescapeShellArg \"esc'ape\\nme\"\n=> \"'esc'\\\\''ape\\nme'\"\n\n\n\n\nLocated at lib/strings.nix:443 in <nixpkgs>.\n\nlib.strings.escapeShellArgs\n\nType: escapeShellArgs :: [string] -> string\n\nQuote all arguments to be safely passed to the Bourne shell.\n\nExample 70. lib.strings.escapeShellArgs usage example\n\nescapeShellArgs [\"one\" \"two three\" \"four'five\"]\n=> \"'one' 'two three' 'four'\\\\''five'\"\n\n\n\n\nLocated at lib/strings.nix:453 in <nixpkgs>.\n\nlib.strings.isValidPosixName\n\nType: string -> bool\n\nTest whether the given name is a valid POSIX shell variable name.\n\nname\n\nFunction argument\n\nExample 71. lib.strings.isValidPosixName usage example\n\nisValidPosixName \"foo_bar000\"\n=> true\nisValidPosixName \"0-bad.jpg\"\n=> false\n\n\n\n\nLocated at lib/strings.nix:465 in <nixpkgs>.\n\nlib.strings.toShellVar\n\nType: string -> (string | listOf string | attrsOf string) -> string\n\nTranslate a Nix value into a shell variable declaration, with proper escaping.\n\nThe value can be a string (mapped to a regular variable), a list of strings (mapped to a Bash-style array) or an attribute set of strings (mapped to a Bash-style associative array). Note that “string” includes string-coercible values like paths or derivations.\n\nStrings are translated into POSIX sh-compatible code; lists and attribute sets assume a shell that understands Bash syntax (e.g. Bash or ZSH).\n\nname\n\nFunction argument\n\nvalue\n\nFunction argument\n\nExample 72. lib.strings.toShellVar usage example\n\n''\n  ${toShellVar \"foo\" \"some string\"}\n  [[ \"$foo\" == \"some string\" ]]\n''\n\n\n\n\nLocated at lib/strings.nix:485 in <nixpkgs>.\n\nlib.strings.toShellVars\n\nType: attrsOf (string | listOf string | attrsOf string) -> string\n\nTranslate an attribute set into corresponding shell variable declarations using toShellVar.\n\nvars\n\nFunction argument\n\nExample 73. lib.strings.toShellVars usage example\n\nlet\n  foo = \"value\";\n  bar = foo;\nin ''\n  ${toShellVars { inherit foo bar; }}\n  [[ \"$foo\" == \"$bar\" ]]\n''\n\n\n\n\nLocated at lib/strings.nix:513 in <nixpkgs>.\n\nlib.strings.escapeNixString\n\nType: string -> string\n\nTurn a string into a Nix expression representing that string\n\ns\n\nFunction argument\n\nExample 74. lib.strings.escapeNixString usage example\n\nescapeNixString \"hello\\${}\\n\"\n=> \"\\\"hello\\\\\\${}\\\\n\\\"\"\n\n\n\n\nLocated at lib/strings.nix:523 in <nixpkgs>.\n\nlib.strings.escapeRegex\n\nType: string -> string\n\nTurn a string into an exact regular expression\n\nExample 75. lib.strings.escapeRegex usage example\n\nescapeRegex \"[^a-z]*\"\n=> \"\\\\[\\\\^a-z]\\\\*\"\n\n\n\n\nLocated at lib/strings.nix:533 in <nixpkgs>.\n\nlib.strings.escapeNixIdentifier\n\nType: string -> string\n\nQuotes a string if it can’t be used as an identifier directly.\n\ns\n\nFunction argument\n\nExample 76. lib.strings.escapeNixIdentifier usage example\n\nescapeNixIdentifier \"hello\"\n=> \"hello\"\nescapeNixIdentifier \"0abc\"\n=> \"\\\"0abc\\\"\"\n\n\n\n\nLocated at lib/strings.nix:545 in <nixpkgs>.\n\nlib.strings.escapeXML\n\nType: string -> string\n\nEscapes a string such that it is safe to include verbatim in an XML document.\n\nExample 77. lib.strings.escapeXML usage example\n\nescapeXML ''\"test\" 'test' < & >''\n=> \"&quot;test&quot; &apos;test&apos; &lt; &amp; &gt;\"\n\n\n\n\nLocated at lib/strings.nix:559 in <nixpkgs>.\n\nlib.strings.toLower\n\nType: toLower :: string -> string\n\nConverts an ASCII string to lower-case.\n\nExample 78. lib.strings.toLower usage example\n\ntoLower \"HOME\"\n=> \"home\"\n\n\n\n\nLocated at lib/strings.nix:578 in <nixpkgs>.\n\nlib.strings.toUpper\n\nType: toUpper :: string -> string\n\nConverts an ASCII string to upper-case.\n\nExample 79. lib.strings.toUpper usage example\n\ntoUpper \"home\"\n=> \"HOME\"\n\n\n\n\nLocated at lib/strings.nix:588 in <nixpkgs>.\n\nlib.strings.addContextFrom\n\nAppends string context from another string. This is an implementation detail of Nix and should be used carefully.\n\nStrings in Nix carry an invisible context which is a list of strings representing store paths. If the string is later used in a derivation attribute, the derivation will properly populate the inputDrvs and inputSrcs.\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nExample 80. lib.strings.addContextFrom usage example\n\npkgs = import <nixpkgs> { };\naddContextFrom pkgs.coreutils \"bar\"\n=> \"bar\"\n\n\n\n\nLocated at lib/strings.nix:603 in <nixpkgs>.\n\nlib.strings.splitString\n\nCut a string with a separator and produces a list of strings which were separated by this separator.\n\nsep\n\nFunction argument\n\ns\n\nFunction argument\n\nExample 81. lib.strings.splitString usage example\n\nsplitString \".\" \"foo.bar.baz\"\n=> [ \"foo\" \"bar\" \"baz\" ]\nsplitString \"/\" \"/usr/local/bin\"\n=> [ \"\" \"usr\" \"local\" \"bin\" ]\n\n\n\n\nLocated at lib/strings.nix:614 in <nixpkgs>.\n\nlib.strings.removePrefix\n\nType: string -> string -> string\n\nReturn a string without the specified prefix, if the prefix matches.\n\nprefix\n\nPrefix to remove if it matches\n\nstr\n\nInput string\n\nExample 82. lib.strings.removePrefix usage example\n\nremovePrefix \"foo.\" \"foo.bar.baz\"\n=> \"bar.baz\"\nremovePrefix \"xxx\" \"foo.bar.baz\"\n=> \"foo.bar.baz\"\n\n\n\n\nLocated at lib/strings.nix:630 in <nixpkgs>.\n\nlib.strings.removeSuffix\n\nType: string -> string -> string\n\nReturn a string without the specified suffix, if the suffix matches.\n\nsuffix\n\nSuffix to remove if it matches\n\nstr\n\nInput string\n\nExample 83. lib.strings.removeSuffix usage example\n\nremoveSuffix \"front\" \"homefront\"\n=> \"home\"\nremoveSuffix \"xxx\" \"homefront\"\n=> \"homefront\"\n\n\n\n\nLocated at lib/strings.nix:663 in <nixpkgs>.\n\nlib.strings.versionOlder\n\nReturn true if string v1 denotes a version older than v2.\n\nv1\n\nFunction argument\n\nv2\n\nFunction argument\n\nExample 84. lib.strings.versionOlder usage example\n\nversionOlder \"1.1\" \"1.2\"\n=> true\nversionOlder \"1.1\" \"1.1\"\n=> false\n\n\n\n\nLocated at lib/strings.nix:694 in <nixpkgs>.\n\nlib.strings.versionAtLeast\n\nReturn true if string v1 denotes a version equal to or newer than v2.\n\nv1\n\nFunction argument\n\nv2\n\nFunction argument\n\nExample 85. lib.strings.versionAtLeast usage example\n\nversionAtLeast \"1.1\" \"1.0\"\n=> true\nversionAtLeast \"1.1\" \"1.1\"\n=> true\nversionAtLeast \"1.1\" \"1.2\"\n=> false\n\n\n\n\nLocated at lib/strings.nix:706 in <nixpkgs>.\n\nlib.strings.getName\n\nThis function takes an argument that’s either a derivation or a derivation’s “name” attribute and extracts the name part from that argument.\n\nx\n\nFunction argument\n\nExample 86. lib.strings.getName usage example\n\ngetName \"youtube-dl-2016.01.01\"\n=> \"youtube-dl\"\ngetName pkgs.youtube-dl\n=> \"youtube-dl\"\n\n\n\n\nLocated at lib/strings.nix:718 in <nixpkgs>.\n\nlib.strings.getVersion\n\nThis function takes an argument that’s either a derivation or a derivation’s “name” attribute and extracts the version part from that argument.\n\nx\n\nFunction argument\n\nExample 87. lib.strings.getVersion usage example\n\ngetVersion \"youtube-dl-2016.01.01\"\n=> \"2016.01.01\"\ngetVersion pkgs.youtube-dl\n=> \"2016.01.01\"\n\n\n\n\nLocated at lib/strings.nix:735 in <nixpkgs>.\n\nlib.strings.nameFromURL\n\nExtract name with version from URL. Ask for separator which is supposed to start extension.\n\nurl\n\nFunction argument\n\nsep\n\nFunction argument\n\nExample 88. lib.strings.nameFromURL usage example\n\nnameFromURL \"https://nixos.org/releases/nix/nix-1.7/nix-1.7-x86_64-linux.tar.bz2\" \"-\"\n=> \"nix\"\nnameFromURL \"https://nixos.org/releases/nix/nix-1.7/nix-1.7-x86_64-linux.tar.bz2\" \"_\"\n=> \"nix-1.7-x86\"\n\n\n\n\nLocated at lib/strings.nix:751 in <nixpkgs>.\n\nlib.strings.cmakeOptionType\n\nType:\n\ncmakeOptionType :: string -> string -> string -> string\n\n@param feature The feature to be set\n@param type The type of the feature to be set, as described in\n            https://cmake.org/cmake/help/latest/command/set.html\n            the possible values (case insensitive) are:\n            BOOL FILEPATH PATH STRING INTERNAL\n@param value The desired value\n\n\nCreate a “-D<feature>:<type>=<value>” string that can be passed to typical CMake invocations.\n\ntype\n\nFunction argument\n\nfeature\n\nFunction argument\n\nvalue\n\nFunction argument\n\nExample 89. lib.strings.cmakeOptionType usage example\n\ncmakeOptionType \"string\" \"ENGINE\" \"sdl2\"\n=> \"-DENGINE:STRING=sdl2\"\n\n\n\n\nLocated at lib/strings.nix:774 in <nixpkgs>.\n\nlib.strings.cmakeBool\n\nType:\n\ncmakeBool :: string -> bool -> string\n\n@param condition The condition to be made true or false\n@param flag The controlling flag of the condition\n\n\nCreate a -D<condition>={TRUE,FALSE} string that can be passed to typical CMake invocations.\n\ncondition\n\nFunction argument\n\nflag\n\nFunction argument\n\nExample 90. lib.strings.cmakeBool usage example\n\ncmakeBool \"ENABLE_STATIC_LIBS\" false\n=> \"-DENABLESTATIC_LIBS:BOOL=FALSE\"\n\n\n\n\nLocated at lib/strings.nix:793 in <nixpkgs>.\n\nlib.strings.cmakeFeature\n\nType:\n\ncmakeFeature :: string -> string -> string\n\n@param condition The condition to be made true or false\n@param flag The controlling flag of the condition\n\n\nCreate a -D<feature>:STRING=<value> string that can be passed to typical CMake invocations. This is the most typical usage, so it deserves a special case.\n\nfeature\n\nFunction argument\n\nvalue\n\nFunction argument\n\nExample 91. lib.strings.cmakeFeature usage example\n\ncmakeFeature \"MODULES\" \"badblock\"\n=> \"-DMODULES:STRING=badblock\"\n\n\n\n\nLocated at lib/strings.nix:811 in <nixpkgs>.\n\nlib.strings.mesonOption\n\nType:\n\nmesonOption :: string -> string -> string\n\n@param feature The feature to be set\n@param value The desired value\n\n\nCreate a -D<feature>=<value> string that can be passed to typical Meson invocations.\n\nfeature\n\nFunction argument\n\nvalue\n\nFunction argument\n\nExample 92. lib.strings.mesonOption usage example\n\nmesonOption \"engine\" \"opengl\"\n=> \"-Dengine=opengl\"\n\n\n\n\nLocated at lib/strings.nix:828 in <nixpkgs>.\n\nlib.strings.mesonBool\n\nType:\n\nmesonBool :: string -> bool -> string\n\n@param condition The condition to be made true or false\n@param flag The controlling flag of the condition\n\n\nCreate a -D<condition>={true,false} string that can be passed to typical Meson invocations.\n\ncondition\n\nFunction argument\n\nflag\n\nFunction argument\n\nExample 93. lib.strings.mesonBool usage example\n\nmesonBool \"hardened\" true\n=> \"-Dhardened=true\"\nmesonBool \"static\" false\n=> \"-Dstatic=false\"\n\n\n\n\nLocated at lib/strings.nix:847 in <nixpkgs>.\n\nlib.strings.mesonEnable\n\nType:\n\nmesonEnable :: string -> bool -> string\n\n@param feature The feature to be enabled or disabled\n@param flag The controlling flag\n\n\nCreate a -D<feature>={enabled,disabled} string that can be passed to typical Meson invocations.\n\nfeature\n\nFunction argument\n\nflag\n\nFunction argument\n\nExample 94. lib.strings.mesonEnable usage example\n\nmesonEnable \"docs\" true\n=> \"-Ddocs=enabled\"\nmesonEnable \"savage\" false\n=> \"-Dsavage=disabled\"\n\n\n\n\nLocated at lib/strings.nix:866 in <nixpkgs>.\n\nlib.strings.enableFeature\n\nCreate an --{enable,disable}-<feature> string that can be passed to standard GNU Autoconf scripts.\n\nflag\n\nFunction argument\n\nfeature\n\nFunction argument\n\nExample 95. lib.strings.enableFeature usage example\n\nenableFeature true \"shared\"\n=> \"--enable-shared\"\nenableFeature false \"shared\"\n=> \"--disable-shared\"\n\n\n\n\nLocated at lib/strings.nix:880 in <nixpkgs>.\n\nlib.strings.enableFeatureAs\n\nCreate an --{enable-<feature>=<value>,disable-<feature>} string that can be passed to standard GNU Autoconf scripts.\n\nflag\n\nFunction argument\n\nfeature\n\nFunction argument\n\nvalue\n\nFunction argument\n\nExample 96. lib.strings.enableFeatureAs usage example\n\nenableFeatureAs true \"shared\" \"foo\"\n=> \"--enable-shared=foo\"\nenableFeatureAs false \"shared\" (throw \"ignored\")\n=> \"--disable-shared\"\n\n\n\n\nLocated at lib/strings.nix:894 in <nixpkgs>.\n\nlib.strings.withFeature\n\nCreate an --{with,without}-<feature> string that can be passed to standard GNU Autoconf scripts.\n\nflag\n\nFunction argument\n\nfeature\n\nFunction argument\n\nExample 97. lib.strings.withFeature usage example\n\nwithFeature true \"shared\"\n=> \"--with-shared\"\nwithFeature false \"shared\"\n=> \"--without-shared\"\n\n\n\n\nLocated at lib/strings.nix:906 in <nixpkgs>.\n\nlib.strings.withFeatureAs\n\nCreate an --{with-<feature>=<value>,without-<feature>} string that can be passed to standard GNU Autoconf scripts.\n\nflag\n\nFunction argument\n\nfeature\n\nFunction argument\n\nvalue\n\nFunction argument\n\nExample 98. lib.strings.withFeatureAs usage example\n\nwithFeatureAs true \"shared\" \"foo\"\n=> \"--with-shared=foo\"\nwithFeatureAs false \"shared\" (throw \"ignored\")\n=> \"--without-shared\"\n\n\n\n\nLocated at lib/strings.nix:919 in <nixpkgs>.\n\nlib.strings.fixedWidthString\n\nType: fixedWidthString :: int -> string -> string -> string\n\nCreate a fixed width string with additional prefix to match required width.\n\nThis function will fail if the input string is longer than the requested length.\n\nwidth\n\nFunction argument\n\nfiller\n\nFunction argument\n\nstr\n\nFunction argument\n\nExample 99. lib.strings.fixedWidthString usage example\n\nfixedWidthString 5 \"0\" (toString 15)\n=> \"00015\"\n\n\n\n\nLocated at lib/strings.nix:934 in <nixpkgs>.\n\nlib.strings.fixedWidthNumber\n\nFormat a number adding leading zeroes up to fixed width.\n\nwidth\n\nFunction argument\n\nn\n\nFunction argument\n\nExample 100. lib.strings.fixedWidthNumber usage example\n\nfixedWidthNumber 5 15\n=> \"00015\"\n\n\n\n\nLocated at lib/strings.nix:951 in <nixpkgs>.\n\nlib.strings.floatToString\n\nConvert a float to a string, but emit a warning when precision is lost during the conversion\n\nfloat\n\nFunction argument\n\nExample 101. lib.strings.floatToString usage example\n\nfloatToString 0.000001\n=> \"0.000001\"\nfloatToString 0.0000001\n=> trace: warning: Imprecise conversion from float to string 0.000000\n   \"0.000000\"\n\n\n\n\nLocated at lib/strings.nix:963 in <nixpkgs>.\n\nlib.strings.isCoercibleToString\n\nSoft-deprecated function. While the original implementation is available as isConvertibleWithToString, consider using isStringLike instead, if suitable.\n\nLocated at lib/strings.nix:971 in <nixpkgs>.\n\nlib.strings.isConvertibleWithToString\n\nCheck whether a list or other value can be passed to toString.\n\nMany types of value are coercible to string this way, including int, float, null, bool, list of similarly coercible values.\n\nx\n\nFunction argument\n\nLocated at lib/strings.nix:980 in <nixpkgs>.\n\nlib.strings.isStringLike\n\nCheck whether a value can be coerced to a string. The value must be a string, path, or attribute set.\n\nString-like values can be used without explicit conversion in string interpolations and in most functions that expect a string.\n\nx\n\nFunction argument\n\nLocated at lib/strings.nix:991 in <nixpkgs>.\n\nlib.strings.isStorePath\n\nCheck whether a value is a store path.\n\nx\n\nFunction argument\n\nExample 102. lib.strings.isStorePath usage example\n\nisStorePath \"/nix/store/d945ibfx9x185xf04b890y4f9g3cbb63-python-2.7.11/bin/python\"\n=> false\nisStorePath \"/nix/store/d945ibfx9x185xf04b890y4f9g3cbb63-python-2.7.11\"\n=> true\nisStorePath pkgs.python\n=> true\nisStorePath [] || isStorePath 42 || isStorePath {} || …\n=> false\n\n\n\n\nLocated at lib/strings.nix:1009 in <nixpkgs>.\n\nlib.strings.toInt\n\nType: string -> int\n\nParse a string as an int. Does not support parsing of integers with preceding zero due to ambiguity between zero-padded and octal numbers. See toIntBase10.\n\nstr\n\nFunction argument\n\nExample 103. lib.strings.toInt usage example\n\ntoInt \"1337\"\n=> 1337\n\ntoInt \"-4\"\n=> -4\n\ntoInt \" 123 \"\n=> 123\n\ntoInt \"00024\"\n=> error: Ambiguity in interpretation of 00024 between octal and zero padded integer.\n\ntoInt \"3.14\"\n=> error: floating point JSON numbers are not supported\n\n\n\n\nLocated at lib/strings.nix:1039 in <nixpkgs>.\n\nlib.strings.toIntBase10\n\nType: string -> int\n\nParse a string as a base 10 int. This supports parsing of zero-padded integers.\n\nstr\n\nFunction argument\n\nExample 104. lib.strings.toIntBase10 usage example\n\ntoIntBase10 \"1337\"\n=> 1337\n\ntoIntBase10 \"-4\"\n=> -4\n\ntoIntBase10 \" 123 \"\n=> 123\n\ntoIntBase10 \"00024\"\n=> 24\n\ntoIntBase10 \"3.14\"\n=> error: floating point JSON numbers are not supported\n\n\n\n\nLocated at lib/strings.nix:1090 in <nixpkgs>.\n\nlib.strings.readPathsFromFile\n\nRead a list of paths from file, relative to the rootPath. Lines beginning with # are treated as comments and ignored. Whitespace is significant.\n\nNOTE: This function is not performant and should be avoided.\n\nExample 105. lib.strings.readPathsFromFile usage example\n\nreadPathsFromFile /prefix\n  ./pkgs/development/libraries/qt-5/5.4/qtbase/series\n=> [ \"/prefix/dlopen-resolv.patch\" \"/prefix/tzdir.patch\"\n     \"/prefix/dlopen-libXcursor.patch\" \"/prefix/dlopen-openssl.patch\"\n     \"/prefix/dlopen-dbus.patch\" \"/prefix/xdg-config-dirs.patch\"\n     \"/prefix/nix-profiles-library-paths.patch\"\n     \"/prefix/compose-search-path.patch\" ]\n\n\n\n\nLocated at lib/strings.nix:1133 in <nixpkgs>.\n\nlib.strings.fileContents\n\nType: fileContents :: path -> string\n\nRead the contents of a file removing the trailing \\n\n\nfile\n\nFunction argument\n\nExample 106. lib.strings.fileContents usage example\n\n$ echo \"1.0\" > ./version\n\nfileContents ./version\n=> \"1.0\"\n\n\n\n\nLocated at lib/strings.nix:1153 in <nixpkgs>.\n\nlib.strings.sanitizeDerivationName\n\nType: sanitizeDerivationName :: String -> String\n\nCreates a valid derivation name from a potentially invalid one.\n\nExample 107. lib.strings.sanitizeDerivationName usage example\n\nsanitizeDerivationName \"../hello.bar # foo\"\n=> \"-hello.bar-foo\"\nsanitizeDerivationName \"\"\n=> \"unknown\"\nsanitizeDerivationName pkgs.hello\n=> \"-nix-store-2g75chlbpxlrqn15zlby2dfh8hr9qwbk-hello-2.10\"\n\n\n\n\nLocated at lib/strings.nix:1168 in <nixpkgs>.\n\nlib.strings.levenshtein\n\nType: levenshtein :: string -> string -> int\n\nComputes the Levenshtein distance between two strings. Complexity O(n*m) where n and m are the lengths of the strings. Algorithm adjusted from https://stackoverflow.com/a/9750974/6605742\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nExample 108. lib.strings.levenshtein usage example\n\nlevenshtein \"foo\" \"foo\"\n=> 0\nlevenshtein \"book\" \"hook\"\n=> 1\nlevenshtein \"hello\" \"Heyo\"\n=> 3\n\n\n\n\nLocated at lib/strings.nix:1207 in <nixpkgs>.\n\nlib.strings.commonPrefixLength\n\nReturns the length of the prefix common to both strings.\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nLocated at lib/strings.nix:1228 in <nixpkgs>.\n\nlib.strings.commonSuffixLength\n\nReturns the length of the suffix common to both strings.\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nLocated at lib/strings.nix:1236 in <nixpkgs>.\n\nlib.strings.levenshteinAtMost\n\nType: levenshteinAtMost :: int -> string -> string -> bool\n\nReturns whether the levenshtein distance between two strings is at most some value Complexity is O(min(n,m)) for k <= 2 and O(n*m) otherwise\n\nExample 109. lib.strings.levenshteinAtMost usage example\n\nlevenshteinAtMost 0 \"foo\" \"foo\"\n=> true\nlevenshteinAtMost 1 \"foo\" \"boa\"\n=> false\nlevenshteinAtMost 2 \"foo\" \"boa\"\n=> true\nlevenshteinAtMost 2 \"This is a sentence\" \"this is a sentense.\"\n=> false\nlevenshteinAtMost 3 \"This is a sentence\" \"this is a sentense.\"\n=> true\n\n\n\n\nLocated at lib/strings.nix:1260 in <nixpkgs>.\n\nlib.versions: version string functions \nlib.versions.splitVersion\n\nBreak a version string into its component parts.\n\nExample 110. lib.versions.splitVersion usage example\n\nsplitVersion \"1.2.3\"\n=> [\"1\" \"2\" \"3\"]\n\n\n\n\nLocated at lib/versions.nix:12 in <nixpkgs>.\n\nlib.versions.major\n\nGet the major version string from a string.\n\nv\n\nFunction argument\n\nExample 111. lib.versions.major usage example\n\nmajor \"1.2.3\"\n=> \"1\"\n\n\n\n\nLocated at lib/versions.nix:20 in <nixpkgs>.\n\nlib.versions.minor\n\nGet the minor version string from a string.\n\nv\n\nFunction argument\n\nExample 112. lib.versions.minor usage example\n\nminor \"1.2.3\"\n=> \"2\"\n\n\n\n\nLocated at lib/versions.nix:28 in <nixpkgs>.\n\nlib.versions.patch\n\nGet the patch version string from a string.\n\nv\n\nFunction argument\n\nExample 113. lib.versions.patch usage example\n\npatch \"1.2.3\"\n=> \"3\"\n\n\n\n\nLocated at lib/versions.nix:36 in <nixpkgs>.\n\nlib.versions.majorMinor\n\nGet string of the first two parts (major and minor) of a version string.\n\nv\n\nFunction argument\n\nExample 114. lib.versions.majorMinor usage example\n\nmajorMinor \"1.2.3\"\n=> \"1.2\"\n\n\n\n\nLocated at lib/versions.nix:45 in <nixpkgs>.\n\nlib.versions.pad\n\nPad a version string with zeros to match the given number of components.\n\nn\n\nFunction argument\n\nversion\n\nFunction argument\n\nExample 115. lib.versions.pad usage example\n\npad 3 \"1.2\"\n=> \"1.2.0\"\npad 3 \"1.3-rc1\"\n=> \"1.3.0-rc1\"\npad 3 \"1.2.3.4\"\n=> \"1.2.3\"\n\n\n\n\nLocated at lib/versions.nix:59 in <nixpkgs>.\n\nlib.trivial: miscellaneous functions \nlib.trivial.id\n\nType: id :: a -> a\n\nThe identity function For when you need a function that does “nothing”.\n\nx\n\nThe value to return\n\nLocated at lib/trivial.nix:12 in <nixpkgs>.\n\nlib.trivial.const\n\nType: const :: a -> b -> a\n\nThe constant function\n\nIgnores the second argument. If called with only one argument, constructs a function that always returns a static value.\n\nx\n\nValue to return\n\ny\n\nValue to ignore\n\nExample 116. lib.trivial.const usage example\n\nlet f = const 5; in f 10\n=> 5\n\n\n\n\nLocated at lib/trivial.nix:26 in <nixpkgs>.\n\nlib.trivial.pipe\n\nType: pipe :: a -> [<functions>] -> <return type of last function>\n\nPipes a value through a list of functions, left to right.\n\nval\n\nFunction argument\n\nfunctions\n\nFunction argument\n\nExample 117. lib.trivial.pipe usage example\n\npipe 2 [\n    (x: x + 2)  # 2 + 2 = 4\n    (x: x * 2)  # 4 * 2 = 8\n  ]\n  => 8\n\n  # ideal to do text transformations\n  pipe [ \"a/b\" \"a/c\" ] [\n\n    # create the cp command\n    (map (file: ''cp \"${src}/${file}\" $out\\n''))\n\n    # concatenate all commands into one string\n    lib.concatStrings\n\n    # make that string into a nix derivation\n    (pkgs.runCommand \"copy-to-out\" {})\n\n  ]\n  => <drv which copies all files to $out>\n\nThe output type of each function has to be the input type\nof the next function, and the last function returns the\nfinal value.\n\n\n\n\nLocated at lib/trivial.nix:61 in <nixpkgs>.\n\nlib.trivial.concat\n\nType: concat :: [a] -> [a] -> [a]\n\nConcatenate two lists\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nExample 118. lib.trivial.concat usage example\n\nconcat [ 1 2 ] [ 3 4 ]\n=> [ 1 2 3 4 ]\n\n\n\n\nLocated at lib/trivial.nix:80 in <nixpkgs>.\n\nlib.trivial.or\n\nboolean “or”\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nLocated at lib/trivial.nix:83 in <nixpkgs>.\n\nlib.trivial.and\n\nboolean “and”\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nLocated at lib/trivial.nix:86 in <nixpkgs>.\n\nlib.trivial.bitAnd\n\nbitwise “and”\n\nLocated at lib/trivial.nix:89 in <nixpkgs>.\n\nlib.trivial.bitOr\n\nbitwise “or”\n\nLocated at lib/trivial.nix:94 in <nixpkgs>.\n\nlib.trivial.bitXor\n\nbitwise “xor”\n\nLocated at lib/trivial.nix:99 in <nixpkgs>.\n\nlib.trivial.bitNot\n\nbitwise “not”\n\nLocated at lib/trivial.nix:104 in <nixpkgs>.\n\nlib.trivial.boolToString\n\nType: boolToString :: bool -> string\n\nConvert a boolean to a string.\n\nThis function uses the strings “true” and “false” to represent boolean values. Calling toString on a bool instead returns “1” and “” (sic!).\n\nb\n\nFunction argument\n\nLocated at lib/trivial.nix:114 in <nixpkgs>.\n\nlib.trivial.mergeAttrs\n\nMerge two attribute sets shallowly, right side trumps left\n\nmergeAttrs :: attrs -> attrs -> attrs\n\nx\n\nLeft attribute set\n\ny\n\nRight attribute set (higher precedence for equal keys)\n\nExample 119. lib.trivial.mergeAttrs usage example\n\nmergeAttrs { a = 1; b = 2; } { b = 3; c = 4; }\n=> { a = 1; b = 3; c = 4; }\n\n\n\n\nLocated at lib/trivial.nix:124 in <nixpkgs>.\n\nlib.trivial.flip\n\nType: flip :: (a -> b -> c) -> (b -> a -> c)\n\nFlip the order of the arguments of a binary function.\n\nf\n\nFunction argument\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nExample 120. lib.trivial.flip usage example\n\nflip concat [1] [2]\n=> [ 2 1 ]\n\n\n\n\nLocated at lib/trivial.nix:138 in <nixpkgs>.\n\nlib.trivial.mapNullable\n\nApply function if the supplied argument is non-null.\n\nf\n\nFunction to call\n\na\n\nArgument to check for null before passing it to f\n\nExample 121. lib.trivial.mapNullable usage example\n\nmapNullable (x: x+1) null\n=> null\nmapNullable (x: x+1) 22\n=> 23\n\n\n\n\nLocated at lib/trivial.nix:148 in <nixpkgs>.\n\nlib.trivial.version\n\nReturns the current full nixpkgs version number.\n\nLocated at lib/trivial.nix:164 in <nixpkgs>.\n\nlib.trivial.release\n\nReturns the current nixpkgs release number as string.\n\nLocated at lib/trivial.nix:167 in <nixpkgs>.\n\nlib.trivial.oldestSupportedRelease\n\nThe latest release that is supported, at the time of release branch-off, if applicable.\n\nIdeally, out-of-tree modules should be able to evaluate cleanly with all supported Nixpkgs versions (master, release and old release until EOL). So if possible, deprecation warnings should take effect only when all out-of-tree expressions/libs/modules can upgrade to the new way without losing support for supported Nixpkgs versions.\n\nThis release number allows deprecation warnings to be implemented such that they take effect as soon as the oldest release reaches end of life.\n\nLocated at lib/trivial.nix:180 in <nixpkgs>.\n\nlib.trivial.isInOldestRelease\n\nWhether a feature is supported in all supported releases (at the time of release branch-off, if applicable). See oldestSupportedRelease.\n\nrelease\n\nRelease number of feature introduction as an integer, e.g. 2111 for 21.11. Set it to the upcoming release, matching the nixpkgs/.version file.\n\nLocated at lib/trivial.nix:186 in <nixpkgs>.\n\nlib.trivial.codeName\n\nReturns the current nixpkgs release code name.\n\nOn each release the first letter is bumped and a new animal is chosen starting with that new letter.\n\nLocated at lib/trivial.nix:198 in <nixpkgs>.\n\nlib.trivial.versionSuffix\n\nReturns the current nixpkgs version suffix as string.\n\nLocated at lib/trivial.nix:201 in <nixpkgs>.\n\nlib.trivial.revisionWithDefault\n\nType: revisionWithDefault :: string -> string\n\nAttempts to return the the current revision of nixpkgs and returns the supplied default value otherwise.\n\ndefault\n\nDefault value to return if revision can not be determined\n\nLocated at lib/trivial.nix:212 in <nixpkgs>.\n\nlib.trivial.inNixShell\n\nType: inNixShell :: bool\n\nDetermine whether the function is being called from inside a Nix shell.\n\nLocated at lib/trivial.nix:230 in <nixpkgs>.\n\nlib.trivial.inPureEvalMode\n\nType: inPureEvalMode :: bool\n\nDetermine whether the function is being called from inside pure-eval mode by seeing whether builtins contains currentSystem. If not, we must be in pure-eval mode.\n\nLocated at lib/trivial.nix:238 in <nixpkgs>.\n\nlib.trivial.min\n\nReturn minimum of two numbers.\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nLocated at lib/trivial.nix:243 in <nixpkgs>.\n\nlib.trivial.max\n\nReturn maximum of two numbers.\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nLocated at lib/trivial.nix:246 in <nixpkgs>.\n\nlib.trivial.mod\n\nInteger modulus\n\nbase\n\nFunction argument\n\nint\n\nFunction argument\n\nExample 122. lib.trivial.mod usage example\n\nmod 11 10\n=> 1\nmod 1 10\n=> 1\n\n\n\n\nLocated at lib/trivial.nix:256 in <nixpkgs>.\n\nlib.trivial.compare\n\nC-style comparisons\n\na < b, compare a b => -1 a == b, compare a b => 0 a > b, compare a b => 1\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nLocated at lib/trivial.nix:267 in <nixpkgs>.\n\nlib.trivial.splitByAndCompare\n\nType: (a -> bool) -> (a -> a -> int) -> (a -> a -> int) -> (a -> a -> int)\n\nSplit type into two subtypes by predicate p, take all elements of the first subtype to be less than all the elements of the second subtype, compare elements of a single subtype with yes and no respectively.\n\np\n\nPredicate\n\nyes\n\nComparison function if predicate holds for both values\n\nno\n\nComparison function if predicate holds for neither value\n\na\n\nFirst value to compare\n\nb\n\nSecond value to compare\n\nExample 123. lib.trivial.splitByAndCompare usage example\n\nlet cmp = splitByAndCompare (hasPrefix \"foo\") compare compare; in\n\ncmp \"a\" \"z\" => -1\ncmp \"fooa\" \"fooz\" => -1\n\ncmp \"f\" \"a\" => 1\ncmp \"fooa\" \"a\" => -1\n# while\ncompare \"fooa\" \"a\" => 1\n\n\n\n\nLocated at lib/trivial.nix:292 in <nixpkgs>.\n\nlib.trivial.importJSON\n\nType: importJSON :: path -> any\n\nReads a JSON file.\n\npath\n\nFunction argument\n\nLocated at lib/trivial.nix:312 in <nixpkgs>.\n\nlib.trivial.importTOML\n\nType: importTOML :: path -> any\n\nReads a TOML file.\n\npath\n\nFunction argument\n\nLocated at lib/trivial.nix:319 in <nixpkgs>.\n\nlib.trivial.warn\n\nType: string -> a -> a\n\nPrint a warning before returning the second argument. This function behaves like builtins.trace, but requires a string message and formats it as a warning, including the warning: prefix.\n\nTo get a call stack trace and abort evaluation, set the environment variable NIX_ABORT_ON_WARN=true and set the Nix options --option pure-eval false --show-trace\n\nLocated at lib/trivial.nix:347 in <nixpkgs>.\n\nlib.trivial.warnIf\n\nType: bool -> string -> a -> a\n\nLike warn, but only warn when the first argument is true.\n\ncond\n\nFunction argument\n\nmsg\n\nFunction argument\n\nLocated at lib/trivial.nix:357 in <nixpkgs>.\n\nlib.trivial.warnIfNot\n\nType: bool -> string -> a -> a\n\nLike warnIf, but negated (warn if the first argument is false).\n\ncond\n\nFunction argument\n\nmsg\n\nFunction argument\n\nLocated at lib/trivial.nix:364 in <nixpkgs>.\n\nlib.trivial.throwIfNot\n\nType: bool -> string -> a -> a\n\nLike the assert b; e expression, but with a custom error message and without the semicolon.\n\nIf true, return the identity function, r: r.\n\nIf false, throw the error message.\n\nCalls can be juxtaposed using function application, as (r: r) a = a, so (r: r) (r: r) a = a, and so forth.\n\ncond\n\nFunction argument\n\nmsg\n\nFunction argument\n\nExample 124. lib.trivial.throwIfNot usage example\n\nthrowIfNot (lib.isList overlays) \"The overlays argument to nixpkgs must be a list.\"\nlib.foldr (x: throwIfNot (lib.isFunction x) \"All overlays passed to nixpkgs must be functions.\") (r: r) overlays\npkgs\n\n\n\n\nLocated at lib/trivial.nix:386 in <nixpkgs>.\n\nlib.trivial.throwIf\n\nType: bool -> string -> a -> a\n\nLike throwIfNot, but negated (throw if the first argument is true).\n\ncond\n\nFunction argument\n\nmsg\n\nFunction argument\n\nLocated at lib/trivial.nix:393 in <nixpkgs>.\n\nlib.trivial.checkListOfEnum\n\nType: String -> List ComparableVal -> List ComparableVal -> a -> a\n\nCheck if the elements in a list are valid values from a enum, returning the identity function, or throwing an error message otherwise.\n\nmsg\n\nFunction argument\n\nvalid\n\nFunction argument\n\ngiven\n\nFunction argument\n\nExample 125. lib.trivial.checkListOfEnum usage example\n\nlet colorVariants = [\"bright\" \"dark\" \"black\"]\nin checkListOfEnum \"color variants\" [ \"standard\" \"light\" \"dark\" ] colorVariants;\n=>\nerror: color variants: bright, black unexpected; valid ones: standard, light, dark\n\n\n\n\nLocated at lib/trivial.nix:405 in <nixpkgs>.\n\nlib.trivial.setFunctionArgs\n\nAdd metadata about expected function arguments to a function. The metadata should match the format given by builtins.functionArgs, i.e. a set from expected argument to a bool representing whether that argument has a default or not. setFunctionArgs : (a → b) → Map String Bool → (a → b)\n\nThis function is necessary because you can’t dynamically create a function of the { a, b ? foo, … }: format, but some facilities like callPackage expect to be able to query expected arguments.\n\nf\n\nFunction argument\n\nargs\n\nFunction argument\n\nLocated at lib/trivial.nix:428 in <nixpkgs>.\n\nlib.trivial.functionArgs\n\nExtract the expected function arguments from a function. This works both with nix-native { a, b ? foo, … }: style functions and functions with args set with ‘setFunctionArgs’. It has the same return type and semantics as builtins.functionArgs. setFunctionArgs : (a → b) → Map String Bool.\n\nf\n\nFunction argument\n\nLocated at lib/trivial.nix:440 in <nixpkgs>.\n\nlib.trivial.isFunction\n\nCheck whether something is a function or something annotated with function args.\n\nf\n\nFunction argument\n\nLocated at lib/trivial.nix:448 in <nixpkgs>.\n\nlib.trivial.mirrorFunctionArgs\n\nType: mirrorFunctionArgs :: (a -> b) -> (a -> c) -> (a -> c)\n\nmirrorFunctionArgs f g creates a new function g' with the same behavior as g (g' x == g x) but its function arguments mirroring f (lib.functionArgs g' == lib.functionArgs f).\n\nf\n\nFunction to provide the argument metadata\n\nExample 126. lib.trivial.mirrorFunctionArgs usage example\n\naddab = {a, b}: a + b\naddab { a = 2; b = 4; }\n=> 6\nlib.functionArgs addab\n=> { a = false; b = false; }\naddab1 = attrs: addab attrs + 1\naddab1 { a = 2; b = 4; }\n=> 7\nlib.functionArgs addab1\n=> { }\naddab1' = lib.mirrorFunctionArgs addab addab1\naddab1' { a = 2; b = 4; }\n=> 7\nlib.functionArgs addab1'\n=> { a = false; b = false; }\n\n\n\n\nLocated at lib/trivial.nix:475 in <nixpkgs>.\n\nlib.trivial.toFunction\n\nTurns any non-callable values into constant functions. Returns callable values as is.\n\nv\n\nAny value\n\nExample 127. lib.trivial.toFunction usage example\n\nnix-repl> lib.toFunction 1 2\n1\n\nnix-repl> lib.toFunction (x: x + 1) 2\n3\n\n\n\n\nLocated at lib/trivial.nix:497 in <nixpkgs>.\n\nlib.trivial.toHexString\n\nConvert the given positive integer to a string of its hexadecimal representation. For example:\n\ntoHexString 0 => “0”\n\ntoHexString 16 => “10”\n\ntoHexString 250 => “FA”\n\ni\n\nFunction argument\n\nLocated at lib/trivial.nix:513 in <nixpkgs>.\n\nlib.trivial.toBaseDigits\n\ntoBaseDigits base i converts the positive integer i to a list of its digits in the given base. For example:\n\ntoBaseDigits 10 123 => [ 1 2 3 ]\n\ntoBaseDigits 2 6 => [ 1 1 0 ]\n\ntoBaseDigits 16 250 => [ 15 10 ]\n\nbase\n\nFunction argument\n\ni\n\nFunction argument\n\nLocated at lib/trivial.nix:539 in <nixpkgs>.\n\nlib.fixedPoints: explicit recursion functions \nlib.fixedPoints.fix\n\nType: fix :: (a -> a) -> a\n\nfix f computes the fixed point of the given function f. In other words, the return value is x in x = f x.\n\nf must be a lazy function. This means that x must be a value that can be partially evaluated, such as an attribute set, a list, or a function. This way, f can use one part of x to compute another part.\n\nRelation to syntactic recursion\n\nThis section explains fix by refactoring from syntactic recursion to a call of fix instead.\n\nFor context, Nix lets you define attributes in terms of other attributes syntactically using the rec { } syntax.\n\nnix-repl> rec {\n  foo = \"foo\";\n  bar = \"bar\";\n  foobar = foo + bar;\n}\n{ bar = \"bar\"; foo = \"foo\"; foobar = \"foobar\"; }\n\n\nThis is convenient when constructing a value to pass to a function for example, but an equivalent effect can be achieved with the let binding syntax:\n\nnix-repl> let self = {\n  foo = \"foo\";\n  bar = \"bar\";\n  foobar = self.foo + self.bar;\n}; in self\n{ bar = \"bar\"; foo = \"foo\"; foobar = \"foobar\"; }\n\n\nBut in general you can get more reuse out of let bindings by refactoring them to a function.\n\nnix-repl> f = self: {\n  foo = \"foo\";\n  bar = \"bar\";\n  foobar = self.foo + self.bar;\n}\n\n\nThis is where fix comes in, it contains the syntactic recursion that’s not in f anymore.\n\nnix-repl> fix = f:\n  let self = f self; in self;\n\n\nBy applying fix we get the final result.\n\nnix-repl> fix f\n{ bar = \"bar\"; foo = \"foo\"; foobar = \"foobar\"; }\n\n\nSuch a refactored f using fix is not useful by itself. See extends for an example use case. There self is also often called final.\n\nf\n\nFunction argument\n\nExample 128. lib.fixedPoints.fix usage example\n\nfix (self: { foo = \"foo\"; bar = \"bar\"; foobar = self.foo + self.bar; })\n=> { bar = \"bar\"; foo = \"foo\"; foobar = \"foobar\"; }\n\nfix (self: [ 1 2 (elemAt self 0 + elemAt self 1) ])\n=> [ 1 2 3 ]\n\n\n\n\nLocated at lib/fixed-points.nix:75 in <nixpkgs>.\n\nlib.fixedPoints.fix'\n\nA variant of fix that records the original recursive attribute set in the result, in an attribute named __unfix__.\n\nThis is useful in combination with the extends function to implement deep overriding.\n\nf\n\nFunction argument\n\nLocated at lib/fixed-points.nix:84 in <nixpkgs>.\n\nlib.fixedPoints.converge\n\nType: (a -> a) -> a -> a\n\nReturn the fixpoint that f converges to when called iteratively, starting with the input x.\n\nnix-repl> converge (x: x / 2) 16\n0\n\nf\n\nFunction argument\n\nx\n\nFunction argument\n\nLocated at lib/fixed-points.nix:97 in <nixpkgs>.\n\nlib.fixedPoints.extends\n\nModify the contents of an explicitly recursive attribute set in a way that honors self-references. This is accomplished with a function\n\ng = self: super: { foo = super.foo + \" + \"; }\n\n\nthat has access to the unmodified input (super) as well as the final non-recursive representation of the attribute set (self). extends differs from the native // operator insofar as that it’s applied before references to self are resolved:\n\nnix-repl> fix (extends g f)\n{ bar = \"bar\"; foo = \"foo + \"; foobar = \"foo + bar\"; }\n\n\nThe name of the function is inspired by object-oriented inheritance, i.e. think of it as an infix operator g extends f that mimics the syntax from Java. It may seem counter-intuitive to have the “base class” as the second argument, but it’s nice this way if several uses of extends are cascaded.\n\nTo get a better understanding how extends turns a function with a fix point (the package set we start with) into a new function with a different fix point (the desired packages set) lets just see, how extends g f unfolds with g and f defined above:\n\nextends g f = self: let super = f self; in super // g self super;\n            = self: let super = { foo = \"foo\"; bar = \"bar\"; foobar = self.foo + self.bar; }; in super // g self super\n            = self: { foo = \"foo\"; bar = \"bar\"; foobar = self.foo + self.bar; } // g self { foo = \"foo\"; bar = \"bar\"; foobar = self.foo + self.bar; }\n            = self: { foo = \"foo\"; bar = \"bar\"; foobar = self.foo + self.bar; } // { foo = \"foo\" + \" + \"; }\n            = self: { foo = \"foo + \"; bar = \"bar\"; foobar = self.foo + self.bar; }\n\nf\n\nFunction argument\n\nrattrs\n\nFunction argument\n\nself\n\nFunction argument\n\nLocated at lib/fixed-points.nix:141 in <nixpkgs>.\n\nlib.fixedPoints.composeExtensions\n\nCompose two extending functions of the type expected by ‘extends’ into one where changes made in the first are available in the ‘super’ of the second\n\nf\n\nFunction argument\n\ng\n\nFunction argument\n\nfinal\n\nFunction argument\n\nprev\n\nFunction argument\n\nLocated at lib/fixed-points.nix:148 in <nixpkgs>.\n\nlib.fixedPoints.composeManyExtensions\n\nCompose several extending functions of the type expected by ‘extends’ into one where changes made in preceding functions are made available to subsequent ones.\n\ncomposeManyExtensions : [packageSet -> packageSet -> packageSet] -> packageSet -> packageSet -> packageSet\n                          ^final        ^prev         ^overrides     ^final        ^prev         ^overrides\n\n\nLocated at lib/fixed-points.nix:164 in <nixpkgs>.\n\nlib.fixedPoints.makeExtensible\n\nCreate an overridable, recursive attribute set. For example:\n\nnix-repl> obj = makeExtensible (self: { })\n\nnix-repl> obj\n{ __unfix__ = «lambda»; extend = «lambda»; }\n\nnix-repl> obj = obj.extend (self: super: { foo = \"foo\"; })\n\nnix-repl> obj\n{ __unfix__ = «lambda»; extend = «lambda»; foo = \"foo\"; }\n\nnix-repl> obj = obj.extend (self: super: { foo = super.foo + \" + \"; bar = \"bar\"; foobar = self.foo + self.bar; })\n\nnix-repl> obj\n{ __unfix__ = «lambda»; bar = \"bar\"; extend = «lambda»; foo = \"foo + \"; foobar = \"foo + bar\"; }\n\n\nLocated at lib/fixed-points.nix:187 in <nixpkgs>.\n\nlib.fixedPoints.makeExtensibleWithCustomName\n\nSame as makeExtensible but the name of the extending attribute is customized.\n\nextenderName\n\nFunction argument\n\nrattrs\n\nFunction argument\n\nLocated at lib/fixed-points.nix:193 in <nixpkgs>.\n\nlib.lists: list manipulation functions \nlib.lists.singleton\n\nType: singleton :: a -> [a]\n\nCreate a list consisting of a single element. singleton x is sometimes more convenient with respect to indentation than [x] when x spans multiple lines.\n\nx\n\nFunction argument\n\nExample 129. lib.lists.singleton usage example\n\nsingleton \"foo\"\n=> [ \"foo\" ]\n\n\n\n\nLocated at lib/lists.nix:23 in <nixpkgs>.\n\nlib.lists.forEach\n\nType: forEach :: [a] -> (a -> b) -> [b]\n\nApply the function to each element in the list. Same as map, but arguments flipped.\n\nxs\n\nFunction argument\n\nf\n\nFunction argument\n\nExample 130. lib.lists.forEach usage example\n\nforEach [ 1 2 ] (x:\n  toString x\n)\n=> [ \"1\" \"2\" ]\n\n\n\n\nLocated at lib/lists.nix:36 in <nixpkgs>.\n\nlib.lists.foldr\n\nType: foldr :: (a -> b -> b) -> b -> [a] -> b\n\n“right fold” a binary function op between successive elements of list with nul as the starting value, i.e., foldr op nul [x_1 x_2 ... x_n] == op x_1 (op x_2 ... (op x_n nul)).\n\nop\n\nFunction argument\n\nnul\n\nFunction argument\n\nlist\n\nFunction argument\n\nExample 131. lib.lists.foldr usage example\n\nconcat = foldr (a: b: a + b) \"z\"\nconcat [ \"a\" \"b\" \"c\" ]\n=> \"abcz\"\n# different types\nstrange = foldr (int: str: toString (int + 1) + str) \"a\"\nstrange [ 1 2 3 4 ]\n=> \"2345a\"\n\n\n\n\nLocated at lib/lists.nix:53 in <nixpkgs>.\n\nlib.lists.fold\n\nfold is an alias of foldr for historic reasons\n\nLocated at lib/lists.nix:64 in <nixpkgs>.\n\nlib.lists.foldl\n\nType: foldl :: (b -> a -> b) -> b -> [a] -> b\n\n“left fold”, like foldr, but from the left: foldl op nul [x_1 x_2 ... x_n] == op (... (op (op nul x_1) x_2) ... x_n).\n\nop\n\nFunction argument\n\nnul\n\nFunction argument\n\nlist\n\nFunction argument\n\nExample 132. lib.lists.foldl usage example\n\nlconcat = foldl (a: b: a + b) \"z\"\nlconcat [ \"a\" \"b\" \"c\" ]\n=> \"zabc\"\n# different types\nlstrange = foldl (str: int: str + toString (int + 1)) \"a\"\nlstrange [ 1 2 3 4 ]\n=> \"a2345\"\n\n\n\n\nLocated at lib/lists.nix:81 in <nixpkgs>.\n\nlib.lists.foldl'\n\nType: foldl' :: (acc -> x -> acc) -> acc -> [x] -> acc\n\nReduce a list by applying a binary operator from left to right, starting with an initial accumulator.\n\nBefore each application of the operator, the accumulator value is evaluated. This behavior makes this function stricter than foldl.\n\nUnlike builtins.foldl', the initial accumulator argument is evaluated before the first iteration.\n\nA call like\n\nfoldl' op acc₀ [ x₀ x₁ x₂ ... xₙ₋₁ xₙ ]\n\n\nis (denotationally) equivalent to the following, but with the added benefit that foldl' itself will never overflow the stack.\n\nlet\n  acc₁   = builtins.seq acc₀   (op acc₀   x₀  );\n  acc₂   = builtins.seq acc₁   (op acc₁   x₁  );\n  acc₃   = builtins.seq acc₂   (op acc₂   x₂  );\n  ...\n  accₙ   = builtins.seq accₙ₋₁ (op accₙ₋₁ xₙ₋₁);\n  accₙ₊₁ = builtins.seq accₙ   (op accₙ   xₙ  );\nin\naccₙ₊₁\n\n# Or ignoring builtins.seq\nop (op (... (op (op (op acc₀ x₀) x₁) x₂) ...) xₙ₋₁) xₙ\n\nop\n\nThe binary operation to run, where the two arguments are:\n\nacc: The current accumulator value: Either the initial one for the first iteration, or the result of the previous iteration\n\nx: The corresponding list element for this iteration\n\nacc\n\nThe initial accumulator value\n\nlist\n\nThe list to fold\n\nExample 133. lib.lists.foldl' usage example\n\nfoldl' (acc: x: acc + x) 0 [1 2 3]\n=> 6\n\n\n\n\nLocated at lib/lists.nix:129 in <nixpkgs>.\n\nlib.lists.imap0\n\nType: imap0 :: (int -> a -> b) -> [a] -> [b]\n\nMap with index starting from 0\n\nf\n\nFunction argument\n\nlist\n\nFunction argument\n\nExample 134. lib.lists.imap0 usage example\n\nimap0 (i: v: \"${v}-${toString i}\") [\"a\" \"b\"]\n=> [ \"a-0\" \"b-1\" ]\n\n\n\n\nLocated at lib/lists.nix:155 in <nixpkgs>.\n\nlib.lists.imap1\n\nType: imap1 :: (int -> a -> b) -> [a] -> [b]\n\nMap with index starting from 1\n\nf\n\nFunction argument\n\nlist\n\nFunction argument\n\nExample 135. lib.lists.imap1 usage example\n\nimap1 (i: v: \"${v}-${toString i}\") [\"a\" \"b\"]\n=> [ \"a-1\" \"b-2\" ]\n\n\n\n\nLocated at lib/lists.nix:165 in <nixpkgs>.\n\nlib.lists.concatMap\n\nType: concatMap :: (a -> [b]) -> [a] -> [b]\n\nMap and concatenate the result.\n\nExample 136. lib.lists.concatMap usage example\n\nconcatMap (x: [x] ++ [\"z\"]) [\"a\" \"b\"]\n=> [ \"a\" \"z\" \"b\" \"z\" ]\n\n\n\n\nLocated at lib/lists.nix:175 in <nixpkgs>.\n\nlib.lists.flatten\n\nFlatten the argument into a single list; that is, nested lists are spliced into the top-level lists.\n\nx\n\nFunction argument\n\nExample 137. lib.lists.flatten usage example\n\nflatten [1 [2 [3] 4] 5]\n=> [1 2 3 4 5]\nflatten 1\n=> [1]\n\n\n\n\nLocated at lib/lists.nix:186 in <nixpkgs>.\n\nlib.lists.remove\n\nType: remove :: a -> [a] -> [a]\n\nRemove elements equal to ‘e’ from a list. Useful for buildInputs.\n\ne\n\nElement to remove from the list\n\nExample 138. lib.lists.remove usage example\n\nremove 3 [ 1 3 4 3 ]\n=> [ 1 4 ]\n\n\n\n\nLocated at lib/lists.nix:199 in <nixpkgs>.\n\nlib.lists.findSingle\n\nType: findSingle :: (a -> bool) -> a -> a -> [a] -> a\n\nFind the sole element in the list matching the specified predicate, returns default if no such element exists, or multiple if there are multiple matching elements.\n\npred\n\nPredicate\n\ndefault\n\nDefault value to return if element was not found.\n\nmultiple\n\nDefault value to return if more than one element was found\n\nlist\n\nInput list\n\nExample 139. lib.lists.findSingle usage example\n\nfindSingle (x: x == 3) \"none\" \"multiple\" [ 1 3 3 ]\n=> \"multiple\"\nfindSingle (x: x == 3) \"none\" \"multiple\" [ 1 3 ]\n=> 3\nfindSingle (x: x == 3) \"none\" \"multiple\" [ 1 9 ]\n=> \"none\"\n\n\n\n\nLocated at lib/lists.nix:217 in <nixpkgs>.\n\nlib.lists.findFirstIndex\n\nType: findFirstIndex :: (a -> Bool) -> b -> [a] -> (Int | b)\n\nFind the first index in the list matching the specified predicate or return default if no such element exists.\n\npred\n\nPredicate\n\ndefault\n\nDefault value to return\n\nlist\n\nInput list\n\nExample 140. lib.lists.findFirstIndex usage example\n\nfindFirstIndex (x: x > 3) null [ 0 6 4 ]\n=> 1\nfindFirstIndex (x: x > 9) null [ 0 6 4 ]\n=> null\n\n\n\n\nLocated at lib/lists.nix:242 in <nixpkgs>.\n\nlib.lists.findFirst\n\nType: findFirst :: (a -> bool) -> a -> [a] -> a\n\nFind the first element in the list matching the specified predicate or return default if no such element exists.\n\npred\n\nPredicate\n\ndefault\n\nDefault value to return\n\nlist\n\nInput list\n\nExample 141. lib.lists.findFirst usage example\n\nfindFirst (x: x > 3) 7 [ 1 6 4 ]\n=> 6\nfindFirst (x: x > 9) 7 [ 1 6 4 ]\n=> 7\n\n\n\n\nLocated at lib/lists.nix:293 in <nixpkgs>.\n\nlib.lists.any\n\nType: any :: (a -> bool) -> [a] -> bool\n\nReturn true if function pred returns true for at least one element of list.\n\nExample 142. lib.lists.any usage example\n\nany isString [ 1 \"a\" { } ]\n=> true\nany isString [ 1 { } ]\n=> false\n\n\n\n\nLocated at lib/lists.nix:319 in <nixpkgs>.\n\nlib.lists.all\n\nType: all :: (a -> bool) -> [a] -> bool\n\nReturn true if function pred returns true for all elements of list.\n\nExample 143. lib.lists.all usage example\n\nall (x: x < 3) [ 1 2 ]\n=> true\nall (x: x < 3) [ 1 2 3 ]\n=> false\n\n\n\n\nLocated at lib/lists.nix:332 in <nixpkgs>.\n\nlib.lists.count\n\nType: count :: (a -> bool) -> [a] -> int\n\nCount how many elements of list match the supplied predicate function.\n\npred\n\nPredicate\n\nExample 144. lib.lists.count usage example\n\ncount (x: x == 3) [ 3 2 3 4 6 ]\n=> 2\n\n\n\n\nLocated at lib/lists.nix:343 in <nixpkgs>.\n\nlib.lists.optional\n\nType: optional :: bool -> a -> [a]\n\nReturn a singleton list or an empty list, depending on a boolean value. Useful when building lists with optional elements (e.g. ++ optional (system == \"i686-linux\") firefox).\n\ncond\n\nFunction argument\n\nelem\n\nFunction argument\n\nExample 145. lib.lists.optional usage example\n\noptional true \"foo\"\n=> [ \"foo\" ]\noptional false \"foo\"\n=> [ ]\n\n\n\n\nLocated at lib/lists.nix:359 in <nixpkgs>.\n\nlib.lists.optionals\n\nType: optionals :: bool -> [a] -> [a]\n\nReturn a list or an empty list, depending on a boolean value.\n\ncond\n\nCondition\n\nelems\n\nList to return if condition is true\n\nExample 146. lib.lists.optionals usage example\n\noptionals true [ 2 3 ]\n=> [ 2 3 ]\noptionals false [ 2 3 ]\n=> [ ]\n\n\n\n\nLocated at lib/lists.nix:371 in <nixpkgs>.\n\nlib.lists.toList\n\nIf argument is a list, return it; else, wrap it in a singleton list. If you’re using this, you should almost certainly reconsider if there isn’t a more “well-typed” approach.\n\nx\n\nFunction argument\n\nExample 147. lib.lists.toList usage example\n\ntoList [ 1 2 ]\n=> [ 1 2 ]\ntoList \"hi\"\n=> [ \"hi \"]\n\n\n\n\nLocated at lib/lists.nix:388 in <nixpkgs>.\n\nlib.lists.range\n\nType: range :: int -> int -> [int]\n\nReturn a list of integers from first up to and including last.\n\nfirst\n\nFirst integer in the range\n\nlast\n\nLast integer in the range\n\nExample 148. lib.lists.range usage example\n\nrange 2 4\n=> [ 2 3 4 ]\nrange 3 2\n=> [ ]\n\n\n\n\nLocated at lib/lists.nix:400 in <nixpkgs>.\n\nlib.lists.replicate\n\nType: replicate :: int -> a -> [a]\n\nReturn a list with n copies of an element.\n\nn\n\nFunction argument\n\nelem\n\nFunction argument\n\nExample 149. lib.lists.replicate usage example\n\nreplicate 3 \"a\"\n=> [ \"a\" \"a\" \"a\" ]\nreplicate 2 true\n=> [ true true ]\n\n\n\n\nLocated at lib/lists.nix:420 in <nixpkgs>.\n\nlib.lists.partition\n\nType: (a -> bool) -> [a] -> { right :: [a]; wrong :: [a]; }\n\nSplits the elements of a list in two lists, right and wrong, depending on the evaluation of a predicate.\n\nExample 150. lib.lists.partition usage example\n\npartition (x: x > 2) [ 5 1 2 3 4 ]\n=> { right = [ 5 3 4 ]; wrong = [ 1 2 ]; }\n\n\n\n\nLocated at lib/lists.nix:431 in <nixpkgs>.\n\nlib.lists.groupBy'\n\nSplits the elements of a list into many lists, using the return value of a predicate. Predicate should return a string which becomes keys of attrset groupBy returns.\n\ngroupBy' allows to customise the combining function and initial value\n\nop\n\nFunction argument\n\nnul\n\nFunction argument\n\npred\n\nFunction argument\n\nlst\n\nFunction argument\n\nExample 151. lib.lists.groupBy' usage example\n\ngroupBy (x: boolToString (x > 2)) [ 5 1 2 3 4 ]\n=> { true = [ 5 3 4 ]; false = [ 1 2 ]; }\ngroupBy (x: x.name) [ {name = \"icewm\"; script = \"icewm &\";}\n                      {name = \"xfce\";  script = \"xfce4-session &\";}\n                      {name = \"icewm\"; script = \"icewmbg &\";}\n                      {name = \"mate\";  script = \"gnome-session &\";}\n                    ]\n=> { icewm = [ { name = \"icewm\"; script = \"icewm &\"; }\n               { name = \"icewm\"; script = \"icewmbg &\"; } ];\n     mate  = [ { name = \"mate\";  script = \"gnome-session &\"; } ];\n     xfce  = [ { name = \"xfce\";  script = \"xfce4-session &\"; } ];\n   }\n\ngroupBy' builtins.add 0 (x: boolToString (x > 2)) [ 5 1 2 3 4 ]\n=> { true = 12; false = 3; }\n\n\n\n\nLocated at lib/lists.nix:460 in <nixpkgs>.\n\nlib.lists.zipListsWith\n\nType: zipListsWith :: (a -> b -> c) -> [a] -> [b] -> [c]\n\nMerges two lists of the same size together. If the sizes aren’t the same the merging stops at the shortest. How both lists are merged is defined by the first argument.\n\nf\n\nFunction to zip elements of both lists\n\nfst\n\nFirst list\n\nsnd\n\nSecond list\n\nExample 152. lib.lists.zipListsWith usage example\n\nzipListsWith (a: b: a + b) [\"h\" \"l\"] [\"e\" \"o\"]\n=> [\"he\" \"lo\"]\n\n\n\n\nLocated at lib/lists.nix:480 in <nixpkgs>.\n\nlib.lists.zipLists\n\nType: zipLists :: [a] -> [b] -> [{ fst :: a; snd :: b; }]\n\nMerges two lists of the same size together. If the sizes aren’t the same the merging stops at the shortest.\n\nExample 153. lib.lists.zipLists usage example\n\nzipLists [ 1 2 ] [ \"a\" \"b\" ]\n=> [ { fst = 1; snd = \"a\"; } { fst = 2; snd = \"b\"; } ]\n\n\n\n\nLocated at lib/lists.nix:499 in <nixpkgs>.\n\nlib.lists.reverseList\n\nType: reverseList :: [a] -> [a]\n\nReverse the order of the elements of a list.\n\nxs\n\nFunction argument\n\nExample 154. lib.lists.reverseList usage example\n\nreverseList [ \"b\" \"o\" \"j\" ]\n=> [ \"j\" \"o\" \"b\" ]\n\n\n\n\nLocated at lib/lists.nix:510 in <nixpkgs>.\n\nlib.lists.listDfs\n\nDepth-First Search (DFS) for lists list != [].\n\nbefore a b == true means that b depends on a (there’s an edge from b to a).\n\nstopOnCycles\n\nFunction argument\n\nbefore\n\nFunction argument\n\nlist\n\nFunction argument\n\nExample 155. lib.lists.listDfs usage example\n\nlistDfs true hasPrefix [ \"/home/user\" \"other\" \"/\" \"/home\" ]\n  == { minimal = \"/\";                  # minimal element\n       visited = [ \"/home/user\" ];     # seen elements (in reverse order)\n       rest    = [ \"/home\" \"other\" ];  # everything else\n     }\n\nlistDfs true hasPrefix [ \"/home/user\" \"other\" \"/\" \"/home\" \"/\" ]\n  == { cycle   = \"/\";                  # cycle encountered at this element\n       loops   = [ \"/\" ];              # and continues to these elements\n       visited = [ \"/\" \"/home/user\" ]; # elements leading to the cycle (in reverse order)\n       rest    = [ \"/home\" \"other\" ];  # everything else\n\n\n\n\nLocated at lib/lists.nix:532 in <nixpkgs>.\n\nlib.lists.toposort\n\nSort a list based on a partial ordering using DFS. This implementation is O(N^2), if your ordering is linear, use sort instead.\n\nbefore a b == true means that b should be after a in the result.\n\nbefore\n\nFunction argument\n\nlist\n\nFunction argument\n\nExample 156. lib.lists.toposort usage example\n\ntoposort hasPrefix [ \"/home/user\" \"other\" \"/\" \"/home\" ]\n  == { result = [ \"/\" \"/home\" \"/home/user\" \"other\" ]; }\n\ntoposort hasPrefix [ \"/home/user\" \"other\" \"/\" \"/home\" \"/\" ]\n  == { cycle = [ \"/home/user\" \"/\" \"/\" ]; # path leading to a cycle\n       loops = [ \"/\" ]; }                # loops back to these elements\n\ntoposort hasPrefix [ \"other\" \"/home/user\" \"/home\" \"/\" ]\n  == { result = [ \"other\" \"/\" \"/home\" \"/home/user\" ]; }\n\ntoposort (a: b: a < b) [ 3 2 1 ] == { result = [ 1 2 3 ]; }\n\n\n\n\nLocated at lib/lists.nix:571 in <nixpkgs>.\n\nlib.lists.sort\n\nSort a list based on a comparator function which compares two elements and returns true if the first argument is strictly below the second argument. The returned list is sorted in an increasing order. The implementation does a quick-sort.\n\nExample 157. lib.lists.sort usage example\n\nsort (a: b: a < b) [ 5 3 7 ]\n=> [ 3 5 7 ]\n\n\n\n\nLocated at lib/lists.nix:599 in <nixpkgs>.\n\nlib.lists.compareLists\n\nCompare two lists element-by-element.\n\ncmp\n\nFunction argument\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nExample 158. lib.lists.compareLists usage example\n\ncompareLists compare [] []\n=> 0\ncompareLists compare [] [ \"a\" ]\n=> -1\ncompareLists compare [ \"a\" ] []\n=> 1\ncompareLists compare [ \"a\" \"b\" ] [ \"a\" \"c\" ]\n=> -1\n\n\n\n\nLocated at lib/lists.nix:628 in <nixpkgs>.\n\nlib.lists.naturalSort\n\nSort list using “Natural sorting”. Numeric portions of strings are sorted in numeric order.\n\nlst\n\nFunction argument\n\nExample 159. lib.lists.naturalSort usage example\n\nnaturalSort [\"disk11\" \"disk8\" \"disk100\" \"disk9\"]\n=> [\"disk8\" \"disk9\" \"disk11\" \"disk100\"]\nnaturalSort [\"10.46.133.149\" \"10.5.16.62\" \"10.54.16.25\"]\n=> [\"10.5.16.62\" \"10.46.133.149\" \"10.54.16.25\"]\nnaturalSort [\"v0.2\" \"v0.15\" \"v0.0.9\"]\n=> [ \"v0.0.9\" \"v0.2\" \"v0.15\" ]\n\n\n\n\nLocated at lib/lists.nix:651 in <nixpkgs>.\n\nlib.lists.take\n\nType: take :: int -> [a] -> [a]\n\nReturn the first (at most) N elements of a list.\n\ncount\n\nNumber of elements to take\n\nExample 160. lib.lists.take usage example\n\ntake 2 [ \"a\" \"b\" \"c\" \"d\" ]\n=> [ \"a\" \"b\" ]\ntake 2 [ ]\n=> [ ]\n\n\n\n\nLocated at lib/lists.nix:669 in <nixpkgs>.\n\nlib.lists.drop\n\nType: drop :: int -> [a] -> [a]\n\nRemove the first (at most) N elements of a list.\n\ncount\n\nNumber of elements to drop\n\nlist\n\nInput list\n\nExample 161. lib.lists.drop usage example\n\ndrop 2 [ \"a\" \"b\" \"c\" \"d\" ]\n=> [ \"c\" \"d\" ]\ndrop 2 [ ]\n=> [ ]\n\n\n\n\nLocated at lib/lists.nix:683 in <nixpkgs>.\n\nlib.lists.hasPrefix\n\nType: hasPrefix :: [a] -> [a] -> bool\n\nWhether the first list is a prefix of the second list.\n\nlist1\n\nFunction argument\n\nlist2\n\nFunction argument\n\nExample 162. lib.lists.hasPrefix usage example\n\nhasPrefix [ 1 2 ] [ 1 2 3 4 ]\n=> true\nhasPrefix [ 0 1 ] [ 1 2 3 4 ]\n=> false\n\n\n\n\nLocated at lib/lists.nix:699 in <nixpkgs>.\n\nlib.lists.removePrefix\n\nType: removePrefix :: [a] -> [a] -> [a]\n\nRemove the first list as a prefix from the second list. Error if the first list isn’t a prefix of the second list.\n\nlist1\n\nFunction argument\n\nlist2\n\nFunction argument\n\nExample 163. lib.lists.removePrefix usage example\n\nremovePrefix [ 1 2 ] [ 1 2 3 4 ]\n=> [ 3 4 ]\nremovePrefix [ 0 1 ] [ 1 2 3 4 ]\n=> <error>\n\n\n\n\nLocated at lib/lists.nix:715 in <nixpkgs>.\n\nlib.lists.sublist\n\nType: sublist :: int -> int -> [a] -> [a]\n\nReturn a list consisting of at most count elements of list, starting at index start.\n\nstart\n\nIndex at which to start the sublist\n\ncount\n\nNumber of elements to take\n\nlist\n\nInput list\n\nExample 164. lib.lists.sublist usage example\n\nsublist 1 3 [ \"a\" \"b\" \"c\" \"d\" \"e\" ]\n=> [ \"b\" \"c\" \"d\" ]\nsublist 1 3 [ ]\n=> [ ]\n\n\n\n\nLocated at lib/lists.nix:734 in <nixpkgs>.\n\nlib.lists.commonPrefix\n\nType: commonPrefix :: [a] -> [a] -> [a]\n\nThe common prefix of two lists.\n\nlist1\n\nFunction argument\n\nlist2\n\nFunction argument\n\nExample 165. lib.lists.commonPrefix usage example\n\ncommonPrefix [ 1 2 3 4 5 6 ] [ 1 2 4 8 ]\n=> [ 1 2 ]\ncommonPrefix [ 1 2 3 ] [ 1 2 3 4 5 ]\n=> [ 1 2 3 ]\ncommonPrefix [ 1 2 3 ] [ 4 5 6 ]\n=> [ ]\n\n\n\n\nLocated at lib/lists.nix:760 in <nixpkgs>.\n\nlib.lists.last\n\nType: last :: [a] -> a\n\nReturn the last element of a list.\n\nThis function throws an error if the list is empty.\n\nlist\n\nFunction argument\n\nExample 166. lib.lists.last usage example\n\nlast [ 1 2 3 ]\n=> 3\n\n\n\n\nLocated at lib/lists.nix:784 in <nixpkgs>.\n\nlib.lists.init\n\nType: init :: [a] -> [a]\n\nReturn all elements but the last.\n\nThis function throws an error if the list is empty.\n\nlist\n\nFunction argument\n\nExample 167. lib.lists.init usage example\n\ninit [ 1 2 3 ]\n=> [ 1 2 ]\n\n\n\n\nLocated at lib/lists.nix:798 in <nixpkgs>.\n\nlib.lists.crossLists\n\nReturn the image of the cross product of some lists by a function.\n\nExample 168. lib.lists.crossLists usage example\n\ncrossLists (x:y: \"${toString x}${toString y}\") [[1 2] [3 4]]\n=> [ \"13\" \"14\" \"23\" \"24\" ]\n\n\n\n\nLocated at lib/lists.nix:809 in <nixpkgs>.\n\nlib.lists.unique\n\nType: unique :: [a] -> [a]\n\nRemove duplicate elements from the list. O(n^2) complexity.\n\nExample 169. lib.lists.unique usage example\n\nunique [ 3 2 3 4 ]\n=> [ 3 2 4 ]\n\n\n\n\nLocated at lib/lists.nix:822 in <nixpkgs>.\n\nlib.lists.intersectLists\n\nIntersects list ‘e’ and another list. O(nm) complexity.\n\ne\n\nFunction argument\n\nExample 170. lib.lists.intersectLists usage example\n\nintersectLists [ 1 2 3 ] [ 6 3 2 ]\n=> [ 3 2 ]\n\n\n\n\nLocated at lib/lists.nix:830 in <nixpkgs>.\n\nlib.lists.subtractLists\n\nSubtracts list ‘e’ from another list. O(nm) complexity.\n\ne\n\nFunction argument\n\nExample 171. lib.lists.subtractLists usage example\n\nsubtractLists [ 3 2 ] [ 1 2 3 4 5 3 ]\n=> [ 1 4 5 ]\n\n\n\n\nLocated at lib/lists.nix:838 in <nixpkgs>.\n\nlib.lists.mutuallyExclusive\n\nTest if two lists have no common element. It should be slightly more efficient than (intersectLists a b == [])\n\na\n\nFunction argument\n\nb\n\nFunction argument\n\nLocated at lib/lists.nix:843 in <nixpkgs>.\n\nlib.debug: debugging functions \nlib.debug.traceIf\n\nType: traceIf :: bool -> string -> a -> a\n\nConditionally trace the supplied message, based on a predicate.\n\npred\n\nPredicate to check\n\nmsg\n\nMessage that should be traced\n\nx\n\nValue to return\n\nExample 172. lib.debug.traceIf usage example\n\ntraceIf true \"hello\" 3\ntrace: hello\n=> 3\n\n\n\n\nLocated at lib/debug.nix:44 in <nixpkgs>.\n\nlib.debug.traceValFn\n\nType: traceValFn :: (a -> b) -> a -> a\n\nTrace the supplied value after applying a function to it, and return the original value.\n\nf\n\nFunction to apply\n\nx\n\nValue to trace and return\n\nExample 173. lib.debug.traceValFn usage example\n\ntraceValFn (v: \"mystring ${v}\") \"foo\"\ntrace: mystring foo\n=> \"foo\"\n\n\n\n\nLocated at lib/debug.nix:62 in <nixpkgs>.\n\nlib.debug.traceVal\n\nType: traceVal :: a -> a\n\nTrace the supplied value and return it.\n\nExample 174. lib.debug.traceVal usage example\n\ntraceVal 42\n# trace: 42\n=> 42\n\n\n\n\nLocated at lib/debug.nix:77 in <nixpkgs>.\n\nlib.debug.traceSeq\n\nType: traceSeq :: a -> b -> b\n\nbuiltins.trace, but the value is builtins.deepSeqed first.\n\nx\n\nThe value to trace\n\ny\n\nThe value to return\n\nExample 175. lib.debug.traceSeq usage example\n\ntrace { a.b.c = 3; } null\ntrace: { a = <CODE>; }\n=> null\ntraceSeq { a.b.c = 3; } null\ntrace: { a = { b = { c = 3; }; }; }\n=> null\n\n\n\n\nLocated at lib/debug.nix:91 in <nixpkgs>.\n\nlib.debug.traceSeqN\n\nType: traceSeqN :: Int -> a -> b -> b\n\nLike traceSeq, but only evaluate down to depth n. This is very useful because lots of traceSeq usages lead to an infinite recursion.\n\ndepth\n\nFunction argument\n\nx\n\nFunction argument\n\ny\n\nFunction argument\n\nExample 176. lib.debug.traceSeqN usage example\n\ntraceSeqN 2 { a.b.c = 3; } null\ntrace: { a = { b = {…}; }; }\n=> null\n\n\n\n\nLocated at lib/debug.nix:108 in <nixpkgs>.\n\nlib.debug.traceValSeqFn\n\nA combination of traceVal and traceSeq that applies a provided function to the value to be traced after deepSeqing it.\n\nf\n\nFunction to apply\n\nv\n\nValue to trace\n\nLocated at lib/debug.nix:125 in <nixpkgs>.\n\nlib.debug.traceValSeq\n\nA combination of traceVal and traceSeq.\n\nLocated at lib/debug.nix:132 in <nixpkgs>.\n\nlib.debug.traceValSeqNFn\n\nA combination of traceVal and traceSeqN that applies a provided function to the value to be traced.\n\nf\n\nFunction to apply\n\ndepth\n\nFunction argument\n\nv\n\nValue to trace\n\nLocated at lib/debug.nix:136 in <nixpkgs>.\n\nlib.debug.traceValSeqN\n\nA combination of traceVal and traceSeqN.\n\nLocated at lib/debug.nix:144 in <nixpkgs>.\n\nlib.debug.traceFnSeqN\n\nTrace the input and output of a function f named name, both down to depth.\n\nThis is useful for adding around a function call, to see the before/after of values as they are transformed.\n\ndepth\n\nFunction argument\n\nname\n\nFunction argument\n\nf\n\nFunction argument\n\nv\n\nFunction argument\n\nExample 177. lib.debug.traceFnSeqN usage example\n\ntraceFnSeqN 2 \"id\" (x: x) { a.b.c = 3; }\ntrace: { fn = \"id\"; from = { a.b = {…}; }; to = { a.b = {…}; }; }\n=> { a.b.c = 3; }\n\n\n\n\nLocated at lib/debug.nix:157 in <nixpkgs>.\n\nlib.debug.runTests\n\nType:\n\nrunTests :: {\n  tests = [ String ];\n  ${testName} :: {\n    expr :: a;\n    expected :: a;\n  };\n}\n->\n[\n  {\n    name :: String;\n    expected :: a;\n    result :: a;\n  }\n]\n\n\nEvaluates a set of tests.\n\nA test is an attribute set {expr, expected}, denoting an expression and its expected result.\n\nThe result is a list of failed tests, each represented as {name, expected, result},\n\nexpected\n\nWhat was passed as expected\n\nresult\n\nThe actual result of the test\n\nUsed for regression testing of the functions in lib; see tests.nix for more examples.\n\nImportant: Only attributes that start with test are executed.\n\nIf you want to run only a subset of the tests add the attribute tests = [\"testName\"];\n\ntests\n\nTests to run\n\nExample 178. lib.debug.runTests usage example\n\nrunTests {\n  testAndOk = {\n    expr = lib.and true false;\n    expected = false;\n  };\n  testAndFail = {\n    expr = lib.and true false;\n    expected = true;\n  };\n}\n->\n[\n  {\n    name = \"testAndFail\";\n    expected = true;\n    result = false;\n  }\n]\n\n\n\n\nLocated at lib/debug.nix:229 in <nixpkgs>.\n\nlib.debug.testAllTrue\n\nCreate a test assuming that list elements are true.\n\nexpr\n\nFunction argument\n\nExample 179. lib.debug.testAllTrue usage example\n\n{ testX = allTrue [ true ]; }\n\n\n\n\nLocated at lib/debug.nix:245 in <nixpkgs>.\n\nlib.options: NixOS / nixpkgs option handling \nlib.options.isOption\n\nType: isOption :: a -> bool\n\nReturns true when the given argument is an option\n\nExample 180. lib.options.isOption usage example\n\nisOption 1             // => false\nisOption (mkOption {}) // => true\n\n\n\n\nLocated at lib/options.nix:56 in <nixpkgs>.\n\nlib.options.mkOption\n\nCreates an Option attribute set. mkOption accepts an attribute set with the following keys:\n\nAll keys default to null when not given.\n\nstructured function argument\ndefault\n\nDefault value used when no definition is given in the configuration.\n\ndefaultText\n\nTextual representation of the default, for the manual.\n\nexample\n\nExample value used in the manual.\n\ndescription\n\nString describing the option.\n\nrelatedPackages\n\nRelated packages used in the manual (see genRelatedPackages in …/nixos/lib/make-options-doc/default.nix).\n\ntype\n\nOption type, providing type-checking and value merging.\n\napply\n\nFunction that converts the option value to something else.\n\ninternal\n\nWhether the option is for NixOS developers only.\n\nvisible\n\nWhether the option shows up in the manual. Default: true. Use false to hide the option and any sub-options from submodules. Use “shallow” to hide only sub-options.\n\nreadOnly\n\nWhether the option can be set only once\n\nExample 181. lib.options.mkOption usage example\n\nmkOption { }  // => { _type = \"option\"; }\nmkOption { default = \"foo\"; } // => { _type = \"option\"; default = \"foo\"; }\n\n\n\n\nLocated at lib/options.nix:66 in <nixpkgs>.\n\nlib.options.mkEnableOption\n\nCreates an Option attribute set for a boolean value option i.e an option to be toggled on or off:\n\nname\n\nName for the created option\n\nExample 182. lib.options.mkEnableOption usage example\n\nmkEnableOption \"foo\"\n=> { _type = \"option\"; default = false; description = \"Whether to enable foo.\"; example = true; type = { ... }; }\n\n\n\n\nLocated at lib/options.nix:98 in <nixpkgs>.\n\nlib.options.mkPackageOption\n\nType: mkPackageOption :: pkgs -> (string|[string]) -> { nullable? :: bool, default? :: string|[string], example? :: null|string|[string], extraDescription? :: string, pkgsText? :: string } -> option\n\nCreates an Option attribute set for an option that specifies the package a module should use for some purpose.\n\nThe package is specified in the third argument under default as a list of strings representing its attribute path in nixpkgs (or another package set). Because of this, you need to pass nixpkgs itself (usually pkgs in a module; alternatively to nixpkgs itself, another package set) as the first argument.\n\nIf you pass another package set you should set the pkgsText option. This option is used to display the expression for the package set. It is \"pkgs\" by default. If your expression is complex you should parenthesize it, as the pkgsText argument is usually immediately followed by an attribute lookup (.).\n\nThe second argument may be either a string or a list of strings. It provides the display name of the package in the description of the generated option (using only the last element if the passed value is a list) and serves as the fallback value for the default argument.\n\nTo include extra information in the description, pass extraDescription to append arbitrary text to the generated description.\n\nYou can also pass an example value, either a literal string or an attribute path.\n\nThe default argument can be omitted if the provided name is an attribute of pkgs (if name is a string) or a valid attribute path in pkgs (if name is a list). You can also set default to just a string in which case it is interpreted as an attribute name (a singleton attribute path, if you will).\n\nIf you wish to explicitly provide no default, pass null as default.\n\nIf you want users to be able to set no package, pass nullable = true. In this mode a default = null will not be interpreted as no default and is interpreted literally.\n\npkgs\n\nPackage set (an instantiation of nixpkgs such as pkgs in modules or another package set)\n\nname\n\nName for the package, shown in option description\n\nstructured function argument\nnullable\n\nWhether the package can be null, for example to disable installing a package altogether (defaults to false)\n\ndefault\n\nThe attribute path where the default package is located (may be omitted, in which case it is copied from name)\n\nexample\n\nA string or an attribute path to use as an example (may be omitted)\n\nextraDescription\n\nAdditional text to include in the option description (may be omitted)\n\npkgsText\n\nRepresentation of the package set passed as pkgs (defaults to \"pkgs\")\n\nExample 183. lib.options.mkPackageOption usage example\n\nmkPackageOption pkgs \"hello\" { }\n=> { ...; default = pkgs.hello; defaultText = literalExpression \"pkgs.hello\"; description = \"The hello package to use.\"; type = package; }\n\n\nmkPackageOption pkgs \"GHC\" {\n  default = [ \"ghc\" ];\n  example = \"pkgs.haskell.packages.ghc92.ghc.withPackages (hkgs: [ hkgs.primes ])\";\n}\n=> { ...; default = pkgs.ghc; defaultText = literalExpression \"pkgs.ghc\"; description = \"The GHC package to use.\"; example = literalExpression \"pkgs.haskell.packages.ghc92.ghc.withPackages (hkgs: [ hkgs.primes ])\"; type = package; }\n\n\nmkPackageOption pkgs [ \"python3Packages\" \"pytorch\" ] {\n  extraDescription = \"This is an example and doesn't actually do anything.\";\n}\n=> { ...; default = pkgs.python3Packages.pytorch; defaultText = literalExpression \"pkgs.python3Packages.pytorch\"; description = \"The pytorch package to use. This is an example and doesn't actually do anything.\"; type = package; }\n\n\nmkPackageOption pkgs \"nushell\" {\n  nullable = true;\n}\n=> { ...; default = pkgs.nushell; defaultText = literalExpression \"pkgs.nushell\"; description = \"The nushell package to use.\"; type = nullOr package; }\n\n\nmkPackageOption pkgs \"coreutils\" {\n  default = null;\n}\n=> { ...; description = \"The coreutils package to use.\"; type = package; }\n\n\nmkPackageOption pkgs \"dbus\" {\n  nullable = true;\n  default = null;\n}\n=> { ...; default = null; description = \"The dbus package to use.\"; type = nullOr package; }\n\n\nmkPackageOption pkgs.javaPackages \"OpenJFX\" {\n  default = \"openjfx20\";\n  pkgsText = \"pkgs.javaPackages\";\n}\n=> { ...; default = pkgs.javaPackages.openjfx20; defaultText = literalExpression \"pkgs.javaPackages.openjfx20\"; description = \"The OpenJFX package to use.\"; type = package; }\n\n\n\n\nLocated at lib/options.nix:185 in <nixpkgs>.\n\nlib.options.mkPackageOptionMD\n\nAlias of mkPackageOption. Previously used to create options with markdown documentation, which is no longer required.\n\nLocated at lib/options.nix:226 in <nixpkgs>.\n\nlib.options.mkSinkUndeclaredOptions\n\nThis option accepts anything, but it does not produce any result.\n\nThis is useful for sharing a module across different module sets without having to implement similar features as long as the values of the options are not accessed.\n\nattrs\n\nFunction argument\n\nLocated at lib/options.nix:233 in <nixpkgs>.\n\nlib.options.mergeEqualOption\n\n“Merge” option definitions by checking that they all have the same value.\n\nloc\n\nFunction argument\n\ndefs\n\nFunction argument\n\nLocated at lib/options.nix:266 in <nixpkgs>.\n\nlib.options.getValues\n\nType: getValues :: [ { value :: a; } ] -> [a]\n\nExtracts values of all “value” keys of the given list.\n\nExample 184. lib.options.getValues usage example\n\ngetValues [ { value = 1; } { value = 2; } ] // => [ 1 2 ]\ngetValues [ ]                               // => [ ]\n\n\n\n\nLocated at lib/options.nix:286 in <nixpkgs>.\n\nlib.options.getFiles\n\nType: getFiles :: [ { file :: a; } ] -> [a]\n\nExtracts values of all “file” keys of the given list\n\nExample 185. lib.options.getFiles usage example\n\ngetFiles [ { file = \"file1\"; } { file = \"file2\"; } ] // => [ \"file1\" \"file2\" ]\ngetFiles [ ]                                         // => [ ]\n\n\n\n\nLocated at lib/options.nix:296 in <nixpkgs>.\n\nlib.options.scrubOptionValue\n\nThis function recursively removes all derivation attributes from x except for the name attribute.\n\nThis is to make the generation of options.xml much more efficient: the XML representation of derivations is very large (on the order of megabytes) and is not actually used by the manual generator.\n\nThis function was made obsolete by renderOptionValue and is kept for compatibility with out-of-tree code.\n\nx\n\nFunction argument\n\nLocated at lib/options.nix:354 in <nixpkgs>.\n\nlib.options.renderOptionValue\n\nEnsures that the given option value (default or example) is a _typed string by rendering Nix values to literalExpressions.\n\nv\n\nFunction argument\n\nLocated at lib/options.nix:365 in <nixpkgs>.\n\nlib.options.literalExpression\n\nFor use in the defaultText and example option attributes. Causes the given string to be rendered verbatim in the documentation as Nix code. This is necessary for complex values, e.g. functions, or values that depend on other values or packages.\n\ntext\n\nFunction argument\n\nLocated at lib/options.nix:378 in <nixpkgs>.\n\nlib.options.mdDoc\n\nTransition marker for documentation that’s already migrated to markdown syntax. This is a no-op and no longer needed.\n\nLocated at lib/options.nix:387 in <nixpkgs>.\n\nlib.options.literalMD\n\nFor use in the defaultText and example option attributes. Causes the given MD text to be inserted verbatim in the documentation, for when a literalExpression would be too hard to read.\n\ntext\n\nFunction argument\n\nLocated at lib/options.nix:393 in <nixpkgs>.\n\nlib.options.showOption\n\nConvert an option, described as a list of the option parts to a human-readable version.\n\nparts\n\nFunction argument\n\nExample 186. lib.options.showOption usage example\n\n(showOption [\"foo\" \"bar\" \"baz\"]) == \"foo.bar.baz\"\n  (showOption [\"foo\" \"bar.baz\" \"tux\"]) == \"foo.\\\"bar.baz\\\".tux\"\n  (showOption [\"windowManager\" \"2bwm\" \"enable\"]) == \"windowManager.\\\"2bwm\\\".enable\"\n\nPlaceholders will not be quoted as they are not actual values:\n  (showOption [\"foo\" \"*\" \"bar\"]) == \"foo.*.bar\"\n  (showOption [\"foo\" \"<name>\" \"bar\"]) == \"foo.<name>.bar\"\n\n\n\n\nLocated at lib/options.nix:411 in <nixpkgs>.\n\nlib.path: path functions \nlib.path.append\n\nType: append :: Path -> String -> Path\n\nAppend a subpath string to a path.\n\nLike path + (\"/\" + string) but safer, because it errors instead of returning potentially surprising results. More specifically, it checks that the first argument is a path value type, and that the second argument is a valid subpath string.\n\nLaws:\n\nNot influenced by subpath normalisation:\n\nappend p s == append p (subpath.normalise s)\n\npath\n\nThe absolute path to append to\n\nsubpath\n\nThe subpath string to append\n\nExample 187. lib.path.append usage example\n\nappend /foo \"bar/baz\"\n=> /foo/bar/baz\n\n# subpaths don't need to be normalised\nappend /foo \"./bar//baz/./\"\n=> /foo/bar/baz\n\n# can append to root directory\nappend /. \"foo/bar\"\n=> /foo/bar\n\n# first argument needs to be a path value type\nappend \"/foo\" \"bar\"\n=> <error>\n\n# second argument needs to be a valid subpath string\nappend /foo /bar\n=> <error>\nappend /foo \"\"\n=> <error>\nappend /foo \"/bar\"\n=> <error>\nappend /foo \"../bar\"\n=> <error>\n\n\n\n\nLocated at lib/path/default.nix:166 in <nixpkgs>.\n\nlib.path.hasPrefix\n\nType: hasPrefix :: Path -> Path -> Bool\n\nWhether the first path is a component-wise prefix of the second path.\n\nLaws:\n\nhasPrefix p q is only true if q == append p s for some subpath s.\n\nhasPrefix is a non-strict partial order over the set of all path values.\n\npath1\n\nFunction argument\n\nExample 188. lib.path.hasPrefix usage example\n\nhasPrefix /foo /foo/bar\n=> true\nhasPrefix /foo /foo\n=> true\nhasPrefix /foo/bar /foo\n=> false\nhasPrefix /. /foo\n=> true\n\n\n\n\nLocated at lib/path/default.nix:200 in <nixpkgs>.\n\nlib.path.removePrefix\n\nType: removePrefix :: Path -> Path -> String\n\nRemove the first path as a component-wise prefix from the second path. The result is a normalised subpath string.\n\nLaws:\n\nInverts append for normalised subpath string:\n\nremovePrefix p (append p s) == subpath.normalise s\n\npath1\n\nFunction argument\n\nExample 189. lib.path.removePrefix usage example\n\nremovePrefix /foo /foo/bar/baz\n=> \"./bar/baz\"\nremovePrefix /foo /foo\n=> \"./.\"\nremovePrefix /foo/bar /foo\n=> <error>\nremovePrefix /. /foo\n=> \"./foo\"\n\n\n\n\nLocated at lib/path/default.nix:245 in <nixpkgs>.\n\nlib.path.splitRoot\n\nType: splitRoot :: Path -> { root :: Path, subpath :: String }\n\nSplit the filesystem root from a path. The result is an attribute set with these attributes:\n\nroot: The filesystem root of the path, meaning that this directory has no parent directory.\n\nsubpath: The normalised subpath string that when appended to root returns the original path.\n\nLaws:\n\nAppending the root and subpath gives the original path:\n\np ==\n  append\n    (splitRoot p).root\n    (splitRoot p).subpath\n\n\nTrying to get the parent directory of root using readDir returns root itself:\n\ndirOf (splitRoot p).root == (splitRoot p).root\n\npath\n\nThe path to split the root off of\n\nExample 190. lib.path.splitRoot usage example\n\nsplitRoot /foo/bar\n=> { root = /.; subpath = \"./foo/bar\"; }\n\nsplitRoot /.\n=> { root = /.; subpath = \"./.\"; }\n\n# Nix neutralises `..` path components for all path values automatically\nsplitRoot /foo/../bar\n=> { root = /.; subpath = \"./bar\"; }\n\nsplitRoot \"/foo/bar\"\n=> <error>\n\n\n\n\nLocated at lib/path/default.nix:310 in <nixpkgs>.\n\nlib.path.subpath.isValid\n\nType: subpath.isValid :: String -> Bool\n\nWhether a value is a valid subpath string.\n\nA subpath string points to a specific file or directory within an absolute base directory. It is a stricter form of a relative path that excludes .. components, since those could escape the base directory.\n\nThe value is a string.\n\nThe string is not empty.\n\nThe string doesn’t start with a /.\n\nThe string doesn’t contain any .. path components.\n\nvalue\n\nThe value to check\n\nExample 191. lib.path.subpath.isValid usage example\n\n# Not a string\nsubpath.isValid null\n=> false\n\n# Empty string\nsubpath.isValid \"\"\n=> false\n\n# Absolute path\nsubpath.isValid \"/foo\"\n=> false\n\n# Contains a `..` path component\nsubpath.isValid \"../foo\"\n=> false\n\n# Valid subpath\nsubpath.isValid \"foo/bar\"\n=> true\n\n# Doesn't need to be normalised\nsubpath.isValid \"./foo//bar/\"\n=> true\n\n\n\n\nLocated at lib/path/default.nix:365 in <nixpkgs>.\n\nlib.path.subpath.join\n\nType: subpath.join :: [ String ] -> String\n\nJoin subpath strings together using /, returning a normalised subpath string.\n\nLike concatStringsSep \"/\" but safer, specifically:\n\nAll elements must be valid subpath strings.\n\nThe result gets normalised.\n\nThe edge case of an empty list gets properly handled by returning the neutral subpath \"./.\".\n\nLaws:\n\nAssociativity:\n\nsubpath.join [ x (subpath.join [ y z ]) ] == subpath.join [ (subpath.join [ x y ]) z ]\n\n\nIdentity - \"./.\" is the neutral element for normalised paths:\n\nsubpath.join [ ] == \"./.\"\nsubpath.join [ (subpath.normalise p) \"./.\" ] == subpath.normalise p\nsubpath.join [ \"./.\" (subpath.normalise p) ] == subpath.normalise p\n\n\nNormalisation - the result is normalised:\n\nsubpath.join ps == subpath.normalise (subpath.join ps)\n\n\nFor non-empty lists, the implementation is equivalent to normalising the result of concatStringsSep \"/\". Note that the above laws can be derived from this one:\n\nps != [] -> subpath.join ps == subpath.normalise (concatStringsSep \"/\" ps)\n\nsubpaths\n\nThe list of subpaths to join together\n\nExample 192. lib.path.subpath.join usage example\n\nsubpath.join [ \"foo\" \"bar/baz\" ]\n=> \"./foo/bar/baz\"\n\n# normalise the result\nsubpath.join [ \"./foo\" \".\" \"bar//./baz/\" ]\n=> \"./foo/bar/baz\"\n\n# passing an empty list results in the current directory\nsubpath.join [ ]\n=> \"./.\"\n\n# elements must be valid subpath strings\nsubpath.join [ /foo ]\n=> <error>\nsubpath.join [ \"\" ]\n=> <error>\nsubpath.join [ \"/foo\" ]\n=> <error>\nsubpath.join [ \"../foo\" ]\n=> <error>\n\n\n\n\nLocated at lib/path/default.nix:428 in <nixpkgs>.\n\nlib.path.subpath.components\n\nType: subpath.components :: String -> [ String ]\n\nSplit a subpath into its path component strings. Throw an error if the subpath isn’t valid. Note that the returned path components are also valid subpath strings, though they are intentionally not normalised.\n\nLaws:\n\nSplitting a subpath into components and joining the components gives the same subpath but normalised:\n\nsubpath.join (subpath.components s) == subpath.normalise s\n\nsubpath\n\nThe subpath string to split into components\n\nExample 193. lib.path.subpath.components usage example\n\nsubpath.components \".\"\n=> [ ]\n\nsubpath.components \"./foo//bar/./baz/\"\n=> [ \"foo\" \"bar\" \"baz\" ]\n\nsubpath.components \"/foo\"\n=> <error>\n\n\n\n\nLocated at lib/path/default.nix:470 in <nixpkgs>.\n\nlib.path.subpath.normalise\n\nType: subpath.normalise :: String -> String\n\nNormalise a subpath. Throw an error if the subpath isn’t valid.\n\nLimit repeating / to a single one.\n\nRemove redundant . components.\n\nRemove trailing / and /..\n\nAdd leading ./.\n\nLaws:\n\nIdempotency - normalising multiple times gives the same result:\n\nsubpath.normalise (subpath.normalise p) == subpath.normalise p\n\n\nUniqueness - there’s only a single normalisation for the paths that lead to the same file system node:\n\nsubpath.normalise p != subpath.normalise q -> $(realpath ${p}) != $(realpath ${q})\n\n\nDon’t change the result when appended to a Nix path value:\n\nappend base p == append base (subpath.normalise p)\n\n\nDon’t change the path according to realpath:\n\n$(realpath ${p}) == $(realpath ${subpath.normalise p})\n\n\nOnly error on invalid subpaths:\n\nbuiltins.tryEval (subpath.normalise p)).success == subpath.isValid p\n\nsubpath\n\nThe subpath string to normalise\n\nExample 194. lib.path.subpath.normalise usage example\n\n# limit repeating `/` to a single one\nsubpath.normalise \"foo//bar\"\n=> \"./foo/bar\"\n\n# remove redundant `.` components\nsubpath.normalise \"foo/./bar\"\n=> \"./foo/bar\"\n\n# add leading `./`\nsubpath.normalise \"foo/bar\"\n=> \"./foo/bar\"\n\n# remove trailing `/`\nsubpath.normalise \"foo/bar/\"\n=> \"./foo/bar\"\n\n# remove trailing `/.`\nsubpath.normalise \"foo/bar/.\"\n=> \"./foo/bar\"\n\n# Return the current directory as `./.`\nsubpath.normalise \".\"\n=> \"./.\"\n\n# error on `..` path components\nsubpath.normalise \"foo/../bar\"\n=> <error>\n\n# error on empty string\nsubpath.normalise \"\"\n=> <error>\n\n# error on absolute path\nsubpath.normalise \"/foo\"\n=> <error>\n\n\n\n\nLocated at lib/path/default.nix:551 in <nixpkgs>.\n\nlib.filesystem: filesystem functions \nlib.filesystem.pathType\n\nType: pathType :: Path -> String\n\nThe type of a path. The path needs to exist and be accessible. The result is either “directory” for a directory, “regular” for a regular file, “symlink” for a symlink, or “unknown” for anything else.\n\nExample 195. lib.filesystem.pathType usage example\n\npathType /.\n=> \"directory\"\n\npathType /some/file.nix\n=> \"regular\"\n\n\n\n\nLocated at lib/filesystem.nix:33 in <nixpkgs>.\n\nlib.filesystem.pathIsDirectory\n\nType: pathIsDirectory :: Path -> Bool\n\nWhether a path exists and is a directory.\n\npath\n\nFunction argument\n\nExample 196. lib.filesystem.pathIsDirectory usage example\n\npathIsDirectory /.\n=> true\n\npathIsDirectory /this/does/not/exist\n=> false\n\npathIsDirectory /some/file.nix\n=> false\n\n\n\n\nLocated at lib/filesystem.nix:65 in <nixpkgs>.\n\nlib.filesystem.pathIsRegularFile\n\nType: pathIsRegularFile :: Path -> Bool\n\nWhether a path exists and is a regular file, meaning not a symlink or any other special file type.\n\npath\n\nFunction argument\n\nExample 197. lib.filesystem.pathIsRegularFile usage example\n\npathIsRegularFile /.\n=> false\n\npathIsRegularFile /this/does/not/exist\n=> false\n\npathIsRegularFile /some/file.nix\n=> true\n\n\n\n\nLocated at lib/filesystem.nix:84 in <nixpkgs>.\n\nlib.filesystem.haskellPathsInDir\n\nType: Path -> Map String Path\n\nA map of all haskell packages defined in the given path, identified by having a cabal file with the same name as the directory itself.\n\nroot\n\nThe directory within to search\n\nLocated at lib/filesystem.nix:94 in <nixpkgs>.\n\nlib.filesystem.locateDominatingFile\n\nType: RegExp -> Path -> Nullable { path : Path; matches : [ MatchResults ]; }\n\nFind the first directory containing a file matching ‘pattern’ upward from a given ‘file’. Returns ‘null’ if no directories contain a file matching ‘pattern’.\n\npattern\n\nThe pattern to search for\n\nfile\n\nThe file to start searching upward from\n\nLocated at lib/filesystem.nix:117 in <nixpkgs>.\n\nlib.filesystem.listFilesRecursive\n\nType: Path -> [ Path ]\n\nGiven a directory, return a flattened list of all files within it recursively.\n\ndir\n\nThe path to recursively list\n\nLocated at lib/filesystem.nix:145 in <nixpkgs>.\n\nlib.fileset: file set functions \nlib.fileset.toSource\n\nType:\n\ntoSource :: {\n  root :: Path,\n  fileset :: FileSet,\n} -> SourceLike\n\n\nAdd the local files contained in fileset to the store as a single store path rooted at root.\n\nThe result is the store path as a string-like value, making it usable e.g. as the src of a derivation, or in string interpolation:\n\nstdenv.mkDerivation {\n  src = lib.fileset.toSource { ... };\n  # ...\n}\n\n\nThe name of the store path is always source.\n\nstructured function argument\nroot\n\n(required) The local directory path that will correspond to the root of the resulting store path. Paths in strings, including Nix store paths, cannot be passed as root. root has to be a directory.\n\nNote\n\nChanging root only affects the directory structure of the resulting store path, it does not change which files are added to the store. The only way to change which files get added to the store is by changing the fileset attribute.\n\nfileset\n\n(required) The file set whose files to import into the store. File sets can be created using other functions in this library. This argument can also be a path, which gets implicitly coerced to a file set.\n\nNote\n\nIf a directory does not recursively contain any file, it is omitted from the store path contents.\n\nExample 198. lib.fileset.toSource usage example\n\n# Import the current directory into the store\n# but only include files under ./src\ntoSource {\n  root = ./.;\n  fileset = ./src;\n}\n=> \"/nix/store/...-source\"\n\n# Import the current directory into the store\n# but only include ./Makefile and all files under ./src\ntoSource {\n  root = ./.;\n  fileset = union\n    ./Makefile\n    ./src;\n}\n=> \"/nix/store/...-source\"\n\n# Trying to include a file outside the root will fail\ntoSource {\n  root = ./.;\n  fileset = unions [\n    ./Makefile\n    ./src\n    ../LICENSE\n  ];\n}\n=> <error>\n\n# The root needs to point to a directory that contains all the files\ntoSource {\n  root = ../.;\n  fileset = unions [\n    ./Makefile\n    ./src\n    ../LICENSE\n  ];\n}\n=> \"/nix/store/...-source\"\n\n# The root has to be a local filesystem path\ntoSource {\n  root = \"/nix/store/...-source\";\n  fileset = ./.;\n}\n=> <error>\n\n\n\n\nLocated at lib/fileset/default.nix:121 in <nixpkgs>.\n\nlib.fileset.fromSource\n\nType: fromSource :: SourceLike -> FileSet\n\nCreate a file set with the same files as a lib.sources-based value. This does not import any of the files into the store.\n\nThis can be used to gradually migrate from lib.sources-based filtering to lib.fileset.\n\nA file set can be turned back into a source using toSource.\n\nNote\n\nFile sets cannot represent empty directories. Turning the result of this function back into a source using toSource will therefore not preserve empty directories.\n\nsource\n\nFunction argument\n\nExample 199. lib.fileset.fromSource usage example\n\n# There's no cleanSource-like function for file sets yet,\n# but we can just convert cleanSource to a file set and use it that way\ntoSource {\n  root = ./.;\n  fileset = fromSource (lib.sources.cleanSource ./.);\n}\n\n# Keeping a previous sourceByRegex (which could be migrated to `lib.fileset.unions`),\n# but removing a subdirectory using file set functions\ndifference\n  (fromSource (lib.sources.sourceByRegex ./. [\n    \"^README\\.md$\"\n    # This regex includes everything in ./doc\n    \"^doc(/.*)?$\"\n  ])\n  ./doc/generated\n\n# Use cleanSource, but limit it to only include ./Makefile and files under ./src\nintersection\n  (fromSource (lib.sources.cleanSource ./.))\n  (unions [\n    ./Makefile\n    ./src\n  ]);\n\n\n\n\nLocated at lib/fileset/default.nix:240 in <nixpkgs>.\n\nlib.fileset.union\n\nType: union :: FileSet -> FileSet -> FileSet\n\nThe file set containing all files that are in either of two given file sets. This is the same as unions, but takes just two file sets instead of a list. See also Union (set theory).\n\nThe given file sets are evaluated as lazily as possible, with the first argument being evaluated first if needed.\n\nfileset1\n\nThe first file set. This argument can also be a path, which gets implicitly coerced to a file set.\n\nfileset2\n\nThe second file set. This argument can also be a path, which gets implicitly coerced to a file set.\n\nExample 200. lib.fileset.union usage example\n\n# Create a file set containing the file `Makefile`\n# and all files recursively in the `src` directory\nunion ./Makefile ./src\n\n# Create a file set containing the file `Makefile`\n# and the LICENSE file from the parent directory\nunion ./Makefile ../LICENSE\n\n\n\n\nLocated at lib/fileset/default.nix:288 in <nixpkgs>.\n\nlib.fileset.unions\n\nType: unions :: [ FileSet ] -> FileSet\n\nThe file set containing all files that are in any of the given file sets. This is the same as union, but takes a list of file sets instead of just two. See also Union (set theory).\n\nThe given file sets are evaluated as lazily as possible, with earlier elements being evaluated first if needed.\n\nfilesets\n\nA list of file sets. The elements can also be paths, which get implicitly coerced to file sets.\n\nExample 201. lib.fileset.unions usage example\n\n# Create a file set containing selected files\nunions [\n  # Include the single file `Makefile` in the current directory\n  # This errors if the file doesn't exist\n  ./Makefile\n\n  # Recursively include all files in the `src/code` directory\n  # If this directory is empty this has no effect\n  ./src/code\n\n  # Include the files `run.sh` and `unit.c` from the `tests` directory\n  ./tests/run.sh\n  ./tests/unit.c\n\n  # Include the `LICENSE` file from the parent directory\n  ../LICENSE\n]\n\n\n\n\nLocated at lib/fileset/default.nix:340 in <nixpkgs>.\n\nlib.fileset.fileFilter\n\nType:\n\nfileFilter ::\n  ({\n    name :: String,\n    type :: String,\n    ...\n  } -> Bool)\n  -> FileSet\n  -> FileSet\n\n\nFilter a file set to only contain files matching some predicate.\n\npredicate\n\nThe predicate function to call on all files contained in given file set. A file is included in the resulting file set if this function returns true for it.\n\nThis function is called with an attribute set containing these attributes:\n\nname (String): The name of the file\n\ntype (String, one of \"regular\", \"symlink\" or \"unknown\"): The type of the file. This matches result of calling builtins.readFileType on the file’s path.\n\nOther attributes may be added in the future.\n\nfileset\n\nThe file set to filter based on the predicate function\n\nExample 202. lib.fileset.fileFilter usage example\n\n# Include all regular `default.nix` files in the current directory\nfileFilter (file: file.name == \"default.nix\") ./.\n\n# Include all non-Nix files from the current directory\nfileFilter (file: ! hasSuffix \".nix\" file.name) ./.\n\n# Include all files that start with a \".\" in the current directory\nfileFilter (file: hasPrefix \".\" file.name) ./.\n\n# Include all regular files (not symlinks or others) in the current directory\nfileFilter (file: file.type == \"regular\")\n\n\n\n\nLocated at lib/fileset/default.nix:385 in <nixpkgs>.\n\nlib.fileset.intersection\n\nType: intersection :: FileSet -> FileSet -> FileSet\n\nThe file set containing all files that are in both of two given file sets. See also Intersection (set theory).\n\nThe given file sets are evaluated as lazily as possible, with the first argument being evaluated first if needed.\n\nfileset1\n\nThe first file set. This argument can also be a path, which gets implicitly coerced to a file set.\n\nfileset2\n\nThe second file set. This argument can also be a path, which gets implicitly coerced to a file set.\n\nExample 203. lib.fileset.intersection usage example\n\n# Limit the selected files to the ones in ./., so only ./src and ./Makefile\nintersection ./. (unions [ ../LICENSE ./src ./Makefile ])\n\n\n\n\nLocated at lib/fileset/default.nix:423 in <nixpkgs>.\n\nlib.fileset.difference\n\nType: union :: FileSet -> FileSet -> FileSet\n\nThe file set containing all files from the first file set that are not in the second file set. See also Difference (set theory).\n\nThe given file sets are evaluated as lazily as possible, with the first argument being evaluated first if needed.\n\npositive\n\nThe positive file set. The result can only contain files that are also in this file set. This argument can also be a path, which gets implicitly coerced to a file set.\n\nnegative\n\nThe negative file set. The result will never contain files that are also in this file set. This argument can also be a path, which gets implicitly coerced to a file set.\n\nExample 204. lib.fileset.difference usage example\n\n# Create a file set containing all files from the current directory,\n# except ones under ./tests\ndifference ./. ./tests\n\nlet\n  # A set of Nix-related files\n  nixFiles = unions [ ./default.nix ./nix ./tests/default.nix ];\nin\n# Create a file set containing all files under ./tests, except ones in `nixFiles`,\n# meaning only without ./tests/default.nix\ndifference ./tests nixFiles\n\n\n\n\nLocated at lib/fileset/default.nix:471 in <nixpkgs>.\n\nlib.fileset.trace\n\nType: trace :: FileSet -> Any -> Any\n\nIncrementally evaluate and trace a file set in a pretty way. This function is only intended for debugging purposes. The exact tracing format is unspecified and may change.\n\nThis function takes a final argument to return. In comparison, traceVal returns the given file set argument.\n\nThis variant is useful for tracing file sets in the Nix repl.\n\nfileset\n\nThe file set to trace.\n\nThis argument can also be a path, which gets implicitly coerced to a file set.\n\nExample 205. lib.fileset.trace usage example\n\ntrace (unions [ ./Makefile ./src ./tests/run.sh ]) null\n=>\ntrace: /home/user/src/myProject\ntrace: - Makefile (regular)\ntrace: - src (all files in directory)\ntrace: - tests\ntrace:   - run.sh (regular)\nnull\n\n\n\n\nLocated at lib/fileset/default.nix:524 in <nixpkgs>.\n\nlib.fileset.traceVal\n\nType: traceVal :: FileSet -> FileSet\n\nIncrementally evaluate and trace a file set in a pretty way. This function is only intended for debugging purposes. The exact tracing format is unspecified and may change.\n\nThis function returns the given file set. In comparison, trace takes another argument to return.\n\nThis variant is useful for tracing file sets passed as arguments to other functions.\n\nfileset\n\nThe file set to trace and return.\n\nThis argument can also be a path, which gets implicitly coerced to a file set.\n\nExample 206. lib.fileset.traceVal usage example\n\ntoSource {\n  root = ./.;\n  fileset = traceVal (unions [\n    ./Makefile\n    ./src\n    ./tests/run.sh\n  ]);\n}\n=>\ntrace: /home/user/src/myProject\ntrace: - Makefile (regular)\ntrace: - src (all files in directory)\ntrace: - tests\ntrace:   - run.sh (regular)\n\"/nix/store/...-source\"\n\n\n\n\nLocated at lib/fileset/default.nix:571 in <nixpkgs>.\n\nlib.sources: source filtering functions \nlib.sources.commitIdFromGitRepo\n\nGet the commit id of a git repo.\n\npath\n\nFunction argument\n\nExample 207. lib.sources.commitIdFromGitRepo usage example\n\ncommitIdFromGitRepo <nixpkgs/.git>\n\n\n\n\nLocated at lib/sources.nix:271 in <nixpkgs>.\n\nlib.sources.cleanSource\n\nFilters a source tree removing version control files and directories using cleanSourceFilter.\n\nsrc\n\nFunction argument\n\nExample 208. lib.sources.cleanSource usage example\n\ncleanSource ./.\n\n\n\n\nLocated at lib/sources.nix:271 in <nixpkgs>.\n\nlib.sources.cleanSourceWith\n\nLike builtins.filterSource, except it will compose with itself, allowing you to chain multiple calls together without any intermediate copies being put in the nix store.\n\nstructured function argument\nsrc\n\nA path or cleanSourceWith result to filter and/or rename.\n\nfilter\n\nOptional with default value: constant true (include everything) The function will be combined with the && operator such that src.filter is called lazily. For implementing a filter, see https://nixos.org/nix/manual/#builtin-filterSource Type: A function (path -> type -> bool)\n\nname\n\nOptional name to use as part of the store path. This defaults to src.name or otherwise \"source\".\n\nExample 209. lib.sources.cleanSourceWith usage example\n\nlib.cleanSourceWith {\n  filter = f;\n  src = lib.cleanSourceWith {\n    filter = g;\n    src = ./.;\n  };\n}\n# Succeeds!\n\nbuiltins.filterSource f (builtins.filterSource g ./.)\n# Fails!\n\n\n\n\nLocated at lib/sources.nix:271 in <nixpkgs>.\n\nlib.sources.cleanSourceFilter\n\nA basic filter for cleanSourceWith that removes directories of version control system, backup files (*~) and some generated files.\n\nname\n\nFunction argument\n\ntype\n\nFunction argument\n\nLocated at lib/sources.nix:271 in <nixpkgs>.\n\nlib.sources.sourceByRegex\n\nFilter sources by a list of regular expressions.\n\nsrc\n\nFunction argument\n\nregexes\n\nFunction argument\n\nExample 210. lib.sources.sourceByRegex usage example\n\nsrc = sourceByRegex ./my-subproject [\".*\\.py$\" \"^database.sql$\"]\n\n\n\n\nLocated at lib/sources.nix:271 in <nixpkgs>.\n\nlib.sources.sourceFilesBySuffices\n\nType: sourceLike -> [String] -> Source\n\nGet all files ending with the specified suffices from the given source directory or its descendants, omitting files that do not match any suffix. The result of the example below will include files like ./dir/module.c and ./dir/subdir/doc.xml if present.\n\nsrc\n\nPath or source containing the files to be returned\n\nexts\n\nA list of file suffix strings\n\nExample 211. lib.sources.sourceFilesBySuffices usage example\n\nsourceFilesBySuffices ./. [ \".xml\" \".c\" ]\n\n\n\n\nLocated at lib/sources.nix:271 in <nixpkgs>.\n\nlib.sources.trace\n\nType: sources.trace :: sourceLike -> Source\n\nAdd logging to a source, for troubleshooting the filtering behavior.\n\nsrc\n\nSource to debug. The returned source will behave like this source, but also log its filter invocations.\n\nLocated at lib/sources.nix:271 in <nixpkgs>.\n\nlib.cli: command-line serialization functions \nlib.cli.toGNUCommandLineShell\n\nAutomatically convert an attribute set to command-line options.\n\nThis helps protect against malformed command lines and also to reduce boilerplate related to command-line construction for simple use cases.\n\ntoGNUCommandLine returns a list of nix strings. toGNUCommandLineShell returns an escaped shell string.\n\noptions\n\nFunction argument\n\nattrs\n\nFunction argument\n\nExample 212. lib.cli.toGNUCommandLineShell usage example\n\ncli.toGNUCommandLine {} {\n  data = builtins.toJSON { id = 0; };\n  X = \"PUT\";\n  retry = 3;\n  retry-delay = null;\n  url = [ \"https://example.com/foo\" \"https://example.com/bar\" ];\n  silent = false;\n  verbose = true;\n}\n=> [\n  \"-X\" \"PUT\"\n  \"--data\" \"{\\\"id\\\":0}\"\n  \"--retry\" \"3\"\n  \"--url\" \"https://example.com/foo\"\n  \"--url\" \"https://example.com/bar\"\n  \"--verbose\"\n]\n\ncli.toGNUCommandLineShell {} {\n  data = builtins.toJSON { id = 0; };\n  X = \"PUT\";\n  retry = 3;\n  retry-delay = null;\n  url = [ \"https://example.com/foo\" \"https://example.com/bar\" ];\n  silent = false;\n  verbose = true;\n}\n=> \"'-X' 'PUT' '--data' '{\\\"id\\\":0}' '--retry' '3' '--url' 'https://example.com/foo' '--url' 'https://example.com/bar' '--verbose'\";\n\n\n\n\nLocated at lib/cli.nix:42 in <nixpkgs>.\n\nlib.gvariant: GVariant formatted string serialization functions \nlib.gvariant.isGVariant\n\nType: isGVariant :: Any -> Bool\n\nCheck if a value is a GVariant value\n\nv\n\nFunction argument\n\nLocated at lib/gvariant.nix:53 in <nixpkgs>.\n\nlib.gvariant.mkValue\n\nType: mkValue :: Any -> gvariant\n\nReturns the GVariant value that most closely matches the given Nix value. If no GVariant value can be found unambiguously then error is thrown.\n\nv\n\nFunction argument\n\nLocated at lib/gvariant.nix:61 in <nixpkgs>.\n\nlib.gvariant.mkArray\n\nType: mkArray :: [Any] -> gvariant\n\nReturns the GVariant array from the given type of the elements and a Nix list.\n\nelems\n\nFunction argument\n\nExample 213. lib.gvariant.mkArray usage example\n\n# Creating a string array\nlib.gvariant.mkArray [ \"a\" \"b\" \"c\" ]\n\n\n\n\nLocated at lib/gvariant.nix:84 in <nixpkgs>.\n\nlib.gvariant.mkEmptyArray\n\nType: mkEmptyArray :: gvariant.type -> gvariant\n\nReturns the GVariant array from the given empty Nix list.\n\nelemType\n\nFunction argument\n\nExample 214. lib.gvariant.mkEmptyArray usage example\n\n# Creating an empty string array\nlib.gvariant.mkEmptyArray (lib.gvariant.type.string)\n\n\n\n\nLocated at lib/gvariant.nix:105 in <nixpkgs>.\n\nlib.gvariant.mkVariant\n\nType: mkVariant :: Any -> gvariant\n\nReturns the GVariant variant from the given Nix value. Variants are containers of different GVariant type.\n\nelem\n\nFunction argument\n\nExample 215. lib.gvariant.mkVariant usage example\n\nlib.gvariant.mkArray [\n  (lib.gvariant.mkVariant \"a string\")\n  (lib.gvariant.mkVariant (lib.gvariant.mkInt32 1))\n]\n\n\n\n\nLocated at lib/gvariant.nix:122 in <nixpkgs>.\n\nlib.gvariant.mkDictionaryEntry\n\nType: mkDictionaryEntry :: String -> Any -> gvariant\n\nReturns the GVariant dictionary entry from the given key and value.\n\nname\n\nThe key of the entry\n\nvalue\n\nThe value of the entry\n\nExample 216. lib.gvariant.mkDictionaryEntry usage example\n\n# A dictionary describing an Epiphany’s search provider\n[\n  (lib.gvariant.mkDictionaryEntry \"url\" (lib.gvariant.mkVariant \"https://duckduckgo.com/?q=%s&t=epiphany\"))\n  (lib.gvariant.mkDictionaryEntry \"bang\" (lib.gvariant.mkVariant \"!d\"))\n  (lib.gvariant.mkDictionaryEntry \"name\" (lib.gvariant.mkVariant \"DuckDuckGo\"))\n]\n\n\n\n\nLocated at lib/gvariant.nix:141 in <nixpkgs>.\n\nlib.gvariant.mkMaybe\n\nType: mkMaybe :: gvariant.type -> Any -> gvariant\n\nReturns the GVariant maybe from the given element type.\n\nelemType\n\nFunction argument\n\nelem\n\nFunction argument\n\nLocated at lib/gvariant.nix:160 in <nixpkgs>.\n\nlib.gvariant.mkNothing\n\nType: mkNothing :: gvariant.type -> gvariant\n\nReturns the GVariant nothing from the given element type.\n\nelemType\n\nFunction argument\n\nLocated at lib/gvariant.nix:174 in <nixpkgs>.\n\nlib.gvariant.mkJust\n\nType: mkJust :: Any -> gvariant\n\nReturns the GVariant just from the given Nix value.\n\nelem\n\nFunction argument\n\nLocated at lib/gvariant.nix:181 in <nixpkgs>.\n\nlib.gvariant.mkTuple\n\nType: mkTuple :: [Any] -> gvariant\n\nReturns the GVariant tuple from the given Nix list.\n\nelems\n\nFunction argument\n\nLocated at lib/gvariant.nix:188 in <nixpkgs>.\n\nlib.gvariant.mkBoolean\n\nType: mkBoolean :: Bool -> gvariant\n\nReturns the GVariant boolean from the given Nix bool value.\n\nv\n\nFunction argument\n\nLocated at lib/gvariant.nix:203 in <nixpkgs>.\n\nlib.gvariant.mkString\n\nType: mkString :: String -> gvariant\n\nReturns the GVariant string from the given Nix string value.\n\nv\n\nFunction argument\n\nLocated at lib/gvariant.nix:213 in <nixpkgs>.\n\nlib.gvariant.mkObjectpath\n\nType: mkObjectpath :: String -> gvariant\n\nReturns the GVariant object path from the given Nix string value.\n\nv\n\nFunction argument\n\nLocated at lib/gvariant.nix:224 in <nixpkgs>.\n\nlib.gvariant.mkUchar\n\nType: mkUchar :: Int -> gvariant\n\nReturns the GVariant uchar from the given Nix int value.\n\nLocated at lib/gvariant.nix:234 in <nixpkgs>.\n\nlib.gvariant.mkInt16\n\nType: mkInt16 :: Int -> gvariant\n\nReturns the GVariant int16 from the given Nix int value.\n\nLocated at lib/gvariant.nix:241 in <nixpkgs>.\n\nlib.gvariant.mkUint16\n\nType: mkUint16 :: Int -> gvariant\n\nReturns the GVariant uint16 from the given Nix int value.\n\nLocated at lib/gvariant.nix:248 in <nixpkgs>.\n\nlib.gvariant.mkInt32\n\nType: mkInt32 :: Int -> gvariant\n\nReturns the GVariant int32 from the given Nix int value.\n\nv\n\nFunction argument\n\nLocated at lib/gvariant.nix:255 in <nixpkgs>.\n\nlib.gvariant.mkUint32\n\nType: mkUint32 :: Int -> gvariant\n\nReturns the GVariant uint32 from the given Nix int value.\n\nLocated at lib/gvariant.nix:265 in <nixpkgs>.\n\nlib.gvariant.mkInt64\n\nType: mkInt64 :: Int -> gvariant\n\nReturns the GVariant int64 from the given Nix int value.\n\nLocated at lib/gvariant.nix:272 in <nixpkgs>.\n\nlib.gvariant.mkUint64\n\nType: mkUint64 :: Int -> gvariant\n\nReturns the GVariant uint64 from the given Nix int value.\n\nLocated at lib/gvariant.nix:279 in <nixpkgs>.\n\nlib.gvariant.mkDouble\n\nType: mkDouble :: Float -> gvariant\n\nReturns the GVariant double from the given Nix float value.\n\nv\n\nFunction argument\n\nLocated at lib/gvariant.nix:286 in <nixpkgs>.\n\nlib.customisation: Functions to customise (derivation-related) functions, derivatons, or attribute sets \nlib.customisation.overrideDerivation\n\nType: overrideDerivation :: Derivation -> ( Derivation -> AttrSet ) -> Derivation\n\noverrideDerivation drv f takes a derivation (i.e., the result of a call to the builtin function derivation) and returns a new derivation in which the attributes of the original are overridden according to the function f. The function f is called with the original derivation attributes.\n\noverrideDerivation allows certain “ad-hoc” customisation scenarios (e.g. in ~/.config/nixpkgs/config.nix). For instance, if you want to “patch” the derivation returned by a package function in Nixpkgs to build another version than what the function itself provides.\n\nFor another application, see build-support/vm, where this function is used to build arbitrary derivations inside a QEMU virtual machine.\n\nNote that in order to preserve evaluation errors, the new derivation’s outPath depends on the old one’s, which means that this function cannot be used in circular situations when the old derivation also depends on the new one.\n\nYou should in general prefer drv.overrideAttrs over this function; see the nixpkgs manual for more information on overriding.\n\ndrv\n\nFunction argument\n\nf\n\nFunction argument\n\nExample 217. lib.customisation.overrideDerivation usage example\n\nmySed = overrideDerivation pkgs.gnused (oldAttrs: {\n  name = \"sed-4.2.2-pre\";\n  src = fetchurl {\n    url = ftp://alpha.gnu.org/gnu/sed/sed-4.2.2-pre.tar.bz2;\n    hash = \"sha256-MxBJRcM2rYzQYwJ5XKxhXTQByvSg5jZc5cSHEZoB2IY=\";\n  };\n  patches = [];\n});\n\n\n\n\nLocated at lib/customisation.nix:43 in <nixpkgs>.\n\nlib.customisation.makeOverridable\n\nType: makeOverridable :: (AttrSet -> a) -> AttrSet -> a\n\nmakeOverridable takes a function from attribute set to attribute set and injects override attribute which can be used to override arguments of the function.\n\nPlease refer to documentation on <pkg>.overrideDerivation to learn about overrideDerivation and caveats related to its use.\n\nf\n\nFunction argument\n\nExample 218. lib.customisation.makeOverridable usage example\n\nnix-repl> x = {a, b}: { result = a + b; }\n\nnix-repl> y = lib.makeOverridable x { a = 1; b = 2; }\n\nnix-repl> y\n{ override = «lambda»; overrideDerivation = «lambda»; result = 3; }\n\nnix-repl> y.override { a = 10; }\n{ override = «lambda»; overrideDerivation = «lambda»; result = 12; }\n\n\n\n\nLocated at lib/customisation.nix:79 in <nixpkgs>.\n\nlib.customisation.callPackageWith\n\nType: callPackageWith :: AttrSet -> ((AttrSet -> a) | Path) -> AttrSet -> a\n\nCall the package function in the file fn with the required arguments automatically. The function is called with the arguments args, but any missing arguments are obtained from autoArgs. This function is intended to be partially parameterised, e.g.,\n\ncallPackage = callPackageWith pkgs;\npkgs = {\n  libfoo = callPackage ./foo.nix { };\n  libbar = callPackage ./bar.nix { };\n};\n\n\nIf the libbar function expects an argument named libfoo, it is automatically passed as an argument. Overrides or missing arguments can be supplied in args, e.g.\n\nlibbar = callPackage ./bar.nix {\n  libfoo = null;\n  enableX11 = true;\n};\n\nautoArgs\n\nFunction argument\n\nfn\n\nFunction argument\n\nargs\n\nFunction argument\n\nLocated at lib/customisation.nix:141 in <nixpkgs>.\n\nlib.customisation.callPackagesWith\n\nType: callPackagesWith :: AttrSet -> ((AttrSet -> AttrSet) | Path) -> AttrSet -> AttrSet\n\nLike callPackage, but for a function that returns an attribute set of derivations. The override function is added to the individual attributes.\n\nautoArgs\n\nFunction argument\n\nfn\n\nFunction argument\n\nargs\n\nFunction argument\n\nLocated at lib/customisation.nix:202 in <nixpkgs>.\n\nlib.customisation.extendDerivation\n\nType: extendDerivation :: Bool -> Any -> Derivation -> Derivation\n\nAdd attributes to each output of a derivation without changing the derivation itself and check a given condition when evaluating.\n\ncondition\n\nFunction argument\n\npassthru\n\nFunction argument\n\ndrv\n\nFunction argument\n\nLocated at lib/customisation.nix:223 in <nixpkgs>.\n\nlib.customisation.hydraJob\n\nType: hydraJob :: (Derivation | Null) -> (Derivation | Null)\n\nStrip a derivation of all non-essential attributes, returning only those needed by hydra-eval-jobs. Also strictly evaluate the result to ensure that there are no thunks kept alive to prevent garbage collection.\n\ndrv\n\nFunction argument\n\nLocated at lib/customisation.nix:261 in <nixpkgs>.\n\nlib.customisation.makeScope\n\nType: makeScope :: (AttrSet -> ((AttrSet -> a) | Path) -> AttrSet -> a) -> (AttrSet -> AttrSet) -> AttrSet\n\nMake a set of packages with a common scope. All packages called with the provided callPackage will be evaluated with the same arguments. Any package in the set may depend on any other. The overrideScope' function allows subsequent modification of the package set in a consistent way, i.e. all packages in the set will be called with the overridden packages. The package sets may be hierarchical: the packages in the set are called with the scope provided by newScope and the set provides a newScope attribute which can form the parent scope for later package sets.\n\nnewScope\n\nFunction argument\n\nf\n\nFunction argument\n\nLocated at lib/customisation.nix:303 in <nixpkgs>.\n\nlib.customisation.makeScopeWithSplicing\n\nbackward compatibility with old uncurried form; deprecated\n\nsplicePackages\n\nFunction argument\n\nnewScope\n\nFunction argument\n\notherSplices\n\nFunction argument\n\nkeep\n\nFunction argument\n\nextra\n\nFunction argument\n\nf\n\nFunction argument\n\nLocated at lib/customisation.nix:317 in <nixpkgs>.\n\nlib.customisation.makeScopeWithSplicing'\n\nType:\n\nmakeScopeWithSplicing' ::\n  { splicePackages :: Splice -> AttrSet\n  , newScope :: AttrSet -> ((AttrSet -> a) | Path) -> AttrSet -> a\n  }\n  -> { otherSplices :: Splice, keep :: AttrSet -> AttrSet, extra :: AttrSet -> AttrSet }\n  -> AttrSet\n\nSplice ::\n  { pkgsBuildBuild :: AttrSet\n  , pkgsBuildHost :: AttrSet\n  , pkgsBuildTarget :: AttrSet\n  , pkgsHostHost :: AttrSet\n  , pkgsHostTarget :: AttrSet\n  , pkgsTargetTarget :: AttrSet\n  }\n\n\nLike makeScope, but aims to support cross compilation. It’s still ugly, but hopefully it helps a little bit.\n\nstructured function argument\nsplicePackages\n\nFunction argument\n\nnewScope\n\nFunction argument\n\nstructured function argument\notherSplices\n\nFunction argument\n\nkeep\n\nFunction argument\n\nextra\n\nFunction argument\n\nf\n\nFunction argument\n\nLocated at lib/customisation.nix:343 in <nixpkgs>.\n\nGenerators \n\nGenerators are functions that create file formats from nix data structures, e. g. for configuration files. There are generators available for: INI, JSON and YAML\n\nAll generators follow a similar call interface: generatorName configFunctions data, where configFunctions is an attrset of user-defined functions that format nested parts of the content. They each have common defaults, so often they do not need to be set manually. An example is mkSectionName ? (name: libStr.escape [ \"[\" \"]\" ] name) from the INI generator. It receives the name of a section and sanitizes it. The default mkSectionName escapes [ and ] with a backslash.\n\nGenerators can be fine-tuned to produce exactly the file format required by your application/service. One example is an INI-file format which uses : as separator, the strings \"yes\"/\"no\" as boolean values and requires all string values to be quoted:\n\nwith lib;\nlet\n  customToINI = generators.toINI {\n    # specifies how to format a key/value pair\n    mkKeyValue = generators.mkKeyValueDefault {\n      # specifies the generated string for a subset of nix values\n      mkValueString = v:\n             if v == true then ''\"yes\"''\n        else if v == false then ''\"no\"''\n        else if isString v then ''\"${v}\"''\n        # and delegates all other values to the default generator\n        else generators.mkValueStringDefault {} v;\n    } \":\";\n  };\n\n# the INI file can now be given as plain old nix values\nin customToINI {\n  main = {\n    pushinfo = true;\n    autopush = false;\n    host = \"localhost\";\n    port = 42;\n  };\n  mergetool = {\n    merge = \"diff3\";\n  };\n}\n\n\nThis will produce the following INI file as nix string:\n\n[main]\nautopush:\"no\"\nhost:\"localhost\"\nport:42\npushinfo:\"yes\"\nstr\\:ange:\"very::strange\"\n\n[mergetool]\nmerge:\"diff3\"\n\nNote\n\nNix store paths can be converted to strings by enclosing a derivation attribute like so: \"${drv}\".\n\nDetailed documentation for each generator can be found in lib/generators.nix.\n\nDebugging Nix Expressions \n\nNix is a unityped, dynamic language, this means every value can potentially appear anywhere. Since it is also non-strict, evaluation order and what ultimately is evaluated might surprise you. Therefore it is important to be able to debug nix expressions.\n\nIn the lib/debug.nix file you will find a number of functions that help (pretty-)printing values while evaluation is running. You can even specify how deep these values should be printed recursively, and transform them on the fly. Please consult the docstrings in lib/debug.nix for usage information.\n\nprefer-remote-fetch overlay \n\nprefer-remote-fetch is an overlay that download sources on remote builder. This is useful when the evaluating machine has a slow upload while the builder can fetch faster directly from the source. To use it, put the following snippet as a new overlay:\n\nself: super:\n  (super.prefer-remote-fetch self super)\n\n\nA full configuration example for that sets the overlay up for your own account, could look like this\n\n$ mkdir ~/.config/nixpkgs/overlays/\n$ cat > ~/.config/nixpkgs/overlays/prefer-remote-fetch.nix <<EOF\n  self: super: super.prefer-remote-fetch self super\nEOF\n\npkgs.nix-gitignore \nUsage\ngitignore files in subdirectories\n\npkgs.nix-gitignore is a function that acts similarly to builtins.filterSource but also allows filtering with the help of the gitignore format.\n\nUsage \n\npkgs.nix-gitignore exports a number of functions, but you’ll most likely need either gitignoreSource or gitignoreSourcePure. As their first argument, they both accept either 1. a file with gitignore lines or 2. a string with gitignore lines, or 3. a list of either of the two. They will be concatenated into a single big string.\n\n{ pkgs ? import <nixpkgs> {} }:\n\n nix-gitignore.gitignoreSource [] ./source\n     # Simplest version\n\n nix-gitignore.gitignoreSource \"supplemental-ignores\\n\" ./source\n     # This one reads the ./source/.gitignore and concats the auxiliary ignores\n\n nix-gitignore.gitignoreSourcePure \"ignore-this\\nignore-that\\n\" ./source\n     # Use this string as gitignore, don't read ./source/.gitignore.\n\n nix-gitignore.gitignoreSourcePure [\"ignore-this\\nignore-that\\n\", ~/.gitignore] ./source\n     # It also accepts a list (of strings and paths) that will be concatenated\n     # once the paths are turned to strings via readFile.\n\n\nThese functions are derived from the Filter functions by setting the first filter argument to (_: _: true):\n\ngitignoreSourcePure = gitignoreFilterSourcePure (_: _: true);\ngitignoreSource = gitignoreFilterSource (_: _: true);\n\n\nThose filter functions accept the same arguments the builtins.filterSource function would pass to its filters, thus fn: gitignoreFilterSourcePure fn \"\" should be extensionally equivalent to filterSource. The file is blacklisted if it’s blacklisted by either your filter or the gitignoreFilter.\n\nIf you want to make your own filter from scratch, you may use\n\ngitignoreFilter = ign: root: filterPattern (gitignoreToPatterns ign) root;\n\ngitignore files in subdirectories \n\nIf you wish to use a filter that would search for .gitignore files in subdirectories, just like git does by default, use this function:\n\ngitignoreFilterRecursiveSource = filter: patterns: root:\n# OR\ngitignoreRecursiveSource = gitignoreFilterSourcePure (_: _: true);\n\nFile sets \nImplicit coercion from paths to file sets\n\nThe lib.fileset library allows you to work with file sets. A file set is a mathematical set of local files that can be added to the Nix store for use in Nix derivations. File sets are easy and safe to use, providing obvious and composable semantics with good error messages to prevent mistakes.\n\nSee the function reference for function-specific documentation.\n\nImplicit coercion from paths to file sets \n\nAll functions accepting file sets as arguments can also accept paths as arguments. Such path arguments are implicitly coerced to file sets containing all files under that path:\n\nA path to a file turns into a file set containing that single file.\n\nA path to a directory turns into a file set containing all files recursively in that directory.\n\nIf the path points to a non-existent location, an error is thrown.\n\nNote\n\nJust like in Git, file sets cannot represent empty directories. Because of this, a path to a directory that contains no files (recursively) will turn into a file set containing no files.\n\nNote\n\nFile set coercion does not add any of the files under the coerced paths to the store. Only the toSource function adds files to the Nix store, and only those files contained in the fileset argument. This is in contrast to using paths in string interpolation, which does add the entire referenced path to the store.\n\nExample\n\nAssume we are in a local directory with a file hierarchy like this:\n\n├─ a/\n│  ├─ x (file)\n│  └─ b/\n│     └─ y (file)\n└─ c/\n   └─ d/\n\n\nHere’s a listing of which files get included when different path expressions get coerced to file sets:\n\n./. as a file set contains both a/x and a/b/y (c/ does not contain any files and is therefore omitted).\n\n./a as a file set contains both a/x and a/b/y.\n\n./a/x as a file set contains only a/x.\n\n./a/b as a file set contains only a/b/y.\n\n./c as a file set is empty, since neither c nor c/d contain any files.\n\nModule System \n\nTable of Contents\n\nIntroduction\nlib.evalModules\nIntroduction \n\nThe module system is a language for handling configuration, implemented as a Nix library.\n\nCompared to plain Nix, it adds documentation, type checking and composition or extensibility.\n\nNote\n\nThis chapter is new and not complete yet. For a gentle introduction to the module system, in the context of NixOS, see Writing NixOS Modules in the NixOS manual.\n\nlib.evalModules \nParameters\nReturn value\n\nEvaluate a set of modules. This function is typically only used once per application (e.g. once in NixOS, once in Home Manager, …).\n\nParameters \nmodules\n\nA list of modules. These are merged together to form the final configuration.\n\nspecialArgs\n\nAn attribute set of module arguments that can be used in imports.\n\nThis is in contrast to config._module.args, which is only available after all imports have been resolved.\n\nclass\n\nIf the class attribute is set and non-null, the module system will reject imports with a different _class declaration.\n\nThe class value should be a string in lower camel case.\n\nIf applicable, the class should match the “prefix” of the attributes used in (experimental) flakes. Some examples are:\n\nnixos as in flake.nixosModules\n\nnixosTest: modules that constitute a NixOS VM test\n\nprefix\n\nA list of strings representing the location at or below which all options are evaluated. This is used by types.submodule to improve error reporting and find the implicit name module argument.\n\nReturn value \n\nThe result is an attribute set with the following attributes:\n\noptions\n\nThe nested attribute set of all option declarations.\n\nconfig\n\nThe nested attribute set of all option values.\n\ntype\n\nA module system type. This type is an instance of types.submoduleWith containing the current modules.\n\nThe option definitions that are typed with this type will extend the current set of modules, like extendModules.\n\nHowever, the value returned from the type is just the config, like any submodule.\n\nIf you’re familiar with prototype inheritance, you can think of this evalModules invocation as the prototype, and usages of this type as the instances.\n\nThis type is also available to the modules as the module argument moduleType.\n\nextendModules\n\nA function similar to evalModules but building on top of the already passed modules. Its arguments, modules and specialArgs are added to the existing values.\n\nIf you’re familiar with prototype inheritance, you can think of the current, actual evalModules invocation as the prototype, and the return value of extendModules as the instance.\n\nThis functionality is also available to modules as the extendModules module argument.\n\nNote\n\nEvaluation Performance\n\nextendModules returns a configuration that shares very little with the original evalModules invocation, because the module arguments may be different.\n\nSo if you have a configuration that has been (or will be) largely evaluated, almost none of the computation is shared with the configuration returned by extendModules.\n\nThe real work of module evaluation happens while computing the values in config and options, so multiple invocations of extendModules have a particularly small cost, as long as only the final config and options are evaluated.\n\nIf you do reference multiple config (or options) from before and after extendModules, evaluation performance is the same as with multiple evalModules invocations, because the new modules’ ability to override existing configuration fundamentally requires constructing a new config and options fixpoint.\n\n_module\n\nA portion of the configuration tree which is elided from config.\n\n_type\n\nA nominal type marker, always \"configuration\".\n\nclass\n\nThe class argument.\n\nStandard environment \n\nTable of Contents\n\nThe Standard Environment\nMeta-attributes\nMultiple-output packages\nCross-compilation\nPlatform Notes\nThe Standard Environment \n\nTable of Contents\n\nUsing stdenv\nTools provided by stdenv\nSpecifying dependencies\nAttributes\nPhases\nShell functions and utilities\nPackage setup hooks\nPurity in Nixpkgs\nHardening in Nixpkgs\n\nThe standard build environment in the Nix Packages collection provides an environment for building Unix packages that does a lot of common build tasks automatically. In fact, for Unix packages that use the standard ./configure; make; make install build interface, you don’t need to write a build script at all; the standard environment does everything automatically. If stdenv doesn’t do what you need automatically, you can easily customise or override the various build phases.\n\nUsing stdenv \nBuilding a stdenv package in nix-shell\n\nTo build a package with the standard environment, you use the function stdenv.mkDerivation, instead of the primitive built-in function derivation, e.g.\n\nstdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  src = fetchurl {\n    url = \"http://example.org/libfoo-1.2.3.tar.bz2\";\n    hash = \"sha256-tWxU/LANbQE32my+9AXyt3nCT7NBVfJ45CX757EMT3Q=\";\n  };\n}\n\n\n(stdenv needs to be in scope, so if you write this in a separate Nix expression from pkgs/all-packages.nix, you need to pass it as a function argument.) Specifying a name and a src is the absolute minimum Nix requires. For convenience, you can also use pname and version attributes and mkDerivation will automatically set name to \"${pname}-${version}\" by default. Since RFC 0035, this is preferred for packages in Nixpkgs, as it allows us to reuse the version easily:\n\nstdenv.mkDerivation rec {\n  pname = \"libfoo\";\n  version = \"1.2.3\";\n  src = fetchurl {\n    url = \"http://example.org/libfoo-source-${version}.tar.bz2\";\n    hash = \"sha256-tWxU/LANbQE32my+9AXyt3nCT7NBVfJ45CX757EMT3Q=\";\n  };\n}\n\n\nMany packages have dependencies that are not provided in the standard environment. It’s usually sufficient to specify those dependencies in the buildInputs attribute:\n\nstdenv.mkDerivation {\n  pname = \"libfoo\";\n  version = \"1.2.3\";\n  ...\n  buildInputs = [libbar perl ncurses];\n}\n\n\nThis attribute ensures that the bin subdirectories of these packages appear in the PATH environment variable during the build, that their include subdirectories are searched by the C compiler, and so on. (See the section called “Package setup hooks” for details.)\n\nOften it is necessary to override or modify some aspect of the build. To make this easier, the standard environment breaks the package build into a number of phases, all of which can be overridden or modified individually: unpacking the sources, applying patches, configuring, building, and installing. (There are some others; see the section called “Phases”.) For instance, a package that doesn’t supply a makefile but instead has to be compiled “manually” could be handled like this:\n\nstdenv.mkDerivation {\n  pname = \"fnord\";\n  version = \"4.5\";\n  ...\n  buildPhase = ''\n    gcc foo.c -o foo\n  '';\n  installPhase = ''\n    mkdir -p $out/bin\n    cp foo $out/bin\n  '';\n}\n\n\n(Note the use of ''-style string literals, which are very convenient for large multi-line script fragments because they don’t need escaping of \" and \\, and because indentation is intelligently removed.)\n\nThere are many other attributes to customise the build. These are listed in the section called “Attributes”.\n\nWhile the standard environment provides a generic builder, you can still supply your own build script:\n\nstdenv.mkDerivation {\n  pname = \"libfoo\";\n  version = \"1.2.3\";\n  ...\n  builder = ./builder.sh;\n}\n\n\nwhere the builder can do anything it wants, but typically starts with\n\nsource $stdenv/setup\n\n\nto let stdenv set up the environment (e.g. by resetting PATH and populating it from build inputs). If you want, you can still use stdenv’s generic builder:\n\nsource $stdenv/setup\n\nbuildPhase() {\n  echo \"... this is my custom build phase ...\"\n  gcc foo.c -o foo\n}\n\ninstallPhase() {\n  mkdir -p $out/bin\n  cp foo $out/bin\n}\n\ngenericBuild\n\nBuilding a stdenv package in nix-shell \n\nTo build a stdenv package in a nix-shell, enter a shell, find the phases you wish to build, then invoke genericBuild manually:\n\nGo to an empty directory, invoke nix-shell with the desired package, and from inside the shell, set the output variables to a writable directory:\n\ncd \"$(mktemp -d)\"\nnix-shell '<nixpkgs>' -A some_package\nexport out=$(pwd)/out\n\n\nNext, invoke the desired parts of the build. First, run the phases that generate a working copy of the sources, which will change directory to the sources for you:\n\nphases=\"${prePhases[*]:-} unpackPhase patchPhase\" genericBuild\n\n\nThen, run more phases up until the failure is reached. For example, if the failure is in the build phase, the following phases would be required:\n\nphases=\"${preConfigurePhases[*]:-} configurePhase ${preBuildPhases[*]:-} buildPhase\" genericBuild\n\n\nRe-run a single phase as many times as necessary to examine the failure like so:\n\nphases=\"buildPhase\" genericBuild\n\n\nTo modify a phase, first print it with\n\necho \"$buildPhase\"\n\n\nOr, if that is empty, for instance, if it is using a function:\n\ntype buildPhase\n\n\nthen change it in a text editor, and paste it back to the terminal.\n\nNote\n\nThis method may have some inconsistencies in environment variables and behaviour compared to a normal build within the Nix build sandbox. The following is a non-exhaustive list of such differences:\n\nTMP, TMPDIR, and similar variables likely point to non-empty directories that the build might conflict with files in.\n\nOutput store paths are not writable, so the variables for outputs need to be overridden to writable paths.\n\nOther environment variables may be inconsistent with a nix-build either due to nix-shell’s initialization script or due to the use of nix-shell without the --pure option.\n\nIf the build fails differently inside the shell than in the sandbox, consider using breakpointHook and invoking nix-build instead. The --keep-failed option for nix-build may also be useful to examine the build directory of a failed build.\n\nTools provided by stdenv \n\nThe standard environment provides the following packages:\n\nThe GNU C Compiler, configured with C and C++ support.\n\nGNU coreutils (contains a few dozen standard Unix commands).\n\nGNU findutils (contains find).\n\nGNU diffutils (contains diff, cmp).\n\nGNU sed.\n\nGNU grep.\n\nGNU awk.\n\nGNU tar.\n\ngzip, bzip2 and xz.\n\nGNU Make.\n\nBash. This is the shell used for all builders in the Nix Packages collection. Not using /bin/sh removes a large source of portability problems.\n\nThe patch command.\n\nOn Linux, stdenv also includes the patchelf utility.\n\nSpecifying dependencies \nOverview\nReference\n\nBuild systems often require more dependencies than just what stdenv provides. This section describes attributes accepted by stdenv.mkDerivation that can be used to make these dependencies available to the build system.\n\nOverview \n\nA full reference of the different kinds of dependencies is provided in the section called “Reference”, but here is an overview of the most common ones. It should cover most use cases.\n\nAdd dependencies to nativeBuildInputs if they are executed during the build:\n\nthose which are needed on $PATH during the build, for example cmake and pkg-config\n\nsetup hooks, for example makeWrapper\n\ninterpreters needed by patchShebangs for build scripts (with the --build flag), which can be the case for e.g. perl\n\nAdd dependencies to buildInputs if they will end up copied or linked into the final output or otherwise used at runtime:\n\nlibraries used by compilers, for example zlib,\n\ninterpreters needed by patchShebangs for scripts which are installed, which can be the case for e.g. perl\n\nNote\n\nThese criteria are independent.\n\nFor example, software using Wayland usually needs the wayland library at runtime, so wayland should be added to buildInputs. But it also executes the wayland-scanner program as part of the build to generate code, so wayland should also be added to nativeBuildInputs.\n\nDependencies needed only to run tests are similarly classified between native (executed during build) and non-native (executed at runtime):\n\nnativeCheckInputs for test tools needed on $PATH (such as ctest) and setup hooks (for example pytestCheckHook)\n\ncheckInputs for libraries linked into test executables (for example the qcheck OCaml package)\n\nThese dependencies are only injected when doCheck is set to true.\n\nExample\n\nConsider for example this simplified derivation for solo5, a sandboxing tool:\n\nstdenv.mkDerivation rec {\n  pname = \"solo5\";\n  version = \"0.7.5\";\n\n  src = fetchurl {\n    url = \"https://github.com/Solo5/solo5/releases/download/v${version}/solo5-v${version}.tar.gz\";\n    hash = \"sha256-viwrS9lnaU8sTGuzK/+L/PlMM/xRRtgVuK5pixVeDEw=\";\n  };\n\n  nativeBuildInputs = [ makeWrapper pkg-config ];\n  buildInputs = [ libseccomp ];\n\n  postInstall = ''\n    substituteInPlace $out/bin/solo5-virtio-mkimage \\\n      --replace \"/usr/lib/syslinux\" \"${syslinux}/share/syslinux\" \\\n      --replace \"/usr/share/syslinux\" \"${syslinux}/share/syslinux\" \\\n      --replace \"cp \" \"cp --no-preserve=mode \"\n\n    wrapProgram $out/bin/solo5-virtio-mkimage \\\n      --prefix PATH : ${lib.makeBinPath [ dosfstools mtools parted syslinux ]}\n  '';\n\n  doCheck = true;\n  nativeCheckInputs = [ util-linux qemu ];\n  checkPhase = '' [elided] '';\n}\n\n\nmakeWrapper is a setup hook, i.e., a shell script sourced by the generic builder of stdenv. It is thus executed during the build and must be added to nativeBuildInputs.\n\npkg-config is a build tool which the configure script of solo5 expects to be on $PATH during the build: therefore, it must be added to nativeBuildInputs.\n\nlibseccomp is a library linked into $out/bin/solo5-elftool. As it is used at runtime, it must be added to buildInputs.\n\nTests need qemu and getopt (from util-linux) on $PATH, these must be added to nativeCheckInputs.\n\nSome dependencies are injected directly in the shell code of phases: syslinux, dosfstools, mtools, and parted. In this specific case, they will end up in the output of the derivation ($out here). As Nix marks dependencies whose absolute path is present in the output as runtime dependencies, adding them to buildInputs is not required.\n\nFor more complex cases, like libraries linked into an executable which is then executed as part of the build system, see the section called “Reference”.\n\nReference \n\nAs described in the Nix manual, almost any *.drv store path in a derivation’s attribute set will induce a dependency on that derivation. mkDerivation, however, takes a few attributes intended to include all the dependencies of a package. This is done both for structure and consistency, but also so that certain other setup can take place. For example, certain dependencies need their bin directories added to the PATH. That is built-in, but other setup is done via a pluggable mechanism that works in conjunction with these dependency attributes. See the section called “Package setup hooks” for details.\n\nDependencies can be broken down along three axes: their host and target platforms relative to the new derivation’s, and whether they are propagated. The platform distinctions are motivated by cross compilation; see Cross-compilation for exactly what each platform means. [1] But even if one is not cross compiling, the platforms imply whether or not the dependency is needed at run-time or build-time, a concept that makes perfect sense outside of cross compilation. By default, the run-time/build-time distinction is just a hint for mental clarity, but with strictDeps set it is mostly enforced even in the native case.\n\nThe extension of PATH with dependencies, alluded to above, proceeds according to the relative platforms alone. The process is carried out only for dependencies whose host platform matches the new derivation’s build platform i.e. dependencies which run on the platform where the new derivation will be built. [2] For each dependency <dep> of those dependencies, dep/bin, if present, is added to the PATH environment variable.\n\nA dependency is said to be propagated when some of its other-transitive (non-immediate) downstream dependencies also need it as an immediate dependency. [3]\n\nIt is important to note that dependencies are not necessarily propagated as the same sort of dependency that they were before, but rather as the corresponding sort so that the platform rules still line up. To determine the exact rules for dependency propagation, we start by assigning to each dependency a couple of ternary numbers (-1 for build, 0 for host, and 1 for target) representing its dependency type, which captures how its host and target platforms are each “offset” from the depending derivation’s host and target platforms. The following table summarize the different combinations that can be obtained:\n\nhost → target\tattribute name\toffset\nbuild --> build\tdepsBuildBuild\t-1, -1\nbuild --> host\tnativeBuildInputs\t-1, 0\nbuild --> target\tdepsBuildTarget\t-1, 1\nhost --> host\tdepsHostHost\t0, 0\nhost --> target\tbuildInputs\t0, 1\ntarget --> target\tdepsTargetTarget\t1, 1\n\nAlgorithmically, we traverse propagated inputs, accumulating every propagated dependency’s propagated dependencies and adjusting them to account for the “shift in perspective” described by the current dependency’s platform offsets. This results is sort of a transitive closure of the dependency relation, with the offsets being approximately summed when two dependency links are combined. We also prune transitive dependencies whose combined offsets go out-of-bounds, which can be viewed as a filter over that transitive closure removing dependencies that are blatantly absurd.\n\nWe can define the process precisely with Natural Deduction using the inference rules. This probably seems a bit obtuse, but so is the bash code that actually implements it! [4] They’re confusing in very different ways so… hopefully if something doesn’t make sense in one presentation, it will in the other!\n\nlet mapOffset(h, t, i) = i + (if i <= 0 then h else t - 1)\n\npropagated-dep(h0, t0, A, B)\npropagated-dep(h1, t1, B, C)\nh0 + h1 in {-1, 0, 1}\nh0 + t1 in {-1, 0, 1}\n-------------------------------------- Transitive property\npropagated-dep(mapOffset(h0, t0, h1),\n               mapOffset(h0, t0, t1),\n               A, C)\n\nlet mapOffset(h, t, i) = i + (if i <= 0 then h else t - 1)\n\ndep(h0, t0, A, B)\npropagated-dep(h1, t1, B, C)\nh0 + h1 in {-1, 0, 1}\nh0 + t1 in {-1, 0, -1}\n----------------------------- Take immediate dependencies' propagated dependencies\npropagated-dep(mapOffset(h0, t0, h1),\n               mapOffset(h0, t0, t1),\n               A, C)\n\npropagated-dep(h, t, A, B)\n----------------------------- Propagated dependencies count as dependencies\ndep(h, t, A, B)\n\n\nSome explanation of this monstrosity is in order. In the common case, the target offset of a dependency is the successor to the target offset: t = h + 1. That means that:\n\nlet f(h, t, i) = i + (if i <= 0 then h else t - 1)\nlet f(h, h + 1, i) = i + (if i <= 0 then h else (h + 1) - 1)\nlet f(h, h + 1, i) = i + (if i <= 0 then h else h)\nlet f(h, h + 1, i) = i + h\n\n\nThis is where “sum-like” comes in from above: We can just sum all of the host offsets to get the host offset of the transitive dependency. The target offset is the transitive dependency is the host offset + 1, just as it was with the dependencies composed to make this transitive one; it can be ignored as it doesn’t add any new information.\n\nBecause of the bounds checks, the uncommon cases are h = t and h + 2 = t. In the former case, the motivation for mapOffset is that since its host and target platforms are the same, no transitive dependency of it should be able to “discover” an offset greater than its reduced target offsets. mapOffset effectively “squashes” all its transitive dependencies’ offsets so that none will ever be greater than the target offset of the original h = t package. In the other case, h + 1 is skipped over between the host and target offsets. Instead of squashing the offsets, we need to “rip” them apart so no transitive dependencies’ offset is that one.\n\nOverall, the unifying theme here is that propagation shouldn’t be introducing transitive dependencies involving platforms the depending package is unaware of. [One can imagine the depending package asking for dependencies with the platforms it knows about; other platforms it doesn’t know how to ask for. The platform description in that scenario is a kind of unforgeable capability.] The offset bounds checking and definition of mapOffset together ensure that this is the case. Discovering a new offset is discovering a new platform, and since those platforms weren’t in the derivation “spec” of the needing package, they cannot be relevant. From a capability perspective, we can imagine that the host and target platforms of a package are the capabilities a package requires, and the depending package must provide the capability to the dependency.\n\nVariables specifying dependencies\ndepsBuildBuild\n\nA list of dependencies whose host and target platforms are the new derivation’s build platform. These are programs and libraries used at build time that produce programs and libraries also used at build time. If the dependency doesn’t care about the target platform (i.e. isn’t a compiler or similar tool), put it in nativeBuildInputs instead. The most common use of this buildPackages.stdenv.cc, the default C compiler for this role. That example crops up more than one might think in old commonly used C libraries.\n\nSince these packages are able to be run at build-time, they are always added to the PATH, as described above. But since these packages are only guaranteed to be able to run then, they shouldn’t persist as run-time dependencies. This isn’t currently enforced, but could be in the future.\n\nnativeBuildInputs\n\nA list of dependencies whose host platform is the new derivation’s build platform, and target platform is the new derivation’s host platform. These are programs and libraries used at build-time that, if they are a compiler or similar tool, produce code to run at run-time—i.e. tools used to build the new derivation. If the dependency doesn’t care about the target platform (i.e. isn’t a compiler or similar tool), put it here, rather than in depsBuildBuild or depsBuildTarget. This could be called depsBuildHost but nativeBuildInputs is used for historical continuity.\n\nSince these packages are able to be run at build-time, they are added to the PATH, as described above. But since these packages are only guaranteed to be able to run then, they shouldn’t persist as run-time dependencies. This isn’t currently enforced, but could be in the future.\n\ndepsBuildTarget\n\nA list of dependencies whose host platform is the new derivation’s build platform, and target platform is the new derivation’s target platform. These are programs used at build time that produce code to run with code produced by the depending package. Most commonly, these are tools used to build the runtime or standard library that the currently-being-built compiler will inject into any code it compiles. In many cases, the currently-being-built-compiler is itself employed for that task, but when that compiler won’t run (i.e. its build and host platform differ) this is not possible. Other times, the compiler relies on some other tool, like binutils, that is always built separately so that the dependency is unconditional.\n\nThis is a somewhat confusing concept to wrap one’s head around, and for good reason. As the only dependency type where the platform offsets, -1 and 1, are not adjacent integers, it requires thinking of a bootstrapping stage two away from the current one. It and its use-case go hand in hand and are both considered poor form: try to not need this sort of dependency, and try to avoid building standard libraries and runtimes in the same derivation as the compiler produces code using them. Instead strive to build those like a normal library, using the newly-built compiler just as a normal library would. In short, do not use this attribute unless you are packaging a compiler and are sure it is needed.\n\nSince these packages are able to run at build time, they are added to the PATH, as described above. But since these packages are only guaranteed to be able to run then, they shouldn’t persist as run-time dependencies. This isn’t currently enforced, but could be in the future.\n\ndepsHostHost\n\nA list of dependencies whose host and target platforms match the new derivation’s host platform. In practice, this would usually be tools used by compilers for macros or a metaprogramming system, or libraries used by the macros or metaprogramming code itself. It’s always preferable to use a depsBuildBuild dependency in the derivation being built over a depsHostHost on the tool doing the building for this purpose.\n\nbuildInputs\n\nA list of dependencies whose host platform and target platform match the new derivation’s. This would be called depsHostTarget but for historical continuity. If the dependency doesn’t care about the target platform (i.e. isn’t a compiler or similar tool), put it here, rather than in depsBuildBuild.\n\nThese are often programs and libraries used by the new derivation at run-time, but that isn’t always the case. For example, the machine code in a statically-linked library is only used at run-time, but the derivation containing the library is only needed at build-time. Even in the dynamic case, the library may also be needed at build-time to appease the linker.\n\ndepsTargetTarget\n\nA list of dependencies whose host platform matches the new derivation’s target platform. These are packages that run on the target platform, e.g. the standard library or run-time deps of standard library that a compiler insists on knowing about. It’s poor form in almost all cases for a package to depend on another from a future stage [future stage corresponding to positive offset]. Do not use this attribute unless you are packaging a compiler and are sure it is needed.\n\ndepsBuildBuildPropagated\n\nThe propagated equivalent of depsBuildBuild. This perhaps never ought to be used, but it is included for consistency [see below for the others].\n\npropagatedNativeBuildInputs\n\nThe propagated equivalent of nativeBuildInputs. This would be called depsBuildHostPropagated but for historical continuity. For example, if package Y has propagatedNativeBuildInputs = [X], and package Z has buildInputs = [Y], then package Z will be built as if it included package X in its nativeBuildInputs. If instead, package Z has nativeBuildInputs = [Y], then Z will be built as if it included X in the depsBuildBuild of package Z, because of the sum of the two -1 host offsets.\n\ndepsBuildTargetPropagated\n\nThe propagated equivalent of depsBuildTarget. This is prefixed for the same reason of alerting potential users.\n\ndepsHostHostPropagated\n\nThe propagated equivalent of depsHostHost.\n\npropagatedBuildInputs\n\nThe propagated equivalent of buildInputs. This would be called depsHostTargetPropagated but for historical continuity.\n\ndepsTargetTargetPropagated\n\nThe propagated equivalent of depsTargetTarget. This is prefixed for the same reason of alerting potential users.\n\nAttributes \nVariables affecting stdenv initialisation\nAttributes affecting build properties\nSpecial variables\nFixed-point arguments of mkDerivation\nVariables affecting stdenv initialisation \nNIX_DEBUG\n\nA number between 0 and 7 indicating how much information to log. If set to 1 or higher, stdenv will print moderate debugging information during the build. In particular, the gcc and ld wrapper scripts will print out the complete command line passed to the wrapped tools. If set to 6 or higher, the stdenv setup script will be run with set -x tracing. If set to 7 or higher, the gcc and ld wrapper scripts will also be run with set -x tracing.\n\nAttributes affecting build properties \nenableParallelBuilding\n\nIf set to true, stdenv will pass specific flags to make and other build tools to enable parallel building with up to build-cores workers.\n\nUnless set to false, some build systems with good support for parallel building including cmake, meson, and qmake will set it to true.\n\nSpecial variables \npassthru\n\nThis is an attribute set which can be filled with arbitrary values. For example:\n\npassthru = {\n  foo = \"bar\";\n  baz = {\n    value1 = 4;\n    value2 = 5;\n  };\n}\n\n\nValues inside it are not passed to the builder, so you can change them without triggering a rebuild. However, they can be accessed outside of a derivation directly, as if they were set inside a derivation itself, e.g. hello.baz.value1. We don’t specify any usage or schema of passthru - it is meant for values that would be useful outside the derivation in other parts of a Nix expression (e.g. in other derivations). An example would be to convey some specific dependency of your derivation which contains a program with plugins support. Later, others who make derivations with plugins can use passed-through dependency to ensure that their plugin would be binary-compatible with built program.\n\npassthru.updateScript\n\nA script to be run by maintainers/scripts/update.nix when the package is matched. The attribute can contain one of the following:\n\nan executable file, either on the file system:\n\npassthru.updateScript = ./update.sh;\n\n\nor inside the expression itself:\n\npassthru.updateScript = writeScript \"update-zoom-us\" ''\n  #!/usr/bin/env nix-shell\n  #!nix-shell -i bash -p curl pcre common-updater-scripts\n\n  set -eu -o pipefail\n\n  version=\"$(curl -sI https://zoom.us/client/latest/zoom_x86_64.tar.xz | grep -Fi 'Location:' | pcregrep -o1 '/(([0-9]\\.?)+)/')\"\n  update-source-version zoom-us \"$version\"\n'';\n\n\na list, a script followed by arguments to be passed to it:\n\npassthru.updateScript = [ ../../update.sh pname \"--requested-release=unstable\" ];\n\n\nan attribute set containing:\n\ncommand – a string or list in the format expected by passthru.updateScript.\n\nattrPath (optional) – a string containing the canonical attribute path for the package. If present, it will be passed to the update script instead of the attribute path on which the package was discovered during Nixpkgs traversal.\n\nsupportedFeatures (optional) – a list of the extra features the script supports.\n\npassthru.updateScript = {\n  command = [ ../../update.sh pname ];\n  attrPath = pname;\n  supportedFeatures = [ … ];\n};\n\nTip\n\nA common pattern is to use the nix-update-script attribute provided in Nixpkgs, which runs nix-update:\n\npassthru.updateScript = nix-update-script { };\n\n\nFor simple packages, this is often enough, and will ensure that the package is updated automatically by nixpkgs-update when a new version is released. The update bot runs periodically to attempt to automatically update packages, and will run passthru.updateScript if set. While not strictly necessary if the project is listed on Repology, using nix-update-script allows the package to update via many more sources (e.g. GitHub releases).\n\nHow update scripts are executed?\n\nUpdate scripts are to be invoked by maintainers/scripts/update.nix script. You can run nix-shell maintainers/scripts/update.nix in the root of Nixpkgs repository for information on how to use it. update.nix offers several modes for selecting packages to update (e.g. select by attribute path, traverse Nixpkgs and filter by maintainer, etc.), and it will execute update scripts for all matched packages that have an updateScript attribute.\n\nEach update script will be passed the following environment variables:\n\nUPDATE_NIX_NAME – content of the name attribute of the updated package.\n\nUPDATE_NIX_PNAME – content of the pname attribute of the updated package.\n\nUPDATE_NIX_OLD_VERSION – content of the version attribute of the updated package.\n\nUPDATE_NIX_ATTR_PATH – attribute path the update.nix discovered the package on (or the canonical attrPath when available). Example: pantheon.elementary-terminal\n\nNote\n\nAn update script will be usually run from the root of the Nixpkgs repository but you should not rely on that. Also note that update.nix executes update scripts in parallel by default so you should avoid running git commit or any other commands that cannot handle that.\n\nTip\n\nWhile update scripts should not create commits themselves, maintainers/scripts/update.nix supports automatically creating commits when running it with --argstr commit true. If you need to customize commit message, you can have the update script implement commit feature.\n\nSupported features\ncommit\n\nThis feature allows update scripts to ask update.nix to create Git commits.\n\nWhen support of this feature is declared, whenever the update script exits with 0 return status, it is expected to print a JSON list containing an object described below for each updated attribute to standard output.\n\nWhen update.nix is run with --argstr commit true arguments, it will create a separate commit for each of the objects. An empty list can be returned when the script did not update any files, for example, when the package is already at the latest version.\n\nThe commit object contains the following values:\n\nattrPath – a string containing attribute path.\n\noldVersion – a string containing old version.\n\nnewVersion – a string containing new version.\n\nfiles – a non-empty list of file paths (as strings) to add to the commit.\n\ncommitBody (optional) – a string with extra content to be appended to the default commit message (useful for adding changelog links).\n\ncommitMessage (optional) – a string to use instead of the default commit message.\n\nIf the returned array contains exactly one object (e.g. [{}]), all values are optional and will be determined automatically.\n\nExample 219. Standard output of an update script using commit feature\n\n[\n  {\n    \"attrPath\": \"volume_key\",\n    \"oldVersion\": \"0.3.11\",\n    \"newVersion\": \"0.3.12\",\n    \"files\": [\n      \"/path/to/nixpkgs/pkgs/development/libraries/volume-key/default.nix\"\n    ]\n  }\n]\n\n\n\nFixed-point arguments of mkDerivation \n\nIf you pass a function to mkDerivation, it will receive as its argument the final arguments, including the overrides when reinvoked via overrideAttrs. For example:\n\nmkDerivation (finalAttrs: {\n  pname = \"hello\";\n  withFeature = true;\n  configureFlags =\n    lib.optionals finalAttrs.withFeature [\"--with-feature\"];\n})\n\n\nNote that this does not use the rec keyword to reuse withFeature in configureFlags. The rec keyword works at the syntax level and is unaware of overriding.\n\nInstead, the definition references finalAttrs, allowing users to change withFeature consistently with overrideAttrs.\n\nfinalAttrs also contains the attribute finalPackage, which includes the output paths, etc.\n\nLet’s look at a more elaborate example to understand the differences between various bindings:\n\n# `pkg` is the _original_ definition (for illustration purposes)\nlet pkg =\n  mkDerivation (finalAttrs: {\n    # ...\n\n    # An example attribute\n    packages = [];\n\n    # `passthru.tests` is a commonly defined attribute.\n    passthru.tests.simple = f finalAttrs.finalPackage;\n\n    # An example of an attribute containing a function\n    passthru.appendPackages = packages':\n      finalAttrs.finalPackage.overrideAttrs (newSelf: super: {\n        packages = super.packages ++ packages';\n      });\n\n    # For illustration purposes; referenced as\n    # `(pkg.overrideAttrs(x)).finalAttrs` etc in the text below.\n    passthru.finalAttrs = finalAttrs;\n    passthru.original = pkg;\n  });\nin pkg\n\n\nUnlike the pkg binding in the above example, the finalAttrs parameter always references the final attributes. For instance (pkg.overrideAttrs(x)).finalAttrs.finalPackage is identical to pkg.overrideAttrs(x), whereas (pkg.overrideAttrs(x)).original is the same as the original pkg.\n\nSee also the section about passthru.tests.\n\nPhases \nControlling phases\nThe unpack phase\nThe patch phase\nThe configure phase\nThe build phase\nThe check phase\nThe install phase\nThe fixup phase\nThe installCheck phase\nThe distribution phase\n\nstdenv.mkDerivation sets the Nix derivation’s builder to a script that loads the stdenv setup.sh bash library and calls genericBuild. Most packaging functions rely on this default builder.\n\nThis generic command invokes a number of phases. Package builds are split into phases to make it easier to override specific parts of the build (e.g., unpacking the sources or installing the binaries).\n\nEach phase can be overridden in its entirety either by setting the environment variable namePhase to a string containing some shell commands to be executed, or by redefining the shell function namePhase. The former is convenient to override a phase from the derivation, while the latter is convenient from a build script. However, typically one only wants to add some commands to a phase, e.g. by defining postInstall or preFixup, as skipping some of the default actions may have unexpected consequences. The default script for each phase is defined in the file pkgs/stdenv/generic/setup.sh.\n\nWhen overriding a phase, for example installPhase, it is important to start with runHook preInstall and end it with runHook postInstall, otherwise preInstall and postInstall will not be run. Even if you don’t use them directly, it is good practice to do so anyways for downstream users who would want to add a postInstall by overriding your derivation.\n\nWhile inside an interactive nix-shell, if you wanted to run all phases in the order they would be run in an actual build, you can invoke genericBuild yourself.\n\nControlling phases \n\nThere are a number of variables that control what phases are executed and in what order:\n\nVariables affecting phase control\nphases\n\nSpecifies the phases. You can change the order in which phases are executed, or add new phases, by setting this variable. If it’s not set, the default value is used, which is $prePhases unpackPhase patchPhase $preConfigurePhases configurePhase $preBuildPhases buildPhase checkPhase $preInstallPhases installPhase fixupPhase installCheckPhase $preDistPhases distPhase $postPhases.\n\nIt is discouraged to set this variable, as it is easy to miss some important functionality hidden in some of the less obviously needed phases (like fixupPhase which patches the shebang of scripts). Usually, if you just want to add a few phases, it’s more convenient to set one of the variables below (such as preInstallPhases).\n\nprePhases\n\nAdditional phases executed before any of the default phases.\n\npreConfigurePhases\n\nAdditional phases executed just before the configure phase.\n\npreBuildPhases\n\nAdditional phases executed just before the build phase.\n\npreInstallPhases\n\nAdditional phases executed just before the install phase.\n\npreFixupPhases\n\nAdditional phases executed just before the fixup phase.\n\npreDistPhases\n\nAdditional phases executed just before the distribution phase.\n\npostPhases\n\nAdditional phases executed after any of the default phases.\n\nThe unpack phase \n\nThe unpack phase is responsible for unpacking the source code of the package. The default implementation of unpackPhase unpacks the source files listed in the src environment variable to the current directory. It supports the following files by default:\n\nTar files\n\nThese can optionally be compressed using gzip (.tar.gz, .tgz or .tar.Z), bzip2 (.tar.bz2, .tbz2 or .tbz) or xz (.tar.xz, .tar.lzma or .txz).\n\nZip files\n\nZip files are unpacked using unzip. However, unzip is not in the standard environment, so you should add it to nativeBuildInputs yourself.\n\nDirectories in the Nix store\n\nThese are copied to the current directory. The hash part of the file name is stripped, e.g. /nix/store/1wydxgby13cz...-my-sources would be copied to my-sources.\n\nAdditional file types can be supported by setting the unpackCmd variable (see below).\n\nVariables controlling the unpack phase\nsrcs / src\n\nThe list of source files or directories to be unpacked or copied. One of these must be set. Note that if you use srcs, you should also set sourceRoot or setSourceRoot.\n\nsourceRoot\n\nAfter unpacking all of src and srcs, if neither of sourceRoot and setSourceRoot are set, unpackPhase of the generic builder checks that the unpacking produced a single directory and moves the current working directory into it.\n\nIf unpackPhase produces multiple source directories, you should set sourceRoot to the name of the intended directory. You can also set sourceRoot = \".\"; if you want to control it yourself in a later phase.\n\nFor example, if your want your build to start in a sub-directory inside your sources, and you are using fetchzip-derived src (like fetchFromGitHub or similar), you need to set sourceRoot = \"${src.name}/my-sub-directory\".\n\nsetSourceRoot\n\nAlternatively to setting sourceRoot, you can set setSourceRoot to a shell command to be evaluated by the unpack phase after the sources have been unpacked. This command must set sourceRoot.\n\nFor example, if you are using fetchurl on an archive file that gets unpacked into a single directory the name of which changes between package versions, and you want your build to start in its sub-directory, you need to set setSourceRoot = \"sourceRoot=$(echo */my-sub-directory)\";, or in the case of multiple sources, you could use something more specific, like setSourceRoot = \"sourceRoot=$(echo ${pname}-*/my-sub-directory)\";.\n\npreUnpack\n\nHook executed at the start of the unpack phase.\n\npostUnpack\n\nHook executed at the end of the unpack phase.\n\ndontUnpack\n\nSet to true to skip the unpack phase.\n\ndontMakeSourcesWritable\n\nIf set to 1, the unpacked sources are not made writable. By default, they are made writable to prevent problems with read-only sources. For example, copied store directories would be read-only without this.\n\nunpackCmd\n\nThe unpack phase evaluates the string $unpackCmd for any unrecognised file. The path to the current source file is contained in the curSrc variable.\n\nThe patch phase \n\nThe patch phase applies the list of patches defined in the patches variable.\n\nVariables controlling the patch phase\ndontPatch\n\nSet to true to skip the patch phase.\n\npatches\n\nThe list of patches. They must be in the format accepted by the patch command, and may optionally be compressed using gzip (.gz), bzip2 (.bz2) or xz (.xz).\n\npatchFlags\n\nFlags to be passed to patch. If not set, the argument -p1 is used, which causes the leading directory component to be stripped from the file names in each patch.\n\nprePatch\n\nHook executed at the start of the patch phase.\n\npostPatch\n\nHook executed at the end of the patch phase.\n\nThe configure phase \n\nThe configure phase prepares the source tree for building. The default configurePhase runs ./configure (typically an Autoconf-generated script) if it exists.\n\nVariables controlling the configure phase\nconfigureScript\n\nThe name of the configure script. It defaults to ./configure if it exists; otherwise, the configure phase is skipped. This can actually be a command (like perl ./Configure.pl).\n\nconfigureFlags\n\nA list of strings passed as additional arguments to the configure script.\n\ndontConfigure\n\nSet to true to skip the configure phase.\n\nconfigureFlagsArray\n\nA shell array containing additional arguments passed to the configure script. You must use this instead of configureFlags if the arguments contain spaces.\n\ndontAddPrefix\n\nBy default, the flag --prefix=$prefix is added to the configure flags. If this is undesirable, set this variable to true.\n\nprefix\n\nThe prefix under which the package must be installed, passed via the --prefix option to the configure script. It defaults to $out.\n\nprefixKey\n\nThe key to use when specifying the prefix. By default, this is set to --prefix= as that is used by the majority of packages.\n\ndontAddStaticConfigureFlags\n\nBy default, when building statically, stdenv will try to add build system appropriate configure flags to try to enable static builds.\n\nIf this is undesirable, set this variable to true.\n\ndontAddDisableDepTrack\n\nBy default, the flag --disable-dependency-tracking is added to the configure flags to speed up Automake-based builds. If this is undesirable, set this variable to true.\n\ndontFixLibtool\n\nBy default, the configure phase applies some special hackery to all files called ltmain.sh before running the configure script in order to improve the purity of Libtool-based packages [5] . If this is undesirable, set this variable to true.\n\ndontDisableStatic\n\nBy default, when the configure script has --enable-static, the option --disable-static is added to the configure flags.\n\nIf this is undesirable, set this variable to true. It is automatically set to true when building statically, for example through pkgsStatic.\n\nconfigurePlatforms\n\nBy default, when cross compiling, the configure script has --build=... and --host=... passed. Packages can instead pass [ \"build\" \"host\" \"target\" ] or a subset to control exactly which platform flags are passed. Compilers and other tools can use this to also pass the target platform. [6]\n\npreConfigure\n\nHook executed at the start of the configure phase.\n\npostConfigure\n\nHook executed at the end of the configure phase.\n\nThe build phase \n\nThe build phase is responsible for actually building the package (e.g. compiling it). The default buildPhase calls make if a file named Makefile, makefile or GNUmakefile exists in the current directory (or the makefile is explicitly set); otherwise it does nothing.\n\nVariables controlling the build phase\ndontBuild\n\nSet to true to skip the build phase.\n\nmakefile\n\nThe file name of the Makefile.\n\nmakeFlags\n\nA list of strings passed as additional flags to make. These flags are also used by the default install and check phase. For setting make flags specific to the build phase, use buildFlags (see below).\n\nmakeFlags = [ \"PREFIX=$(out)\" ];\n\nNote\n\nThe flags are quoted in bash, but environment variables can be specified by using the make syntax.\n\nmakeFlagsArray\n\nA shell array containing additional arguments passed to make. You must use this instead of makeFlags if the arguments contain spaces, e.g.\n\npreBuild = ''\n  makeFlagsArray+=(CFLAGS=\"-O0 -g\" LDFLAGS=\"-lfoo -lbar\")\n'';\n\n\nNote that shell arrays cannot be passed through environment variables, so you cannot set makeFlagsArray in a derivation attribute (because those are passed through environment variables): you have to define them in shell code.\n\nbuildFlags / buildFlagsArray\n\nA list of strings passed as additional flags to make. Like makeFlags and makeFlagsArray, but only used by the build phase.\n\npreBuild\n\nHook executed at the start of the build phase.\n\npostBuild\n\nHook executed at the end of the build phase.\n\nYou can set flags for make through the makeFlags variable.\n\nBefore and after running make, the hooks preBuild and postBuild are called, respectively.\n\nThe check phase \n\nThe check phase checks whether the package was built correctly by running its test suite. The default checkPhase calls make $checkTarget, but only if the doCheck variable is enabled.\n\nVariables controlling the check phase\ndoCheck\n\nControls whether the check phase is executed. By default it is skipped, but if doCheck is set to true, the check phase is usually executed. Thus you should set\n\ndoCheck = true;\n\n\nin the derivation to enable checks. The exception is cross compilation. Cross compiled builds never run tests, no matter how doCheck is set, as the newly-built program won’t run on the platform used to build it.\n\nmakeFlags / makeFlagsArray / makefile\n\nSee the build phase for details.\n\ncheckTarget\n\nThe make target that runs the tests. If unset, use check if it exists, otherwise test; if neither is found, do nothing.\n\ncheckFlags / checkFlagsArray\n\nA list of strings passed as additional flags to make. Like makeFlags and makeFlagsArray, but only used by the check phase.\n\ncheckInputs\n\nA list of host dependencies used by the phase, usually libraries linked into executables built during tests. This gets included in buildInputs when doCheck is set.\n\nnativeCheckInputs\n\nA list of native dependencies used by the phase, notably tools needed on $PATH. This gets included in nativeBuildInputs when doCheck is set.\n\npreCheck\n\nHook executed at the start of the check phase.\n\npostCheck\n\nHook executed at the end of the check phase.\n\nThe install phase \n\nThe install phase is responsible for installing the package in the Nix store under out. The default installPhase creates the directory $out and calls make install.\n\nVariables controlling the install phase\ndontInstall\n\nSet to true to skip the install phase.\n\nmakeFlags / makeFlagsArray / makefile\n\nSee the build phase for details.\n\ninstallTargets\n\nThe make targets that perform the installation. Defaults to install. Example:\n\ninstallTargets = \"install-bin install-doc\";\n\ninstallFlags / installFlagsArray\n\nA list of strings passed as additional flags to make. Like makeFlags and makeFlagsArray, but only used by the install phase.\n\npreInstall\n\nHook executed at the start of the install phase.\n\npostInstall\n\nHook executed at the end of the install phase.\n\nThe fixup phase \n\nThe fixup phase performs (Nix-specific) post-processing actions on the files installed under $out by the install phase. The default fixupPhase does the following:\n\nIt moves the man/, doc/ and info/ subdirectories of $out to share/.\n\nIt strips libraries and executables of debug information.\n\nOn Linux, it applies the patchelf command to ELF executables and libraries to remove unused directories from the RPATH in order to prevent unnecessary runtime dependencies.\n\nIt rewrites the interpreter paths of shell scripts to paths found in PATH. E.g., /usr/bin/perl will be rewritten to /nix/store/some-perl/bin/perl found in PATH. See the section called “patch-shebangs.sh” for details.\n\nVariables controlling the fixup phase\ndontFixup\n\nSet to true to skip the fixup phase.\n\ndontStrip\n\nIf set, libraries and executables are not stripped. By default, they are.\n\ndontStripHost\n\nLike dontStrip, but only affects the strip command targeting the package’s host platform. Useful when supporting cross compilation, but otherwise feel free to ignore.\n\ndontStripTarget\n\nLike dontStrip, but only affects the strip command targeting the packages’ target platform. Useful when supporting cross compilation, but otherwise feel free to ignore.\n\ndontMoveSbin\n\nIf set, files in $out/sbin are not moved to $out/bin. By default, they are.\n\nstripAllList\n\nList of directories to search for libraries and executables from which all symbols should be stripped. By default, it’s empty. Stripping all symbols is risky, since it may remove not just debug symbols but also ELF information necessary for normal execution.\n\nstripAllListTarget\n\nLike stripAllList, but only applies to packages’ target platform. By default, it’s empty. Useful when supporting cross compilation.\n\nstripAllFlags\n\nFlags passed to the strip command applied to the files in the directories listed in stripAllList. Defaults to -s (i.e. --strip-all).\n\nstripDebugList\n\nList of directories to search for libraries and executables from which only debugging-related symbols should be stripped. It defaults to lib lib32 lib64 libexec bin sbin.\n\nstripDebugListTarget\n\nLike stripDebugList, but only applies to packages’ target platform. By default, it’s empty. Useful when supporting cross compilation.\n\nstripDebugFlags\n\nFlags passed to the strip command applied to the files in the directories listed in stripDebugList. Defaults to -S (i.e. --strip-debug).\n\nstripExclude\n\nA list of filenames or path patterns to avoid stripping. A file is excluded if its name or path (from the derivation root) matches.\n\nThis example prevents all *.rlib files from being stripped:\n\nstdenv.mkDerivation {\n  # ...\n  stripExclude = [ \"*.rlib\" ]\n}\n\n\nThis example prevents files within certain paths from being stripped:\n\nstdenv.mkDerivation {\n  # ...\n  stripExclude = [ \"lib/modules/*/build/* ]\n}\n\ndontPatchELF\n\nIf set, the patchelf command is not used to remove unnecessary RPATH entries. Only applies to Linux.\n\ndontPatchShebangs\n\nIf set, scripts starting with #! do not have their interpreter paths rewritten to paths in the Nix store. See the section called “patch-shebangs.sh” on how patching shebangs works.\n\ndontPruneLibtoolFiles\n\nIf set, libtool .la files associated with shared libraries won’t have their dependency_libs field cleared.\n\nforceShare\n\nThe list of directories that must be moved from $out to $out/share. Defaults to man doc info.\n\nsetupHook\n\nA package can export a setup hook by setting this variable. The setup hook, if defined, is copied to $out/nix-support/setup-hook. Environment variables are then substituted in it using substituteAll.\n\npreFixup\n\nHook executed at the start of the fixup phase.\n\npostFixup\n\nHook executed at the end of the fixup phase.\n\nseparateDebugInfo\n\nIf set to true, the standard environment will enable debug information in C/C++ builds. After installation, the debug information will be separated from the executables and stored in the output named debug. (This output is enabled automatically; you don’t need to set the outputs attribute explicitly.) To be precise, the debug information is stored in debug/lib/debug/.build-id/XX/YYYY…, where <XXYYYY…> is the <build ID> of the binary — a SHA-1 hash of the contents of the binary. Debuggers like GDB use the build ID to look up the separated debug information.\n\nExample 220. Enable debug symbols for use with GDB\n\nTo make GDB find debug information for the socat package and its dependencies, you can use the following shell.nix:\n\nlet\n  pkgs = import ./. {\n    config = {};\n    overlays = [\n      (final: prev: {\n        ncurses = prev.ncurses.overrideAttrs { separateDebugInfo = true; };\n        readline = prev.readline.overrideAttrs { separateDebugInfo = true; };\n      })\n    ];\n  };\n\n  myDebugInfoDirs = pkgs.symlinkJoin {\n    name = \"myDebugInfoDirs\";\n    paths = with pkgs; [\n      glibc.debug\n      ncurses.debug\n      openssl.debug\n      readline.debug\n    ];\n  };\nin\n  pkgs.mkShell {\n\n    NIX_DEBUG_INFO_DIRS = \"${pkgs.lib.getLib myDebugInfoDirs}/lib/debug\";\n\n    packages = [\n      pkgs.gdb\n      pkgs.socat\n    ];\n\n    shellHook = ''\n      ${pkgs.lib.getBin pkgs.gdb}/bin/gdb ${pkgs.lib.getBin pkgs.socat}/bin/socat\n    '';\n  }\n\n\nThis setup works as follows:\n\nAdd overlays to the package set, since debug symbols are disabled for ncurses and readline by default.\n\nCreate a derivation to combine all required debug symbols under one path with symlinkJoin.\n\nSet the environment variable NIX_DEBUG_INFO_DIRS in the shell. Nixpkgs patches gdb to use it for looking up debug symbols.\n\nRun gdb on the socat binary on shell startup in the shellHook. Here we use lib.getBin to ensure that the correct derivation output is selected rather than the default one.\n\n\n\nThe installCheck phase \n\nThe installCheck phase checks whether the package was installed correctly by running its test suite against the installed directories. The default installCheck calls make installcheck.\n\nIt is often better to add tests that are not part of the source distribution to passthru.tests (see the section called “tests”). This avoids adding overhead to every build and enables us to run them independently.\n\nVariables controlling the installCheck phase\ndoInstallCheck\n\nControls whether the installCheck phase is executed. By default it is skipped, but if doInstallCheck is set to true, the installCheck phase is usually executed. Thus you should set\n\ndoInstallCheck = true;\n\n\nin the derivation to enable install checks. The exception is cross compilation. Cross compiled builds never run tests, no matter how doInstallCheck is set, as the newly-built program won’t run on the platform used to build it.\n\ninstallCheckTarget\n\nThe make target that runs the install tests. Defaults to installcheck.\n\ninstallCheckFlags / installCheckFlagsArray\n\nA list of strings passed as additional flags to make. Like makeFlags and makeFlagsArray, but only used by the installCheck phase.\n\ninstallCheckInputs\n\nA list of host dependencies used by the phase, usually libraries linked into executables built during tests. This gets included in buildInputs when doInstallCheck is set.\n\nnativeInstallCheckInputs\n\nA list of native dependencies used by the phase, notably tools needed on $PATH. This gets included in nativeBuildInputs when doInstallCheck is set.\n\npreInstallCheck\n\nHook executed at the start of the installCheck phase.\n\npostInstallCheck\n\nHook executed at the end of the installCheck phase.\n\nThe distribution phase \n\nThe distribution phase is intended to produce a source distribution of the package. The default distPhase first calls make dist, then it copies the resulting source tarballs to $out/tarballs/. This phase is only executed if the attribute doDist is set.\n\nVariables controlling the distribution phase\ndoDist\n\nIf set, the distribution phase is executed.\n\ndistTarget\n\nThe make target that produces the distribution. Defaults to dist.\n\ndistFlags / distFlagsArray\n\nAdditional flags passed to make.\n\ntarballs\n\nThe names of the source distribution files to be copied to $out/tarballs/. It can contain shell wildcards. The default is *.tar.gz.\n\ndontCopyDist\n\nIf set, no files are copied to $out/tarballs/.\n\npreDist\n\nHook executed at the start of the distribution phase.\n\npostDist\n\nHook executed at the end of the distribution phase.\n\nShell functions and utilities \nmakeWrapper <executable> <wrapperfile> <args>\nremove-references-to -t <storepath> [ -t <storepath> … ] <file> …\nsubstitute <infile> <outfile> <subs>\nsubstituteInPlace <multiple files> <subs>\nsubstituteAll <infile> <outfile>\nsubstituteAllInPlace <file>\nstripHash <path>\nwrapProgram <executable> <makeWrapperArgs>\nprependToVar <variableName> <elements…>\nappendToVar <variableName> <elements…>\n\nThe standard environment provides a number of useful functions.\n\nmakeWrapper <executable> <wrapperfile> <args> \n\nConstructs a wrapper for a program with various possible arguments. It is defined as part of 2 setup-hooks named makeWrapper and makeBinaryWrapper that implement the same bash functions. Hence, to use it you have to add makeWrapper to your nativeBuildInputs. Here’s an example usage:\n\n# adds `FOOBAR=baz` to `$out/bin/foo`’s environment\nmakeWrapper $out/bin/foo $wrapperfile --set FOOBAR baz\n\n# Prefixes the binary paths of `hello` and `git`\n# and suffixes the binary path of `xdg-utils`.\n# Be advised that paths often should be patched in directly\n# (via string replacements or in `configurePhase`).\nmakeWrapper $out/bin/foo $wrapperfile \\\n  --prefix PATH : ${lib.makeBinPath [ hello git ]} \\\n  --suffix PATH : ${lib.makeBinPath [ xdg-utils ]}\n\n\nPackages may expect or require other utilities to be available at runtime. makeWrapper can be used to add packages to a PATH environment variable local to a wrapper.\n\nUse --prefix to explicitly set dependencies in PATH.\n\nNote\n\n--prefix essentially hard-codes dependencies into the wrapper. They cannot be overridden without rebuilding the package.\n\nIf dependencies should be resolved at runtime, use --suffix to append fallback values to PATH.\n\nThere’s many more kinds of arguments, they are documented in nixpkgs/pkgs/build-support/setup-hooks/make-wrapper.sh for the makeWrapper implementation and in nixpkgs/pkgs/build-support/setup-hooks/make-binary-wrapper/make-binary-wrapper.sh for the makeBinaryWrapper implementation.\n\nwrapProgram is a convenience function you probably want to use most of the time, implemented by both makeWrapper and makeBinaryWrapper.\n\nUsing the makeBinaryWrapper implementation is usually preferred, as it creates a tiny compiled wrapper executable, that can be used as a shebang interpreter. This is needed mostly on Darwin, where shebangs cannot point to scripts, due to a limitation with the execve-syscall. Compiled wrappers generated by makeBinaryWrapper can be inspected with less <path-to-wrapper> - by scrolling past the binary data you should be able to see the shell command that generated the executable and there see the environment variables that were injected into the wrapper.\n\nremove-references-to -t <storepath> [ -t <storepath> … ] <file> … \n\nRemoves the references of the specified files to the specified store files. This is done without changing the size of the file by replacing the hash by eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee, and should work on compiled executables. This is meant to be used to remove the dependency of the output on inputs that are known to be unnecessary at runtime. Of course, reckless usage will break the patched programs. To use this, add removeReferencesTo to nativeBuildInputs.\n\nAs remove-references-to is an actual executable and not a shell function, it can be used with find. Example removing all references to the compiler in the output:\n\npostInstall = ''\n  find \"$out\" -type f -exec remove-references-to -t ${stdenv.cc} '{}' +\n'';\n\nsubstitute <infile> <outfile> <subs> \n\nPerforms string substitution on the contents of <infile>, writing the result to <outfile>. The substitutions in <subs> are of the following form:\n\n--replace <s1> <s2>\n\nReplace every occurrence of the string <s1> by <s2>.\n\n--subst-var <varName>\n\nReplace every occurrence of @varName@ by the contents of the environment variable <varName>. This is useful for generating files from templates, using @...@ in the template as placeholders.\n\n--subst-var-by <varName> <s>\n\nReplace every occurrence of @varName@ by the string <s>.\n\nExample:\n\nsubstitute ./foo.in ./foo.out \\\n    --replace /usr/bin/bar $bar/bin/bar \\\n    --replace \"a string containing spaces\" \"some other text\" \\\n    --subst-var someVar\n\nsubstituteInPlace <multiple files> <subs> \n\nLike substitute, but performs the substitutions in place on the files passed.\n\nsubstituteAll <infile> <outfile> \n\nReplaces every occurrence of @varName@, where <varName> is any environment variable, in <infile>, writing the result to <outfile>. For instance, if <infile> has the contents\n\n#! @bash@/bin/sh\nPATH=@coreutils@/bin\necho @foo@\n\n\nand the environment contains bash=/nix/store/bmwp0q28cf21...-bash-3.2-p39 and coreutils=/nix/store/68afga4khv0w...-coreutils-6.12, but does not contain the variable foo, then the output will be\n\n#! /nix/store/bmwp0q28cf21...-bash-3.2-p39/bin/sh\nPATH=/nix/store/68afga4khv0w...-coreutils-6.12/bin\necho @foo@\n\n\nThat is, no substitution is performed for undefined variables.\n\nEnvironment variables that start with an uppercase letter or an underscore are filtered out, to prevent global variables (like HOME) or private variables (like __ETC_PROFILE_DONE) from accidentally getting substituted. The variables also have to be valid bash “names”, as defined in the bash manpage (alphanumeric or _, must not start with a number).\n\nsubstituteAllInPlace <file> \n\nLike substituteAll, but performs the substitutions in place on the file <file>.\n\nstripHash <path> \n\nStrips the directory and hash part of a store path, outputting the name part to stdout. For example:\n\n# prints coreutils-8.24\nstripHash \"/nix/store/9s9r019176g7cvn2nvcw41gsp862y6b4-coreutils-8.24\"\n\n\nIf you wish to store the result in another variable, then the following idiom may be useful:\n\nname=\"/nix/store/9s9r019176g7cvn2nvcw41gsp862y6b4-coreutils-8.24\"\nsomeVar=$(stripHash $name)\n\nwrapProgram <executable> <makeWrapperArgs> \n\nConvenience function for makeWrapper that replaces <executable> with a wrapper that executes the original program. It takes all the same arguments as makeWrapper, except for --inherit-argv0 (used by the makeBinaryWrapper implementation) and --argv0 (used by both makeWrapper and makeBinaryWrapper wrapper implementations).\n\nIf you will apply it multiple times, it will overwrite the wrapper file and you will end up with double wrapping, which should be avoided.\n\nprependToVar <variableName> <elements…> \n\nPrepend elements to a variable.\n\nExample:\n\n$ configureFlags=\"--disable-static\"\n$ prependToVar configureFlags --disable-dependency-tracking --enable-foo\n$ echo $configureFlags\n--disable-dependency-tracking --enable-foo --disable-static\n\nappendToVar <variableName> <elements…> \n\nAppend elements to a variable.\n\nExample:\n\n$ configureFlags=\"--disable-static\"\n$ appendToVar configureFlags --disable-dependency-tracking --enable-foo\n$ echo $configureFlags\n--disable-static --disable-dependency-tracking --enable-foo\n\nPackage setup hooks \nmove-docs.sh\ncompress-man-pages.sh\nstrip.sh\npatch-shebangs.sh\naudit-tmpdir.sh\nmultiple-outputs.sh\nmove-sbin.sh\nmove-lib64.sh\nmove-systemd-user-units.sh\nset-source-date-epoch-to-latest.sh\nBintools Wrapper and hook\nCC Wrapper and hook\nOther hooks\nCompiler and Linker wrapper hooks\n\nNix itself considers a build-time dependency as merely something that should previously be built and accessible at build time—packages themselves are on their own to perform any additional setup. In most cases, that is fine, and the downstream derivation can deal with its own dependencies. But for a few common tasks, that would result in almost every package doing the same sort of setup work—depending not on the package itself, but entirely on which dependencies were used.\n\nIn order to alleviate this burden, the setup hook mechanism was written, where any package can include a shell script that [by convention rather than enforcement by Nix], any downstream reverse-dependency will source as part of its build process. That allows the downstream dependency to merely specify its dependencies, and lets those dependencies effectively initialize themselves. No boilerplate mirroring the list of dependencies is needed.\n\nThe setup hook mechanism is a bit of a sledgehammer though: a powerful feature with a broad and indiscriminate area of effect. The combination of its power and implicit use may be expedient, but isn’t without costs. Nix itself is unchanged, but the spirit of added dependencies being effect-free is violated even if the latter isn’t. For example, if a derivation path is mentioned more than once, Nix itself doesn’t care and makes sure the dependency derivation is already built just the same—depending is just needing something to exist, and needing is idempotent. However, a dependency specified twice will have its setup hook run twice, and that could easily change the build environment (though a well-written setup hook will therefore strive to be idempotent so this is in fact not observable). More broadly, setup hooks are anti-modular in that multiple dependencies, whether the same or different, should not interfere and yet their setup hooks may well do so.\n\nThe most typical use of the setup hook is actually to add other hooks which are then run (i.e. after all the setup hooks) on each dependency. For example, the C compiler wrapper’s setup hook feeds itself flags for each dependency that contains relevant libraries and headers. This is done by defining a bash function, and appending its name to one of envBuildBuildHooks, envBuildHostHooks, envBuildTargetHooks, envHostHostHooks, envHostTargetHooks, or envTargetTargetHooks. These 6 bash variables correspond to the 6 sorts of dependencies by platform (there’s 12 total but we ignore the propagated/non-propagated axis).\n\nPackages adding a hook should not hard code a specific hook, but rather choose a variable relative to how they are included. Returning to the C compiler wrapper example, if the wrapper itself is an n dependency, then it only wants to accumulate flags from n + 1 dependencies, as only those ones match the compiler’s target platform. The hostOffset variable is defined with the current dependency’s host offset targetOffset with its target offset, before its setup hook is sourced. Additionally, since most environment hooks don’t care about the target platform, that means the setup hook can append to the right bash array by doing something like\n\naddEnvHooks \"$hostOffset\" myBashFunction\n\n\nThe existence of setups hooks has long been documented and packages inside Nixpkgs are free to use this mechanism. Other packages, however, should not rely on these mechanisms not changing between Nixpkgs versions. Because of the existing issues with this system, there’s little benefit from mandating it be stable for any period of time.\n\nFirst, let’s cover some setup hooks that are part of Nixpkgs default stdenv. This means that they are run for every package built using stdenv.mkDerivation or when using a custom builder that has source $stdenv/setup. Some of these are platform specific, so they may run on Linux but not Darwin or vice-versa.\n\nmove-docs.sh \n\nThis setup hook moves any installed documentation to the /share subdirectory directory. This includes the man, doc and info directories. This is needed for legacy programs that do not know how to use the share subdirectory.\n\ncompress-man-pages.sh \n\nThis setup hook compresses any man pages that have been installed. The compression is done using the gzip program. This helps to reduce the installed size of packages.\n\nstrip.sh \n\nThis runs the strip command on installed binaries and libraries. This removes unnecessary information like debug symbols when they are not needed. This also helps to reduce the installed size of packages.\n\npatch-shebangs.sh \n\nThis setup hook patches installed scripts to add Nix store paths to their shebang interpreter as found in the build environment. The shebang line tells a Unix-like operating system which interpreter to use to execute the script’s contents.\n\nNote\n\nThe generic builder populates PATH from inputs of the derivation.\n\nInvocation\n\nMultiple paths can be specified.\n\npatchShebangs [--build | --host] PATH...\n\nFlags\n--build\n\nLook up commands available at build time\n\n--host\n\nLook up commands available at run time\n\nExamples\npatchShebangs --host /nix/store/<hash>-hello-1.0/bin\n\npatchShebangs --build configure\n\n\n#!/bin/sh will be rewritten to #!/nix/store/<hash>-some-bash/bin/sh.\n\n#!/usr/bin/env gets special treatment: #!/usr/bin/env python is rewritten to /nix/store/<hash>/bin/python.\n\nInterpreter paths that point to a valid Nix store location are not changed.\n\nNote\n\nA script file must be marked as executable, otherwise it will not be considered.\n\nThis mechanism ensures that the interpreter for a given script is always found and is exactly the one specified by the build.\n\nIt can be disabled by setting dontPatchShebangs:\n\nstdenv.mkDerivation {\n  # ...\n  dontPatchShebangs = true;\n  # ...\n}\n\n\nThe file patch-shebangs.sh defines the patchShebangs function. It is used to implement patchShebangsAuto, the setup hook that is registered to run during the fixup phase by default.\n\nIf you need to run patchShebangs at build time, it must be called explicitly within one of the build phases.\n\naudit-tmpdir.sh \n\nThis verifies that no references are left from the install binaries to the directory used to build those binaries. This ensures that the binaries do not need things outside the Nix store. This is currently supported in Linux only.\n\nmultiple-outputs.sh \n\nThis setup hook adds configure flags that tell packages to install files into any one of the proper outputs listed in outputs. This behavior can be turned off by setting setOutputFlags to false in the derivation environment. See Multiple-output packages for more information.\n\nmove-sbin.sh \n\nThis setup hook moves any binaries installed in the sbin/ subdirectory into bin/. In addition, a link is provided from sbin/ to bin/ for compatibility.\n\nmove-lib64.sh \n\nThis setup hook moves any libraries installed in the lib64/ subdirectory into lib/. In addition, a link is provided from lib64/ to lib/ for compatibility.\n\nmove-systemd-user-units.sh \n\nThis setup hook moves any systemd user units installed in the lib/ subdirectory into share/. In addition, a link is provided from share/ to lib/ for compatibility. This is needed for systemd to find user services when installed into the user profile.\n\nThis hook only runs when compiling for Linux.\n\nset-source-date-epoch-to-latest.sh \n\nThis sets SOURCE_DATE_EPOCH to the modification time of the most recent file.\n\nBintools Wrapper and hook \n\nThe Bintools Wrapper wraps the binary utilities for a bunch of miscellaneous purposes. These are GNU Binutils when targeting Linux, and a mix of cctools and GNU binutils for Darwin. [The “Bintools” name is supposed to be a compromise between “Binutils” and “cctools” not denoting any specific implementation.] Specifically, the underlying bintools package, and a C standard library (glibc or Darwin’s libSystem, just for the dynamic loader) are all fed in, and dependency finding, hardening (see below), and purity checks for each are handled by the Bintools Wrapper. Packages typically depend on CC Wrapper, which in turn (at run time) depends on the Bintools Wrapper.\n\nThe Bintools Wrapper was only just recently split off from CC Wrapper, so the division of labor is still being worked out. For example, it shouldn’t care about the C standard library, but just take a derivation with the dynamic loader (which happens to be the glibc on linux). Dependency finding however is a task both wrappers will continue to need to share, and probably the most important to understand. It is currently accomplished by collecting directories of host-platform dependencies (i.e. buildInputs and nativeBuildInputs) in environment variables. The Bintools Wrapper’s setup hook causes any lib and lib64 subdirectories to be added to NIX_LDFLAGS. Since the CC Wrapper and the Bintools Wrapper use the same strategy, most of the Bintools Wrapper code is sparsely commented and refers to the CC Wrapper. But the CC Wrapper’s code, by contrast, has quite lengthy comments. The Bintools Wrapper merely cites those, rather than repeating them, to avoid falling out of sync.\n\nA final task of the setup hook is defining a number of standard environment variables to tell build systems which executables fulfill which purpose. They are defined to just be the base name of the tools, under the assumption that the Bintools Wrapper’s binaries will be on the path. Firstly, this helps poorly-written packages, e.g. ones that look for just gcc when CC isn’t defined yet clang is to be used. Secondly, this helps packages not get confused when cross-compiling, in which case multiple Bintools Wrappers may simultaneously be in use. [7] BUILD_- and TARGET_-prefixed versions of the normal environment variable are defined for additional Bintools Wrappers, properly disambiguating them.\n\nA problem with this final task is that the Bintools Wrapper is honest and defines LD as ld. Most packages, however, firstly use the C compiler for linking, secondly use LD anyways, defining it as the C compiler, and thirdly, only so define LD when it is undefined as a fallback. This triple-threat means Bintools Wrapper will break those packages, as LD is already defined as the actual linker which the package won’t override yet doesn’t want to use. The workaround is to define, just for the problematic package, LD as the C compiler. A good way to do this would be preConfigure = \"LD=$CC\".\n\nCC Wrapper and hook \n\nThe CC Wrapper wraps a C toolchain for a bunch of miscellaneous purposes. Specifically, a C compiler (GCC or Clang), wrapped binary tools, and a C standard library (glibc or Darwin’s libSystem, just for the dynamic loader) are all fed in, and dependency finding, hardening (see below), and purity checks for each are handled by the CC Wrapper. Packages typically depend on the CC Wrapper, which in turn (at run-time) depends on the Bintools Wrapper.\n\nDependency finding is undoubtedly the main task of the CC Wrapper. This works just like the Bintools Wrapper, except that any include subdirectory of any relevant dependency is added to NIX_CFLAGS_COMPILE. The setup hook itself contains elaborate comments describing the exact mechanism by which this is accomplished.\n\nSimilarly, the CC Wrapper follows the Bintools Wrapper in defining standard environment variables with the names of the tools it wraps, for the same reasons described above. Importantly, while it includes a cc symlink to the c compiler for portability, the CC will be defined using the compiler’s “real name” (i.e. gcc or clang). This helps lousy build systems that inspect on the name of the compiler rather than run it.\n\nHere are some more packages that provide a setup hook. Since the list of hooks is extensible, this is not an exhaustive list. The mechanism is only to be used as a last resort, so it might cover most uses.\n\nOther hooks \n\nMany other packages provide hooks, that are not part of stdenv. You can find these in the Hooks Reference.\n\nCompiler and Linker wrapper hooks \n\nIf the file ${cc}/nix-support/cc-wrapper-hook exists, it will be run at the end of the compiler wrapper. If the file ${binutils}/nix-support/post-link-hook exists, it will be run at the end of the linker wrapper. These hooks allow a user to inject code into the wrappers. As an example, these hooks can be used to extract extraBefore, params and extraAfter which store all the command line arguments passed to the compiler and linker respectively.\n\nPurity in Nixpkgs \n\nMeasures taken to prevent dependencies on packages outside the store, and what you can do to prevent them.\n\nGCC doesn’t search in locations such as /usr/include. In fact, attempts to add such directories through the -I flag are filtered out. Likewise, the linker (from GNU binutils) doesn’t search in standard locations such as /usr/lib. Programs built on Linux are linked against a GNU C Library that likewise doesn’t search in the default system locations.\n\nHardening in Nixpkgs \nHardening flags enabled by default\nHardening flags disabled by default\n\nThere are flags available to harden packages at compile or link-time. These can be toggled using the stdenv.mkDerivation parameters hardeningDisable and hardeningEnable.\n\nBoth parameters take a list of flags as strings. The special \"all\" flag can be passed to hardeningDisable to turn off all hardening. These flags can also be used as environment variables for testing or development purposes.\n\nFor more in-depth information on these hardening flags and hardening in general, refer to the Debian Wiki, Ubuntu Wiki, Gentoo Wiki, and the Arch Wiki.\n\nHardening flags enabled by default \n\nThe following flags are enabled by default and might require disabling with hardeningDisable if the program to package is incompatible.\n\nformat\n\nAdds the -Wformat -Wformat-security -Werror=format-security compiler options. At present, this warns about calls to printf and scanf functions where the format string is not a string literal and there are no format arguments, as in printf(foo);. This may be a security hole if the format string came from untrusted input and contains %n.\n\nThis needs to be turned off or fixed for errors similar to:\n\n/tmp/nix-build-zynaddsubfx-2.5.2.drv-0/zynaddsubfx-2.5.2/src/UI/guimain.cpp:571:28: error: format not a string literal and no format arguments [-Werror=format-security]\n         printf(help_message);\n                            ^\ncc1plus: some warnings being treated as errors\n\nstackprotector\n\nAdds the -fstack-protector-strong --param ssp-buffer-size=4 compiler options. This adds safety checks against stack overwrites rendering many potential code injection attacks into aborting situations. In the best case this turns code injection vulnerabilities into denial of service or into non-issues (depending on the application).\n\nThis needs to be turned off or fixed for errors similar to:\n\nbin/blib.a(bios_console.o): In function `bios_handle_cup':\n/tmp/nix-build-ipxe-20141124-5cbdc41.drv-0/ipxe-5cbdc41/src/arch/i386/firmware/pcbios/bios_console.c:86: undefined reference to `__stack_chk_fail'\n\nfortify\n\nAdds the -O2 -D_FORTIFY_SOURCE=2 compiler options. During code generation the compiler knows a great deal of information about buffer sizes (where possible), and attempts to replace insecure unlimited length buffer function calls with length-limited ones. This is especially useful for old, crufty code. Additionally, format strings in writable memory that contain %n are blocked. If an application depends on such a format string, it will need to be worked around.\n\nAdditionally, some warnings are enabled which might trigger build failures if compiler warnings are treated as errors in the package build. In this case, set env.NIX_CFLAGS_COMPILE to -Wno-error=warning-type.\n\nThis needs to be turned off or fixed for errors similar to:\n\nmalloc.c:404:15: error: return type is an incomplete type\nmalloc.c:410:19: error: storage size of 'ms' isn't known\n\nstrdup.h:22:1: error: expected identifier or '(' before '__extension__'\n\nstrsep.c:65:23: error: register name not specified for 'delim'\n\ninstallwatch.c:3751:5: error: conflicting types for '__open_2'\n\nfcntl2.h:50:4: error: call to '__open_missing_mode' declared with attribute error: open with O_CREAT or O_TMPFILE in second argument needs 3 arguments\n\npic\n\nAdds the -fPIC compiler options. This options adds support for position independent code in shared libraries and thus making ASLR possible.\n\nMost notably, the Linux kernel, kernel modules and other code not running in an operating system environment like boot loaders won’t build with PIC enabled. The compiler will is most cases complain that PIC is not supported for a specific build.\n\nThis needs to be turned off or fixed for assembler errors similar to:\n\nccbLfRgg.s: Assembler messages:\nccbLfRgg.s:33: Error: missing or invalid displacement expression `private_key_len@GOTOFF'\n\nstrictoverflow\n\nSigned integer overflow is undefined behaviour according to the C standard. If it happens, it is an error in the program as it should check for overflow before it can happen, not afterwards. GCC provides built-in functions to perform arithmetic with overflow checking, which are correct and faster than any custom implementation. As a workaround, the option -fno-strict-overflow makes gcc behave as if signed integer overflows were defined.\n\nThis flag should not trigger any build or runtime errors.\n\nrelro\n\nAdds the -z relro linker option. During program load, several ELF memory sections need to be written to by the linker, but can be turned read-only before turning over control to the program. This prevents some GOT (and .dtors) overwrite attacks, but at least the part of the GOT used by the dynamic linker (.got.plt) is still vulnerable.\n\nThis flag can break dynamic shared object loading. For instance, the module systems of Xorg and OpenCV are incompatible with this flag. In almost all cases the bindnow flag must also be disabled and incompatible programs typically fail with similar errors at runtime.\n\nbindnow\n\nAdds the -z now linker option. During program load, all dynamic symbols are resolved, allowing for the complete GOT to be marked read-only (due to relro). This prevents GOT overwrite attacks. For very large applications, this can incur some performance loss during initial load while symbols are resolved, but this shouldn’t be an issue for daemons.\n\nThis flag can break dynamic shared object loading. For instance, the module systems of Xorg and PHP are incompatible with this flag. Programs incompatible with this flag often fail at runtime due to missing symbols, like:\n\nintel_drv.so: undefined symbol: vgaHWFreeHWRec\n\nHardening flags disabled by default \n\nThe following flags are disabled by default and should be enabled with hardeningEnable for packages that take untrusted input like network services.\n\npie\n\nThis flag is disabled by default for normal glibc based NixOS package builds, but enabled by default for musl based package builds.\n\nAdds the -fPIE compiler and -pie linker options. Position Independent Executables are needed to take advantage of Address Space Layout Randomization, supported by modern kernel versions. While ASLR can already be enforced for data areas in the stack and heap (brk and mmap), the code areas must be compiled as position-independent. Shared libraries already do this with the pic flag, so they gain ASLR automatically, but binary .text regions need to be build with pie to gain ASLR. When this happens, ROP attacks are much harder since there are no static locations to bounce off of during a memory corruption attack.\n\nStatic libraries need to be compiled with -fPIE so that executables can link them in with the -pie linker option. If the libraries lack -fPIE, you will get the error recompile with -fPIE.\n\nThe build platform is ignored because it is a mere implementation detail of the package satisfying the dependency: As a general programming principle, dependencies are always specified as interfaces, not concrete implementation.[1]\n\nCurrently, this means for native builds all dependencies are put on the PATH. But in the future that may not be the case for sake of matching cross: the platforms would be assumed to be unique for native and cross builds alike, so only the depsBuild* and nativeBuildInputs would be added to the PATH.[2]\n\nNix itself already takes a package’s transitive dependencies into account, but this propagation ensures nixpkgs-specific infrastructure like setup hooks also are run as if it were a propagated dependency.[3]\n\nThe findInputs function, currently residing in pkgs/stdenv/generic/setup.sh, implements the propagation logic.[4]\n\nIt clears the sys_lib_*search_path variables in the Libtool script to prevent Libtool from using libraries in /usr/lib and such.[5]\n\nEventually these will be passed building natively as well, to improve determinism: build-time guessing, as is done today, is a risk of impurity.[6]\n\nEach wrapper targets a single platform, so if binaries for multiple platforms are needed, the underlying binaries must be wrapped multiple times. As this is a property of the wrapper itself, the multiple wrappings are needed whether or not the same underlying binaries can target multiple platforms.[7]\n\nMeta-attributes \n\nTable of Contents\n\nStandard meta-attributes\nLicenses\nSource provenance\n\nNix packages can declare meta-attributes that contain information about a package such as a description, its homepage, its license, and so on. For instance, the GNU Hello package has a meta declaration like this:\n\nmeta = with lib; {\n  description = \"A program that produces a familiar, friendly greeting\";\n  longDescription = ''\n    GNU Hello is a program that prints \"Hello, world!\" when you run it.\n    It is fully customizable.\n  '';\n  homepage = \"https://www.gnu.org/software/hello/manual/\";\n  license = licenses.gpl3Plus;\n  maintainers = with maintainers; [ eelco ];\n  platforms = platforms.all;\n};\n\n\nMeta-attributes are not passed to the builder of the package. Thus, a change to a meta-attribute doesn’t trigger a recompilation of the package.\n\nStandard meta-attributes \ndescription\nlongDescription\nbranch\nhomepage\ndownloadPage\nchangelog\nlicense\nmaintainers\nmainProgram\npriority\nplatforms\nbadPlatforms\ntests\ntimeout\nhydraPlatforms\nbroken\n\nIt is expected that each meta-attribute is one of the following:\n\ndescription \n\nA short (one-line) description of the package. This is displayed on search.nixos.org.\n\nDon’t include a period at the end. Don’t include newline characters. Capitalise the first character. For brevity, don’t repeat the name of package — just describe what it does.\n\nWrong: \"libpng is a library that allows you to decode PNG images.\"\n\nRight: \"A library for decoding PNG images\"\n\nlongDescription \n\nAn arbitrarily long description of the package in CommonMark Markdown.\n\nbranch \n\nRelease branch. Used to specify that a package is not going to receive updates that are not in this branch; for example, Linux kernel 3.0 is supposed to be updated to 3.0.X, not 3.1.\n\nhomepage \n\nThe package’s homepage. Example: https://www.gnu.org/software/hello/manual/\n\ndownloadPage \n\nThe page where a link to the current version can be found. Example: https://ftp.gnu.org/gnu/hello/\n\nchangelog \n\nA link or a list of links to the location of Changelog for a package. A link may use expansion to refer to the correct changelog version. Example: \"https://git.savannah.gnu.org/cgit/hello.git/plain/NEWS?h=v${version}\"\n\nlicense \n\nThe license, or licenses, for the package. One from the attribute set defined in nixpkgs/lib/licenses.nix. At this moment using both a list of licenses and a single license is valid. If the license field is in the form of a list representation, then it means that parts of the package are licensed differently. Each license should preferably be referenced by their attribute. The non-list attribute value can also be a space delimited string representation of the contained attribute shortNames or spdxIds. The following are all valid examples:\n\nSingle license referenced by attribute (preferred) lib.licenses.gpl3Only.\n\nSingle license referenced by its attribute shortName (frowned upon) \"gpl3Only\".\n\nSingle license referenced by its attribute spdxId (frowned upon) \"GPL-3.0-only\".\n\nMultiple licenses referenced by attribute (preferred) with lib.licenses; [ asl20 free ofl ].\n\nMultiple licenses referenced as a space delimited string of attribute shortNames (frowned upon) \"asl20 free ofl\".\n\nFor details, see Licenses.\n\nmaintainers \n\nA list of the maintainers of this Nix expression. Maintainers are defined in nixpkgs/maintainers/maintainer-list.nix. There is no restriction to becoming a maintainer, just add yourself to that list in a separate commit titled “maintainers: add alice” in the same pull request, and reference maintainers with maintainers = with lib.maintainers; [ alice bob ].\n\nmainProgram \n\nThe name of the main binary for the package. This affects the binary nix run executes. Example: \"rg\"\n\npriority \n\nThe priority of the package, used by nix-env to resolve file name conflicts between packages. See the manual page for nix-env for details. Example: \"10\" (a low-priority package).\n\nplatforms \n\nThe list of Nix platform types on which the package is supported. Hydra builds packages according to the platform specified. If no platform is specified, the package does not have prebuilt binaries. An example is:\n\nmeta.platforms = lib.platforms.linux;\n\n\nAttribute Set lib.platforms defines various common lists of platforms types.\n\nbadPlatforms \n\nThe list of Nix platform types on which the package is known not to be buildable. Hydra will never create prebuilt binaries for these platform types, even if they are in meta.platforms. In general it is preferable to set meta.platforms = lib.platforms.all and then exclude any platforms on which the package is known not to build. For example, a package which requires dynamic linking and cannot be linked statically could use this:\n\nmeta.platforms = lib.platforms.all;\nmeta.badPlatforms = [ lib.systems.inspect.patterns.isStatic ];\n\n\nThe lib.meta.availableOn function can be used to test whether or not a package is available (i.e. buildable) on a given platform. Some packages use this to automatically detect the maximum set of features with which they can be built. For example, systemd requires dynamic linking, and has a meta.badPlatforms setting similar to the one above. Packages which can be built with or without systemd support will use lib.meta.availableOn to detect whether or not systemd is available on the hostPlatform for which they are being built; if it is not available (e.g. due to a statically-linked host platform like pkgsStatic) this support will be disabled by default.\n\ntests \nWarning\n\nThis attribute is special in that it is not actually under the meta attribute set but rather under the passthru attribute set. This is due to how meta attributes work, and the fact that they are supposed to contain only metadata, not derivations.\n\nAn attribute set with tests as values. A test is a derivation that builds when the test passes and fails to build otherwise.\n\nYou can run these tests with:\n\n$ cd path/to/nixpkgs\n$ nix-build -A your-package.tests\n\nPackage tests\n\nTests that are part of the source package are often executed in the installCheckPhase.\n\nPrefer passthru.tests for tests that are introduced in nixpkgs because:\n\npassthru.tests tests the ‘real’ package, independently from the environment in which it was built\n\nwe can run passthru.tests independently\n\ninstallCheckPhase adds overhead to each build\n\nFor more on how to write and run package tests, see the section called “Package tests”.\n\nNixOS tests\n\nThe NixOS tests are available as nixosTests in parameters of derivations. For instance, the OpenSMTPD derivation includes lines similar to:\n\n{ /* ... */, nixosTests }:\n{\n  # ...\n  passthru.tests = {\n    basic-functionality-and-dovecot-integration = nixosTests.opensmtpd;\n  };\n}\n\n\nNixOS tests run in a VM, so they are slower than regular package tests. For more information see NixOS module tests.\n\nAlternatively, you can specify other derivations as tests. You can make use of the optional parameter to inject the correct package without relying on non-local definitions, even in the presence of overrideAttrs. Here that’s finalAttrs.finalPackage, but you could choose a different name if finalAttrs already exists in your scope.\n\n(mypkg.overrideAttrs f).passthru.tests will be as expected, as long as the definition of tests does not rely on the original mypkg or overrides it in all places.\n\n# my-package/default.nix\n{ stdenv, callPackage }:\nstdenv.mkDerivation (finalAttrs: {\n  # ...\n  passthru.tests.example = callPackage ./example.nix { my-package = finalAttrs.finalPackage; };\n})\n\n# my-package/example.nix\n{ runCommand, lib, my-package, ... }:\nrunCommand \"my-package-test\" {\n  nativeBuildInputs = [ my-package ];\n  src = lib.sources.sourcesByRegex ./. [ \".*.in\" \".*.expected\" ];\n} ''\n  my-package --help\n  my-package <example.in >example.actual\n  diff -U3 --color=auto example.expected example.actual\n  mkdir $out\n''\n\ntimeout \n\nA timeout (in seconds) for building the derivation. If the derivation takes longer than this time to build, Hydra will fail it due to breaking the timeout. However, all computers do not have the same computing power, hence some builders may decide to apply a multiplicative factor to this value. When filling this value in, try to keep it approximately consistent with other values already present in nixpkgs.\n\nmeta attributes are not stored in the instantiated derivation. Therefore, this setting may be lost when the package is used as a dependency. To be effective, it must be presented directly to an evaluation process that handles the meta.timeout attribute.\n\nhydraPlatforms \n\nThe list of Nix platform types for which the Hydra instance at hydra.nixos.org will build the package. (Hydra is the Nix-based continuous build system.) It defaults to the value of meta.platforms. Thus, the only reason to set meta.hydraPlatforms is if you want hydra.nixos.org to build the package on a subset of meta.platforms, or not at all, e.g.\n\nmeta.platforms = lib.platforms.linux;\nmeta.hydraPlatforms = [];\n\nbroken \n\nIf set to true, the package is marked as “broken”, meaning that it won’t show up in search.nixos.org, and cannot be built or installed unless the environment variable NIXPKGS_ALLOW_BROKEN is set. Such unconditionally-broken packages should be removed from Nixpkgs eventually unless they are fixed.\n\nThe value of this attribute can depend on a package’s arguments, including stdenv. This means that broken can be used to express constraints, for example:\n\nDoes not cross compile\n\n meta.broken = !(stdenv.buildPlatform.canExecute stdenv.hostPlatform)\n\n\nBroken if all of a certain set of its dependencies are broken\n\nmeta.broken = lib.all (map (p: p.meta.broken) [ glibc musl ])\n\n\nThis makes broken strictly more powerful than meta.badPlatforms. However meta.availableOn currently examines only meta.platforms and meta.badPlatforms, so meta.broken does not influence the default values for optional dependencies.\n\nLicenses \nlib.licenses.free, \"free\"\nlib.licenses.unfreeRedistributable, \"unfree-redistributable\"\nlib.licenses.unfree, \"unfree\"\nlib.licenses.unfreeRedistributableFirmware, \"unfree-redistributable-firmware\"\n\nThe meta.license attribute should preferably contain a value from lib.licenses defined in nixpkgs/lib/licenses.nix, or in-place license description of the same format if the license is unlikely to be useful in another expression.\n\nAlthough it’s typically better to indicate the specific license, a few generic options are available:\n\nlib.licenses.free, \"free\" \n\nCatch-all for free software licenses not listed above.\n\nlib.licenses.unfreeRedistributable, \"unfree-redistributable\" \n\nUnfree package that can be redistributed in binary form. That is, it’s legal to redistribute the output of the derivation. This means that the package can be included in the Nixpkgs channel.\n\nSometimes proprietary software can only be redistributed unmodified. Make sure the builder doesn’t actually modify the original binaries; otherwise we’re breaking the license. For instance, the NVIDIA X11 drivers can be redistributed unmodified, but our builder applies patchelf to make them work. Thus, its license is \"unfree\" and it cannot be included in the Nixpkgs channel.\n\nlib.licenses.unfree, \"unfree\" \n\nUnfree package that cannot be redistributed. You can build it yourself, but you cannot redistribute the output of the derivation. Thus it cannot be included in the Nixpkgs channel.\n\nlib.licenses.unfreeRedistributableFirmware, \"unfree-redistributable-firmware\" \n\nThis package supplies unfree, redistributable firmware. This is a separate value from unfree-redistributable because not everybody cares whether firmware is free.\n\nSource provenance \nlib.sourceTypes.fromSource\nlib.sourceTypes.binaryNativeCode\nlib.sourceTypes.binaryFirmware\nlib.sourceTypes.binaryBytecode\n\nThe value of a package’s meta.sourceProvenance attribute specifies the provenance of the package’s derivation outputs.\n\nIf a package contains elements that are not built from the original source by a nixpkgs derivation, the meta.sourceProvenance attribute should be a list containing one or more value from lib.sourceTypes defined in nixpkgs/lib/source-types.nix.\n\nAdding this information helps users who have needs related to build transparency and supply-chain security to gain some visibility into their installed software or set policy to allow or disallow installation based on source provenance.\n\nThe presence of a particular sourceType in a package’s meta.sourceProvenance list indicates that the package contains some components falling into that category, though the absence of that sourceType does not guarantee the absence of that category of sourceType in the package’s contents. A package with no meta.sourceProvenance set implies it has no known sourceTypes other than fromSource.\n\nThe meaning of the meta.sourceProvenance attribute does not depend on the value of the meta.license attribute.\n\nlib.sourceTypes.fromSource \n\nPackage elements which are produced by a nixpkgs derivation which builds them from source code.\n\nlib.sourceTypes.binaryNativeCode \n\nNative code to be executed on the target system’s CPU, built by a third party. This includes packages which wrap a downloaded AppImage or Debian package.\n\nlib.sourceTypes.binaryFirmware \n\nCode to be executed on a peripheral device or embedded controller, built by a third party.\n\nlib.sourceTypes.binaryBytecode \n\nCode to run on a VM interpreter or JIT compiled into bytecode by a third party. This includes packages which download Java .jar files from another source.\n\nMultiple-output packages \n\nTable of Contents\n\nUsing a split package\nWriting a split derivation\n\nThe Nix language allows a derivation to produce multiple outputs, which is similar to what is utilized by other Linux distribution packaging systems. The outputs reside in separate Nix store paths, so they can be mostly handled independently of each other, including passing to build inputs, garbage collection or binary substitution. The exception is that building from source always produces all the outputs.\n\nThe main motivation is to save disk space by reducing runtime closure sizes; consequently also sizes of substituted binaries get reduced. Splitting can be used to have more granular runtime dependencies, for example the typical reduction is to split away development-only files, as those are typically not needed during runtime. As a result, closure sizes of many packages can get reduced to a half or even much less.\n\nNote\n\nThe reduction effects could be instead achieved by building the parts in completely separate derivations. That would often additionally reduce build-time closures, but it tends to be much harder to write such derivations, as build systems typically assume all parts are being built at once. This compromise approach of single source package producing multiple binary packages is also utilized often by rpm and deb.\n\nA number of attributes can be used to work with a derivation with multiple outputs. The attribute outputs is a list of strings, which are the names of the outputs. For each of these names, an identically named attribute is created, corresponding to that output.\n\nThe attribute meta.outputsToInstall is used to determine the default set of outputs to install when using the derivation name unqualified: bin, or out, or the first specified output; as well as man if that is specified.\n\nUsing a split package \n\nIn the Nix language the individual outputs can be reached explicitly as attributes, e.g. coreutils.info, but the typical case is just using packages as build inputs.\n\nWhen a multiple-output derivation gets into a build input of another derivation, the dev output is added if it exists, otherwise the first output is added. In addition to that, propagatedBuildOutputs of that package which by default contain $outputBin and $outputLib are also added. (See the section called “File type groups”.)\n\nIn some cases it may be desirable to combine different outputs under a single store path. A function symlinkJoin can be used to do this. (Note that it may negate some closure size benefits of using a multiple-output package.)\n\nWriting a split derivation \n“Binaries first”\nFile type groups\nCommon caveats\n\nHere you find how to write a derivation that produces multiple outputs.\n\nIn nixpkgs there is a framework supporting multiple-output derivations. It tries to cover most cases by default behavior. You can find the source separated in <nixpkgs/pkgs/build-support/setup-hooks/multiple-outputs.sh>; it’s relatively well-readable. The whole machinery is triggered by defining the outputs attribute to contain the list of desired output names (strings).\n\noutputs = [ \"bin\" \"dev\" \"out\" \"doc\" ];\n\n\nOften such a single line is enough. For each output an equally named environment variable is passed to the builder and contains the path in nix store for that output. Typically you also want to have the main out output, as it catches any files that didn’t get elsewhere.\n\nNote\n\nThere is a special handling of the debug output, described at the section called “separateDebugInfo”.\n\n“Binaries first” \n\nA commonly adopted convention in nixpkgs is that executables provided by the package are contained within its first output. This convention allows the dependent packages to reference the executables provided by packages in a uniform manner. For instance, provided with the knowledge that the perl package contains a perl executable it can be referenced as ${pkgs.perl}/bin/perl within a Nix derivation that needs to execute a Perl script.\n\nThe glibc package is a deliberate single exception to the “binaries first” convention. The glibc has libs as its first output allowing the libraries provided by glibc to be referenced directly (e.g. ${glibc}/lib/ld-linux-x86-64.so.2). The executables provided by glibc can be accessed via its bin attribute (e.g. ${lib.getBin stdenv.cc.libc}/bin/ldd).\n\nThe reason for why glibc deviates from the convention is because referencing a library provided by glibc is a very common operation among Nix packages. For instance, third-party executables packaged by Nix are typically patched and relinked with the relevant version of glibc libraries from Nix packages (please see the documentation on patchelf for more details).\n\nFile type groups \n\nThe support code currently recognizes some particular kinds of outputs and either instructs the build system of the package to put files into their desired outputs or it moves the files during the fixup phase. Each group of file types has an outputFoo variable specifying the output name where they should go. If that variable isn’t defined by the derivation writer, it is guessed – a default output name is defined, falling back to other possibilities if the output isn’t defined.\n\n$outputDev\n\nis for development-only files. These include C(++) headers (include/), pkg-config (lib/pkgconfig/), cmake (lib/cmake/) and aclocal files (share/aclocal/). They go to dev or out by default.\n\n$outputBin\n\nis meant for user-facing binaries, typically residing in bin/. They go to bin or out by default.\n\n$outputLib\n\nis meant for libraries, typically residing in lib/ and libexec/. They go to lib or out by default.\n\n$outputDoc\n\nis for user documentation, typically residing in share/doc/. It goes to doc or out by default.\n\n$outputDevdoc\n\nis for developer documentation. Currently we count gtk-doc and devhelp books, typically residing in share/gtk-doc/ and share/devhelp/, in there. It goes to devdoc or is removed (!) by default. This is because e.g. gtk-doc tends to be rather large and completely unused by nixpkgs users.\n\n$outputMan\n\nis for man pages (except for section 3), typically residing in share/man/man[0-9]/. They go to man or $outputBin by default.\n\n$outputDevman\n\nis for section 3 man pages, typically residing in share/man/man[0-9]/. They go to devman or $outputMan by default.\n\n$outputInfo\n\nis for info pages, typically residing in share/info/. They go to info or $outputBin by default.\n\nCommon caveats \n\nSome configure scripts don’t like some of the parameters passed by default by the framework, e.g. --docdir=/foo/bar. You can disable this by setting setOutputFlags = false;.\n\nThe outputs of a single derivation can retain references to each other, but note that circular references are not allowed. (And each strongly-connected component would act as a single output anyway.)\n\nMost of split packages contain their core functionality in libraries. These libraries tend to refer to various kind of data that typically gets into out, e.g. locale strings, so there is often no advantage in separating the libraries into lib, as keeping them in out is easier.\n\nSome packages have hidden assumptions on install paths, which complicates splitting.\n\nCross-compilation \n\nTable of Contents\n\nIntroduction\nPackaging in a cross-friendly manner\nCross-building packages\nCross-compilation infrastructure\nIntroduction \n\n“Cross-compilation” means compiling a program on one machine for another type of machine. For example, a typical use of cross-compilation is to compile programs for embedded devices. These devices often don’t have the computing power and memory to compile their own programs. One might think that cross-compilation is a fairly niche concern. However, there are significant advantages to rigorously distinguishing between build-time and run-time environments! Significant, because the benefits apply even when one is developing and deploying on the same machine. Nixpkgs is increasingly adopting the opinion that packages should be written with cross-compilation in mind, and Nixpkgs should evaluate in a similar way (by minimizing cross-compilation-specific special cases) whether or not one is cross-compiling.\n\nThis chapter will be organized in three parts. First, it will describe the basics of how to package software in a way that supports cross-compilation. Second, it will describe how to use Nixpkgs when cross-compiling. Third, it will describe the internal infrastructure supporting cross-compilation.\n\nPackaging in a cross-friendly manner \nPlatform parameters\nTheory of dependency categorization\nCross packaging cookbook\nPlatform parameters \n\nNixpkgs follows the conventions of GNU autoconf. We distinguish between 3 types of platforms when building a derivation: build, host, and target. In summary, build is the platform on which a package is being built, host is the platform on which it will run. The third attribute, target, is relevant only for certain specific compilers and build tools.\n\nIn Nixpkgs, these three platforms are defined as attribute sets under the names buildPlatform, hostPlatform, and targetPlatform. They are always defined as attributes in the standard environment. That means one can access them like:\n\n{ stdenv, fooDep, barDep, ... }: ...stdenv.buildPlatform...\n\nbuildPlatform\n\nThe “build platform” is the platform on which a package is built. Once someone has a built package, or pre-built binary package, the build platform should not matter and can be ignored.\n\nhostPlatform\n\nThe “host platform” is the platform on which a package will be run. This is the simplest platform to understand, but also the one with the worst name.\n\ntargetPlatform\n\nThe “target platform” attribute is, unlike the other two attributes, not actually fundamental to the process of building software. Instead, it is only relevant for compatibility with building certain specific compilers and build tools. It can be safely ignored for all other packages.\n\nThe build process of certain compilers is written in such a way that the compiler resulting from a single build can itself only produce binaries for a single platform. The task of specifying this single “target platform” is thus pushed to build time of the compiler. The root cause of this is that the compiler (which will be run on the host) and the standard library/runtime (which will be run on the target) are built by a single build process.\n\nThere is no fundamental need to think about a single target ahead of time like this. If the tool supports modular or pluggable backends, both the need to specify the target at build time and the constraint of having only a single target disappear. An example of such a tool is LLVM.\n\nAlthough the existence of a “target platform” is arguably a historical mistake, it is a common one: examples of tools that suffer from it are GCC, Binutils, GHC and Autoconf. Nixpkgs tries to avoid sharing in the mistake where possible. Still, because the concept of a target platform is so ingrained, it is best to support it as is.\n\nThe exact schema these fields follow is a bit ill-defined due to a long and convoluted evolution, but this is slowly being cleaned up. You can see examples of ones used in practice in lib.systems.examples; note how they are not all very consistent. For now, here are few fields can count on them containing:\n\nsystem\n\nThis is a two-component shorthand for the platform. Examples of this would be “x86_64-darwin” and “i686-linux”; see lib.systems.doubles for more. The first component corresponds to the CPU architecture of the platform and the second to the operating system of the platform ([cpu]-[os]). This format has built-in support in Nix, such as the builtins.currentSystem impure string.\n\nconfig\n\nThis is a 3- or 4- component shorthand for the platform. Examples of this would be x86_64-unknown-linux-gnu and aarch64-apple-darwin14. This is a standard format called the “LLVM target triple”, as they are pioneered by LLVM. In the 4-part form, this corresponds to [cpu]-[vendor]-[os]-[abi]. This format is strictly more informative than the “Nix host double”, as the previous format could analogously be termed. This needs a better name than config!\n\nparsed\n\nThis is a Nix representation of a parsed LLVM target triple with white-listed components. This can be specified directly, or actually parsed from the config. See lib.systems.parse for the exact representation.\n\nlibc\n\nThis is a string identifying the standard C library used. Valid identifiers include “glibc” for GNU libc, “libSystem” for Darwin’s Libsystem, and “uclibc” for µClibc. It should probably be refactored to use the module system, like parse.\n\nis*\n\nThese predicates are defined in lib.systems.inspect, and slapped onto every platform. They are superior to the ones in stdenv as they force the user to be explicit about which platform they are inspecting. Please use these instead of those.\n\nplatform\n\nThis is, quite frankly, a dumping ground of ad-hoc settings (it’s an attribute set). See lib.systems.platforms for examples—there’s hopefully one in there that will work verbatim for each platform that is working. Please help us triage these flags and give them better homes!\n\nTheory of dependency categorization \nNote\n\nThis is a rather philosophical description that isn’t very Nixpkgs-specific. For an overview of all the relevant attributes given to mkDerivation, see the section called “Specifying dependencies”. For a description of how everything is implemented, see the section called “Implementation of dependencies”.\n\nIn this section we explore the relationship between both runtime and build-time dependencies and the 3 Autoconf platforms.\n\nA run time dependency between two packages requires that their host platforms match. This is directly implied by the meaning of “host platform” and “runtime dependency”: The package dependency exists while both packages are running on a single host platform.\n\nA build time dependency, however, has a shift in platforms between the depending package and the depended-on package. “build time dependency” means that to build the depending package we need to be able to run the depended-on’s package. The depending package’s build platform is therefore equal to the depended-on package’s host platform.\n\nIf both the dependency and depending packages aren’t compilers or other machine-code-producing tools, we’re done. And indeed buildInputs and nativeBuildInputs have covered these simpler cases for many years. But if the dependency does produce machine code, we might need to worry about its target platform too. In principle, that target platform might be any of the depending package’s build, host, or target platforms, but we prohibit dependencies from a “later” platform to an earlier platform to limit confusion because we’ve never seen a legitimate use for them.\n\nFinally, if the depending package is a compiler or other machine-code-producing tool, it might need dependencies that run at “emit time”. This is for compilers that (regrettably) insist on being built together with their source languages’ standard libraries. Assuming build != host != target, a run-time dependency of the standard library cannot be run at the compiler’s build time or run time, but only at the run time of code emitted by the compiler.\n\nPutting this all together, that means that we have dependency types of the form “X→ E”, which means that the dependency executes on X and emits code for E; each of X and E can be build, host, or target, and E can be * to indicate that the dependency is not a compiler-like package.\n\nDependency types describe the relationships that a package has with each of its transitive dependencies. You could think of attaching one or more dependency types to each of the formal parameters at the top of a package’s .nix file, as well as to all of their formal parameters, and so on. Triples like (foo, bar, baz), on the other hand, are a property of an instantiated derivation – you could would attach a triple (mips-linux, mips-linux, sparc-solaris) to a .drv file in /nix/store.\n\nOnly nine dependency types matter in practice:\n\nPossible dependency types\nDependency type\tDependency’s host platform\tDependency’s target platform\nbuild → *\tbuild\t(none)\nbuild → build\tbuild\tbuild\nbuild → host\tbuild\thost\nbuild → target\tbuild\ttarget\nhost → *\thost\t(none)\nhost → host\thost\thost\nhost → target\thost\ttarget\ntarget → *\ttarget\t(none)\ntarget → target\ttarget\ttarget\n\nLet’s use g++ as an example to make this table clearer. g++ is a C++ compiler written in C. Suppose we are building g++ with a (build, host, target) platform triple of (foo, bar, baz). This means we are using a foo-machine to build a copy of g++ which will run on a bar-machine and emit binaries for the baz-machine.\n\ng++ links against the host platform’s glibc C library, which is a “host→ *” dependency with a triple of (bar, bar, *). Since it is a library, not a compiler, it has no “target”.\n\nSince g++ is written in C, the gcc compiler used to compile it is a “build→ host” dependency of g++ with a triple of (foo, foo, bar). This compiler runs on the build platform and emits code for the host platform.\n\ngcc links against the build platform’s glibc C library, which is a “build→ *” dependency with a triple of (foo, foo, *). Since it is a library, not a compiler, it has no “target”.\n\nThis gcc is itself compiled by an earlier copy of gcc. This earlier copy of gcc is a “build→ build” dependency of g++ with a triple of (foo, foo, foo). This “early gcc” runs on the build platform and emits code for the build platform.\n\ng++ is bundled with libgcc, which includes a collection of target-machine routines for exception handling and software floating point emulation. libgcc would be a “target→ *” dependency with triple (foo, baz, *), because it consists of machine code which gets linked against the output of the compiler that we are building. It is a library, not a compiler, so it has no target of its own.\n\nlibgcc is written in C and compiled with gcc. The gcc that compiles it will be a “build→ target” dependency with triple (foo, foo, baz). It gets compiled and run at g++-build-time (on platform foo), but must emit code for the baz-platform.\n\ng++ allows inline assembler code, so it depends on access to a copy of the gas assembler. This would be a “host→ target” dependency with triple (foo, bar, baz).\n\ng++ (and gcc) include a library libgccjit.so, which wrap the compiler in a library to create a just-in-time compiler. In nixpkgs, this library is in the libgccjit package; if C++ required that programs have access to a JIT, g++ would need to add a “target→ target” dependency for libgccjit with triple (foo, baz, baz). This would ensure that the compiler ships with a copy of libgccjit which both executes on and generates code for the baz-platform.\n\nIf g++ itself linked against libgccjit.so (for example, to allow compile-time-evaluated C++ expressions), then the libgccjit package used to provide this functionality would be a “host→ host” dependency of g++: it is code which runs on the host and emits code for execution on the host.\n\nCross packaging cookbook \n\nSome frequently encountered problems when packaging for cross-compilation should be answered here. Ideally, the information above is exhaustive, so this section cannot provide any new information, but it is ludicrous and cruel to expect everyone to spend effort working through the interaction of many features just to figure out the same answer to the same common problem. Feel free to add to this list!\n\nMy package fails to find a binutils command (cc/ar/ld etc.)\n\nMany packages assume that an unprefixed binutils (cc/ar/ld etc.) is available, but Nix doesn’t provide one. It only provides a prefixed one, just as it only does for all the other binutils programs. It may be necessary to patch the package to fix the build system to use a prefix. For instance, instead of cc, use ${stdenv.cc.targetPrefix}cc.\n\nmakeFlags = [ \"CC=${stdenv.cc.targetPrefix}cc\" ];\n\nHow do I avoid compiling a GCC cross-compiler from source?\n\nOn less powerful machines, it can be inconvenient to cross-compile a package only to find out that GCC has to be compiled from source, which could take up to several hours. Nixpkgs maintains a limited cross-related jobset on Hydra, which tests cross-compilation to various platforms from build platforms “x86_64-darwin”, “x86_64-linux”, and “aarch64-linux”. See pkgs/top-level/release-cross.nix for the full list of target platforms and packages. For instance, the following invocation fetches the pre-built cross-compiled GCC for armv6l-unknown-linux-gnueabihf and builds GNU Hello from source.\n\n$ nix-build '<nixpkgs>' -A pkgsCross.raspberryPi.hello\n\nWhat if my package’s build system needs to build a C program to be run under the build environment?\n\nAdd the following to your mkDerivation invocation.\n\ndepsBuildBuild = [ buildPackages.stdenv.cc ];\n\nMy package’s testsuite needs to run host platform code.\n\nAdd the following to your mkDerivation invocation.\n\ndoCheck = stdenv.buildPlatform.canExecute stdenv.hostPlatform;\n\nPackage using Meson needs to run binaries for the host platform during build.\n\nAdd mesonEmulatorHook to nativeBuildInputs conditionally on if the target binaries can be executed.\n\ne.g.\n\nnativeBuildInputs = [\n  meson\n] ++ lib.optionals (!stdenv.buildPlatform.canExecute stdenv.hostPlatform) [\n  mesonEmulatorHook\n];\n\n\nExample of an error which this fixes.\n\n[Errno 8] Exec format error: './gdk3-scan'\n\nCross-building packages \n\nNixpkgs can be instantiated with localSystem alone, in which case there is no cross-compiling and everything is built by and for that system, or also with crossSystem, in which case packages run on the latter, but all building happens on the former. Both parameters take the same schema as the 3 (build, host, and target) platforms defined in the previous section. As mentioned above, lib.systems.examples has some platforms which are used as arguments for these parameters in practice. You can use them programmatically, or on the command line:\n\n$ nix-build '<nixpkgs>' --arg crossSystem '(import <nixpkgs/lib>).systems.examples.fooBarBaz' -A whatever\n\nNote\n\nEventually we would like to make these platform examples an unnecessary convenience so that\n\n$ nix-build '<nixpkgs>' --arg crossSystem '{ config = \"<arch>-<os>-<vendor>-<abi>\"; }' -A whatever\n\n\nworks in the vast majority of cases. The problem today is dependencies on other sorts of configuration which aren’t given proper defaults. We rely on the examples to crudely to set those configuration parameters in some vaguely sane manner on the users behalf. Issue #34274 tracks this inconvenience along with its root cause in crufty configuration options.\n\nWhile one is free to pass both parameters in full, there’s a lot of logic to fill in missing fields. As discussed in the previous section, only one of system, config, and parsed is needed to infer the other two. Additionally, libc will be inferred from parse. Finally, localSystem.system is also impurely inferred based on the platform evaluation occurs. This means it is often not necessary to pass localSystem at all, as in the command-line example in the previous paragraph.\n\nNote\n\nMany sources (manual, wiki, etc) probably mention passing system, platform, along with the optional crossSystem to Nixpkgs: import <nixpkgs> { system = ..; platform = ..; crossSystem = ..; }. Passing those two instead of localSystem is still supported for compatibility, but is discouraged. Indeed, much of the inference we do for these parameters is motivated by compatibility as much as convenience.\n\nOne would think that localSystem and crossSystem overlap horribly with the three *Platforms (buildPlatform, hostPlatform, and targetPlatform; see stage.nix or the manual). Actually, those identifiers are purposefully not used here to draw a subtle but important distinction: While the granularity of having 3 platforms is necessary to properly build packages, it is overkill for specifying the user’s intent when making a build plan or package set. A simple “build vs deploy” dichotomy is adequate: the sliding window principle described in the previous section shows how to interpolate between the these two “end points” to get the 3 platform triple for each bootstrapping stage. That means for any package a given package set, even those not bound on the top level but only reachable via dependencies or buildPackages, the three platforms will be defined as one of localSystem or crossSystem, with the former replacing the latter as one traverses build-time dependencies. A last simple difference is that crossSystem should be null when one doesn’t want to cross-compile, while the *Platforms are always non-null. localSystem is always non-null.\n\nCross-compilation infrastructure \nImplementation of dependencies\nBootstrapping\nImplementation of dependencies \n\nThe categories of dependencies developed in the section called “Theory of dependency categorization” are specified as lists of derivations given to mkDerivation, as documented in the section called “Specifying dependencies”. In short, each list of dependencies for “host → target” is called deps<host><target> (where host, and target values are either build, host, or target), with exceptions for backwards compatibility that depsBuildHost is instead called nativeBuildInputs and depsHostTarget is instead called buildInputs. Nixpkgs is now structured so that each deps<host><target> is automatically taken from pkgs<host><target>. (These pkgs<host><target>s are quite new, so there is no special case for nativeBuildInputs and buildInputs.) For example, pkgsBuildHost.gcc should be used at build-time, while pkgsHostTarget.gcc should be used at run-time.\n\nNow, for most of Nixpkgs’s history, there were no pkgs<host><target> attributes, and most packages have not been refactored to use it explicitly. Prior to those, there were just buildPackages, pkgs, and targetPackages. Those are now redefined as aliases to pkgsBuildHost, pkgsHostTarget, and pkgsTargetTarget. It is acceptable, even recommended, to use them for libraries to show that the host platform is irrelevant.\n\nBut before that, there was just pkgs, even though both buildInputs and nativeBuildInputs existed. [Cross barely worked, and those were implemented with some hacks on mkDerivation to override dependencies.] What this means is the vast majority of packages do not use any explicit package set to populate their dependencies, just using whatever callPackage gives them even if they do correctly sort their dependencies into the multiple lists described above. And indeed, asking that users both sort their dependencies, and take them from the right attribute set, is both too onerous and redundant, so the recommended approach (for now) is to continue just categorizing by list and not using an explicit package set.\n\nTo make this work, we “splice” together the six pkgsFooBar package sets and have callPackage actually take its arguments from that. This is currently implemented in pkgs/top-level/splice.nix. mkDerivation then, for each dependency attribute, pulls the right derivation out from the splice. This splicing can be skipped when not cross-compiling as the package sets are the same, but still is a bit slow for cross-compiling. We’d like to do something better, but haven’t come up with anything yet.\n\nBootstrapping \n\nEach of the package sets described above come from a single bootstrapping stage. While pkgs/top-level/default.nix, coordinates the composition of stages at a high level, pkgs/top-level/stage.nix “ties the knot” (creates the fixed point) of each stage. The package sets are defined per-stage however, so they can be thought of as edges between stages (the nodes) in a graph. Compositions like pkgsBuildTarget.targetPackages can be thought of as paths to this graph.\n\nWhile there are many package sets, and thus many edges, the stages can also be arranged in a linear chain. In other words, many of the edges are redundant as far as connectivity is concerned. This hinges on the type of bootstrapping we do. Currently for cross it is:\n\n(native, native, native)\n\n(native, native, foreign)\n\n(native, foreign, foreign)\n\nIn each stage, pkgsBuildHost refers to the previous stage, pkgsBuildBuild refers to the one before that, and pkgsHostTarget refers to the current one, and pkgsTargetTarget refers to the next one. When there is no previous or next stage, they instead refer to the current stage. Note how all the invariants regarding the mapping between dependency and depending packages’ build host and target platforms are preserved. pkgsBuildTarget and pkgsHostHost are more complex in that the stage fitting the requirements isn’t always a fixed chain of “prevs” and “nexts” away (modulo the “saturating” self-references at the ends). We just special case each instead. All the primary edges are implemented is in pkgs/stdenv/booter.nix, and secondarily aliases in pkgs/top-level/stage.nix.\n\nNote\n\nThe native stages are bootstrapped in legacy ways that predate the current cross implementation. This is why the bootstrapping stages leading up to the final stages are ignored in the previous paragraph.\n\nIf one looks at the 3 platform triples, one can see that they overlap such that one could put them together into a chain like:\n\n(native, native, native, foreign, foreign)\n\n\nIf one imagines the saturating self references at the end being replaced with infinite stages, and then overlays those platform triples, one ends up with the infinite tuple:\n\n(native..., native, native, native, foreign, foreign, foreign...)\n\n\nOne can then imagine any sequence of platforms such that there are bootstrap stages with their 3 platforms determined by “sliding a window” that is the 3 tuple through the sequence. This was the original model for bootstrapping. Without a target platform (assume a better world where all compilers are multi-target and all standard libraries are built in their own derivation), this is sufficient. Conversely if one wishes to cross compile “faster”, with a “Canadian Cross” bootstrapping stage where build != host != target, more bootstrapping stages are needed since no sliding window provides the pesky pkgsBuildTarget package set since it skips the Canadian cross stage’s “host”.\n\nNote\n\nIt is much better to refer to buildPackages than targetPackages, or more broadly package sets that do not mention “target”. There are three reasons for this.\n\nFirst, it is because bootstrapping stages do not have a unique targetPackages. For example a (x86-linux, x86-linux, arm-linux) and (x86-linux, x86-linux, x86-windows) package set both have a (x86-linux, x86-linux, x86-linux) package set. Because there is no canonical targetPackages for such a native (build == host == target) package set, we set their targetPackages\n\nSecond, it is because this is a frequent source of hard-to-follow “infinite recursions” / cycles. When only package sets that don’t mention target are used, the package set forms a directed acyclic graph. This means that all cycles that exist are confined to one stage. This means they are a lot smaller, and easier to follow in the code or a backtrace. It also means they are present in native and cross builds alike, and so more likely to be caught by CI and other users.\n\nThirdly, it is because everything target-mentioning only exists to accommodate compilers with lousy build systems that insist on the compiler itself and standard library being built together. Of course that is bad because bigger derivations means longer rebuilds. It is also problematic because it tends to make the standard libraries less like other libraries than they could be, complicating code and build systems alike. Because of the other problems, and because of these innate disadvantages, compilers ought to be packaged another way where possible.\n\nNote\n\nIf one explores Nixpkgs, they will see derivations with names like gccCross. Such *Cross derivations is a holdover from before we properly distinguished between the host and target platforms—the derivation with “Cross” in the name covered the build = host != target case, while the other covered the host = target, with build platform the same or not based on whether one was using its .__spliced.buildHost or .__spliced.hostTarget.\n\nPlatform Notes \n\nTable of Contents\n\nDarwin (macOS)\nDarwin (macOS) \n\nSome common issues when packaging software for Darwin:\n\nThe Darwin stdenv uses clang instead of gcc. When referring to the compiler $CC or cc will work in both cases. Some builds hardcode gcc/g++ in their build scripts, that can usually be fixed with using something like makeFlags = [ \"CC=cc\" ]; or by patching the build scripts.\n\nstdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  # ...\n  buildPhase = ''\n    $CC -o hello hello.c\n  '';\n}\n\n\nOn Darwin, libraries are linked using absolute paths, libraries are resolved by their install_name at link time. Sometimes packages won’t set this correctly causing the library lookups to fail at runtime. This can be fixed by adding extra linker flags or by running install_name_tool -id during the fixupPhase.\n\nstdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  # ...\n  makeFlags = lib.optional stdenv.isDarwin \"LDFLAGS=-Wl,-install_name,$(out)/lib/libfoo.dylib\";\n}\n\n\nEven if the libraries are linked using absolute paths and resolved via their install_name correctly, tests can sometimes fail to run binaries. This happens because the checkPhase runs before the libraries are installed.\n\nThis can usually be solved by running the tests after the installPhase or alternatively by using DYLD_LIBRARY_PATH. More information about this variable can be found in the dyld(1) manpage.\n\ndyld: Library not loaded: /nix/store/7hnmbscpayxzxrixrgxvvlifzlxdsdir-jq-1.5-lib/lib/libjq.1.dylib\nReferenced from: /private/tmp/nix-build-jq-1.5.drv-0/jq-1.5/tests/../jq\nReason: image not found\n./tests/jqtest: line 5: 75779 Abort trap: 6\n\nstdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  # ...\n  doInstallCheck = true;\n  installCheckTarget = \"check\";\n}\n\n\nSome packages assume xcode is available and use xcrun to resolve build tools like clang, etc. This causes errors like xcode-select: error: no developer tools were found at '/Applications/Xcode.app' while the build doesn’t actually depend on xcode.\n\nstdenv.mkDerivation {\n  name = \"libfoo-1.2.3\";\n  # ...\n  prePatch = ''\n    substituteInPlace Makefile \\\n        --replace '/usr/bin/xcrun clang' clang\n  '';\n}\n\n\nThe package xcbuild can be used to build projects that really depend on Xcode. However, this replacement is not 100% compatible with Xcode and can occasionally cause issues.\n\nx86_64-darwin uses the 10.12 SDK by default, but some software is not compatible with that version of the SDK. In that case, the 11.0 SDK used by aarch64-darwin is available for use on x86_64-darwin. To use it, reference apple_sdk_11_0 instead of apple_sdk in your derivation and use pkgs.darwin.apple_sdk_11_0.callPackage instead of pkgs.callPackage. On Linux, this will have the same effect as pkgs.callPackage, so you can use pkgs.darwin.apple_sdk_11_0.callPackage regardless of platform.\n\nBuild helpers \n\nA build helper is a function that produces derivations.\n\nWarning\n\nThis is not to be confused with the builder argument of the Nix derivation primitive, which refers to the executable that produces the build result, or remote builder, which refers to a remote machine that could run such an executable.\n\nSuch a function is usually designed to abstract over a typical workflow for a given programming language or framework. This allows declaring a build recipe by setting a limited number of options relevant to the particular use case instead of using the derivation function directly.\n\nstdenv.mkDerivation is the most widely used build helper, and serves as a basis for many others. In addition, it offers various options to customize parts of the builds.\n\nThere is no uniform interface for build helpers. Trivial build helpers and fetchers have various input types for convenience. Language- or framework-specific build helpers usually follow the style of stdenv.mkDerivation, which accepts an attribute set or a fixed-point function taking an attribute set.\n\nTable of Contents\n\nFetchers\nTrivial build helpers\nTesters\nSpecial build helpers\nImages\nHooks reference\nLanguages and frameworks\nPackages\nFetchers \n\nTable of Contents\n\nCaveats\nfetchurl and fetchzip\nfetchpatch\nfetchDebianPatch\nfetchsvn\nfetchgit\nfetchfossil\nfetchcvs\nfetchhg\nfetchFromGitea\nfetchFromGitHub\nfetchFromGitLab\nfetchFromGitiles\nfetchFromBitbucket\nfetchFromSavannah\nfetchFromRepoOrCz\nfetchFromSourcehut\nrequireFile\nfetchtorrent\n\nBuilding software with Nix often requires downloading source code and other files from the internet. To this end, Nixpkgs provides fetchers: functions to obtain remote sources via various protocols and services.\n\nNixpkgs fetchers differ from built-in fetchers such as builtins.fetchTarball:\n\nA built-in fetcher will download and cache files at evaluation time and produce a store path. A Nixpkgs fetcher will create a (fixed-output) derivation, and files are downloaded at build time.\n\nBuilt-in fetchers will invalidate their cache after tarball-ttl expires, and will require network activity to check if the cache entry is up to date. Nixpkgs fetchers only re-download if the specified hash changes or the store object is not otherwise available.\n\nBuilt-in fetchers do not use substituters. Derivations produced by Nixpkgs fetchers will use any configured binary cache transparently.\n\nThis significantly reduces the time needed to evaluate the entirety of Nixpkgs, and allows Hydra to retain and re-distribute sources used by Nixpkgs in the public binary cache. For these reasons, built-in fetchers are not allowed in Nixpkgs source code.\n\nThe following table shows an overview of the differences:\n\nFetchers\tDownload\tOutput\tCache\tRe-download when\nbuiltins.fetch*\tevaluation time\tstore path\t/nix/store, ~/.cache/nix\ttarball-ttl expires, cache miss in ~/.cache/nix, output store object not in local store\npkgs.fetch*\tbuild time\tderivation\t/nix/store, substituters\toutput store object not available\nCaveats \n\nThe fact that the hash belongs to the Nix derivation output and not the file itself can lead to confusion. For example, consider the following fetcher:\n\nfetchurl {\n  url = \"http://www.example.org/hello-1.0.tar.gz\";\n  hash = \"sha256-lTeyxzJNQeMdu1IVdovNMtgn77jRIhSybLdMbTkf2Ww=\";\n};\n\n\nA common mistake is to update a fetcher’s URL, or a version parameter, without updating the hash.\n\nfetchurl {\n  url = \"http://www.example.org/hello-1.1.tar.gz\";\n  hash = \"sha256-lTeyxzJNQeMdu1IVdovNMtgn77jRIhSybLdMbTkf2Ww=\";\n};\n\n\nThis will reuse the old contents. Remember to invalidate the hash argument, in this case by setting the hash attribute to an empty string.\n\nfetchurl {\n  url = \"http://www.example.org/hello-1.1.tar.gz\";\n  hash = \"\";\n};\n\n\nUse the resulting error message to determine the correct hash.\n\nerror: hash mismatch in fixed-output derivation '/path/to/my.drv':\n         specified: sha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n            got:    sha256-lTeyxzJNQeMdu1IVdovNMtgn77jRIhSybLdMbTkf2Ww=\n\n\nA similar problem arises while testing changes to a fetcher’s implementation. If the output of the derivation already exists in the Nix store, test failures can go undetected. The invalidateFetcherByDrvHash function helps prevent reusing cached derivations.\n\nfetchurl and fetchzip \n\nTwo basic fetchers are fetchurl and fetchzip. Both of these have two required arguments, a URL and a hash. The hash is typically hash, although many more hash algorithms are supported. Nixpkgs contributors are currently recommended to use hash. This hash will be used by Nix to identify your source. A typical usage of fetchurl is provided below.\n\n{ stdenv, fetchurl }:\n\nstdenv.mkDerivation {\n  name = \"hello\";\n  src = fetchurl {\n    url = \"http://www.example.org/hello.tar.gz\";\n    hash = \"sha256-BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB=\";\n  };\n}\n\n\nThe main difference between fetchurl and fetchzip is in how they store the contents. fetchurl will store the unaltered contents of the URL within the Nix store. fetchzip on the other hand, will decompress the archive for you, making files and directories directly accessible in the future. fetchzip can only be used with archives. Despite the name, fetchzip is not limited to .zip files and can also be used with any tarball.\n\nfetchpatch \n\nfetchpatch works very similarly to fetchurl with the same arguments expected. It expects patch files as a source and performs normalization on them before computing the checksum. For example, it will remove comments or other unstable parts that are sometimes added by version control systems and can change over time.\n\nrelative: Similar to using git-diff’s --relative flag, only keep changes inside the specified directory, making paths relative to it.\n\nstripLen: Remove the first stripLen components of pathnames in the patch.\n\ndecode: Pipe the downloaded data through this command before processing it as a patch.\n\nextraPrefix: Prefix pathnames by this string.\n\nexcludes: Exclude files matching these patterns (applies after the above arguments).\n\nincludes: Include only files matching these patterns (applies after the above arguments).\n\nrevert: Revert the patch.\n\nNote that because the checksum is computed after applying these effects, using or modifying these arguments will have no effect unless the hash argument is changed as well.\n\nMost other fetchers return a directory rather than a single file.\n\nfetchDebianPatch \n\nA wrapper around fetchpatch, which takes:\n\npatch and hash: the patch’s filename, and its hash after normalization by fetchpatch ;\n\npname: the Debian source package’s name ;\n\nversion: the upstream version number ;\n\ndebianRevision: the Debian revision number if applicable ;\n\nthe area of the Debian archive: main (default), contrib, or non-free.\n\nHere is an example of fetchDebianPatch in action:\n\n{ lib\n, fetchDebianPatch\n, buildPythonPackage\n}:\n\nbuildPythonPackage rec {\n  pname = \"pysimplesoap\";\n  version = \"1.16.2\";\n  src = ...;\n\n  patches = [\n    (fetchDebianPatch {\n      inherit pname version;\n      debianRevision = \"5\";\n      name = \"Add-quotes-to-SOAPAction-header-in-SoapClient.patch\";\n      hash = \"sha256-xA8Wnrpr31H8wy3zHSNfezFNjUJt1HbSXn3qUMzeKc0=\";\n    })\n  ];\n\n  ...\n}\n\n\nPatches are fetched from sources.debian.org, and so must come from a package version that was uploaded to the Debian archive. Packages may be removed from there once that specific version isn’t in any suite anymore (stable, testing, unstable, etc.), so maintainers should use copy-tarballs.pl to archive the patch if it needs to be available longer-term.\n\nfetchsvn \n\nUsed with Subversion. Expects url to a Subversion directory, rev, and hash.\n\nfetchgit \n\nUsed with Git. Expects url to a Git repo, rev, and hash. rev in this case can be full the git commit id (SHA1 hash) or a tag name like refs/tags/v1.0.\n\nAdditionally, the following optional arguments can be given: fetchSubmodules = true makes fetchgit also fetch the submodules of a repository. If deepClone is set to true, the entire repository is cloned as opposing to just creating a shallow clone. deepClone = true also implies leaveDotGit = true which means that the .git directory of the clone won’t be removed after checkout.\n\nIf only parts of the repository are needed, sparseCheckout can be used. This will prevent git from fetching unnecessary blobs from server, see git sparse-checkout for more information:\n\n{ stdenv, fetchgit }:\n\nstdenv.mkDerivation {\n  name = \"hello\";\n  src = fetchgit {\n    url = \"https://...\";\n    sparseCheckout = [\n      \"directory/to/be/included\"\n      \"another/directory\"\n    ];\n    hash = \"sha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\";\n  };\n}\n\nfetchfossil \n\nUsed with Fossil. Expects url to a Fossil archive, rev, and hash.\n\nfetchcvs \n\nUsed with CVS. Expects cvsRoot, tag, and hash.\n\nfetchhg \n\nUsed with Mercurial. Expects url, rev, and hash.\n\nA number of fetcher functions wrap part of fetchurl and fetchzip. They are mainly convenience functions intended for commonly used destinations of source code in Nixpkgs. These wrapper fetchers are listed below.\n\nfetchFromGitea \n\nfetchFromGitea expects five arguments. domain is the gitea server name. owner is a string corresponding to the Gitea user or organization that controls this repository. repo corresponds to the name of the software repository. These are located at the top of every Gitea HTML page as owner/repo. rev corresponds to the Git commit hash or tag (e.g v1.0) that will be downloaded from Git. Finally, hash corresponds to the hash of the extracted directory. Again, other hash algorithms are also available but hash is currently preferred.\n\nfetchFromGitHub \n\nfetchFromGitHub expects four arguments. owner is a string corresponding to the GitHub user or organization that controls this repository. repo corresponds to the name of the software repository. These are located at the top of every GitHub HTML page as owner/repo. rev corresponds to the Git commit hash or tag (e.g v1.0) that will be downloaded from Git. Finally, hash corresponds to the hash of the extracted directory. Again, other hash algorithms are also available, but hash is currently preferred.\n\nTo use a different GitHub instance, use githubBase (defaults to \"github.com\").\n\nfetchFromGitHub uses fetchzip to download the source archive generated by GitHub for the specified revision. If leaveDotGit, deepClone or fetchSubmodules are set to true, fetchFromGitHub will use fetchgit instead. Refer to its section for documentation of these options.\n\nfetchFromGitLab \n\nThis is used with GitLab repositories. It behaves similarly to fetchFromGitHub, and expects owner, repo, rev, and hash.\n\nTo use a specific GitLab instance, use domain (defaults to \"gitlab.com\").\n\nfetchFromGitiles \n\nThis is used with Gitiles repositories. The arguments expected are similar to fetchgit.\n\nfetchFromBitbucket \n\nThis is used with BitBucket repositories. The arguments expected are very similar to fetchFromGitHub above.\n\nfetchFromSavannah \n\nThis is used with Savannah repositories. The arguments expected are very similar to fetchFromGitHub above.\n\nfetchFromRepoOrCz \n\nThis is used with repo.or.cz repositories. The arguments expected are very similar to fetchFromGitHub above.\n\nfetchFromSourcehut \n\nThis is used with sourcehut repositories. Similar to fetchFromGitHub above, it expects owner, repo, rev and hash, but don’t forget the tilde (~) in front of the username! Expected arguments also include vc (“git” (default) or “hg”), domain and fetchSubmodules.\n\nIf fetchSubmodules is true, fetchFromSourcehut uses fetchgit or fetchhg with fetchSubmodules or fetchSubrepos set to true, respectively. Otherwise, the fetcher uses fetchzip.\n\nrequireFile \n\nrequireFile allows requesting files that cannot be fetched automatically, but whose content is known. This is a useful last-resort workaround for license restrictions that prohibit redistribution, or for downloads that are only accessible after authenticating interactively in a browser. If the requested file is present in the Nix store, the resulting derivation will not be built, because its expected output is already available. Otherwise, the builder will run, but fail with a message explaining to the user how to provide the file. The following code, for example:\n\nrequireFile {\n  name = \"jdk-${version}_linux-x64_bin.tar.gz\";\n  url = \"https://www.oracle.com/java/technologies/javase-jdk11-downloads.html\";\n  hash = \"sha256-lL00+F7jjT71nlKJ7HRQuUQ7kkxVYlZh//5msD8sjeI=\";\n}\n\n\nresults in this error message:\n\n***\nUnfortunately, we cannot download file jdk-11.0.10_linux-x64_bin.tar.gz automatically.\nPlease go to https://www.oracle.com/java/technologies/javase-jdk11-downloads.html to download it yourself, and add it to the Nix store\nusing either\n  nix-store --add-fixed sha256 jdk-11.0.10_linux-x64_bin.tar.gz\nor\n  nix-prefetch-url --type sha256 file:///path/to/jdk-11.0.10_linux-x64_bin.tar.gz\n\n***\n\nfetchtorrent \nParameters\n\nfetchtorrent expects two arguments. url which can either be a Magnet URI (Magnet Link) such as magnet:?xt=urn:btih:dd8255ecdc7ca55fb0bbf81323d87062db1f6d1c or an HTTP URL pointing to a .torrent file. It can also take a config argument which will craft a settings.json configuration file and give it to transmission, the underlying program that is performing the fetch. The available config options for transmission can be found here\n\n{ fetchtorrent }:\n\nfetchtorrent {\n  config = { peer-limit-global = 100; };\n  url = \"magnet:?xt=urn:btih:dd8255ecdc7ca55fb0bbf81323d87062db1f6d1c\";\n  sha256 = \"\";\n}\n\nParameters \n\nurl: Magnet URI (Magnet Link) such as magnet:?xt=urn:btih:dd8255ecdc7ca55fb0bbf81323d87062db1f6d1c or an HTTP URL pointing to a .torrent file.\n\nbackend: Which bittorrent program to use. Default: \"transmission\". Valid values are \"rqbit\" or \"transmission\". These are the two most suitable torrent clients for fetching in a fixed-output derivation at the time of writing, as they can be easily exited after usage. rqbit is written in Rust and has a smaller closure size than transmission, and the performance and peer discovery properties differs between these clients, requiring experimentation to decide upon which is the best.\n\nconfig: When using transmission as the backend, a json configuration can be supplied to transmission. Refer to the upstream documentation for information on how to configure.\n\nTrivial build helpers \n\nTable of Contents\n\nrunCommand\nrunCommandCC\nrunCommandLocal\nwriteTextFile, writeText, writeTextDir, writeScript, writeScriptBin\nconcatTextFile, concatText, concatScript\nwriteShellApplication\nsymlinkJoin\nwriteReferencesToFile\nwriteDirectReferencesToFile\n\nNixpkgs provides a couple of functions that help with building derivations. The most important one, stdenv.mkDerivation, has already been documented above. The following functions wrap stdenv.mkDerivation, making it easier to use in certain cases.\n\nrunCommand \n\nrunCommand :: String -> AttrSet -> String -> Derivation\n\nrunCommand name drvAttrs buildCommand returns a derivation that is built by running the specified shell commands.\n\nname :: String\n\nThe name that Nix will append to the store path in the same way that stdenv.mkDerivation uses its name attribute.\n\ndrvAttr :: AttrSet\n\nAttributes to pass to the underlying call to stdenv.mkDerivation.\n\nbuildCommand :: String\n\nShell commands to run in the derivation builder.\n\nNote\n\nYou have to create a file or directory $out for Nix to be able to run the builder successfully.\n\nExample 221. Invocation of runCommand\n\n(import <nixpkgs> {}).runCommand \"my-example\" {} ''\n  echo My example command is running\n\n  mkdir $out\n\n  echo I can write data to the Nix store > $out/message\n\n  echo I can also run basic commands like:\n\n  echo ls\n  ls\n\n  echo whoami\n  whoami\n\n  echo date\n  date\n''\n\n\n\nrunCommandCC \n\nThis works just like runCommand. The only difference is that it also provides a C compiler in buildCommand’s environment. To minimize your dependencies, you should only use this if you are sure you will need a C compiler as part of running your command.\n\nrunCommandLocal \n\nVariant of runCommand that forces the derivation to be built locally, it is not substituted. This is intended for very cheap commands (<1s execution time). It saves on the network round-trip and can speed up a build.\n\nNote\n\nThis sets allowSubstitutes to false, so only use runCommandLocal if you are certain the user will always have a builder for the system of the derivation. This should be true for most trivial use cases (e.g., just copying some files to a different location or adding symlinks) because there the system is usually the same as builtins.currentSystem.\n\nwriteTextFile, writeText, writeTextDir, writeScript, writeScriptBin \n\nThese functions write text to the Nix store. This is useful for creating scripts from Nix expressions. writeTextFile takes an attribute set and expects two arguments, name and text. name corresponds to the name used in the Nix store path. text will be the contents of the file. You can also set executable to true to make this file have the executable bit set.\n\nMany more commands wrap writeTextFile including writeText, writeTextDir, writeScript, and writeScriptBin. These are convenience functions over writeTextFile.\n\nHere are a few examples:\n\n# Writes my-file to /nix/store/<store path>\nwriteTextFile {\n  name = \"my-file\";\n  text = ''\n    Contents of File\n  '';\n}\n# See also the `writeText` helper function below.\n\n# Writes executable my-file to /nix/store/<store path>/bin/my-file\nwriteTextFile {\n  name = \"my-file\";\n  text = ''\n    Contents of File\n  '';\n  executable = true;\n  destination = \"/bin/my-file\";\n}\n# Writes contents of file to /nix/store/<store path>\nwriteText \"my-file\"\n  ''\n  Contents of File\n  '';\n# Writes contents of file to /nix/store/<store path>/share/my-file\nwriteTextDir \"share/my-file\"\n  ''\n  Contents of File\n  '';\n# Writes my-file to /nix/store/<store path> and makes executable\nwriteScript \"my-file\"\n  ''\n  Contents of File\n  '';\n# Writes my-file to /nix/store/<store path>/bin/my-file and makes executable.\nwriteScriptBin \"my-file\"\n  ''\n  Contents of File\n  '';\n# Writes my-file to /nix/store/<store path> and makes executable.\nwriteShellScript \"my-file\"\n  ''\n  Contents of File\n  '';\n# Writes my-file to /nix/store/<store path>/bin/my-file and makes executable.\nwriteShellScriptBin \"my-file\"\n  ''\n  Contents of File\n  '';\n\n\nconcatTextFile, concatText, concatScript \n\nThese functions concatenate files to the Nix store in a single file. This is useful for configuration files structured in lines of text. concatTextFile takes an attribute set and expects two arguments, name and files. name corresponds to the name used in the Nix store path. files will be the files to be concatenated. You can also set executable to true to make this file have the executable bit set. concatText andconcatScript are simple wrappers over concatTextFile.\n\nHere are a few examples:\n\n\n# Writes my-file to /nix/store/<store path>\nconcatTextFile {\n  name = \"my-file\";\n  files = [ drv1 \"${drv2}/path/to/file\" ];\n}\n# See also the `concatText` helper function below.\n\n# Writes executable my-file to /nix/store/<store path>/bin/my-file\nconcatTextFile {\n  name = \"my-file\";\n  files = [ drv1 \"${drv2}/path/to/file\" ];\n  executable = true;\n  destination = \"/bin/my-file\";\n}\n# Writes contents of files to /nix/store/<store path>\nconcatText \"my-file\" [ file1 file2 ]\n\n# Writes contents of files to /nix/store/<store path>\nconcatScript \"my-file\" [ file1 file2 ]\n\nwriteShellApplication \n\nThis can be used to easily produce a shell script that has some dependencies (runtimeInputs). It automatically sets the PATH of the script to contain all of the listed inputs, sets some sanity shellopts (errexit, nounset, pipefail), and checks the resulting script with shellcheck.\n\nFor example, look at the following code:\n\nwriteShellApplication {\n  name = \"show-nixos-org\";\n\n  runtimeInputs = [ curl w3m ];\n\n  text = ''\n    curl -s 'https://nixos.org' | w3m -dump -T text/html\n  '';\n}\n\n\nUnlike with normal writeShellScriptBin, there is no need to manually write out ${curl}/bin/curl, setting the PATH was handled by writeShellApplication. Moreover, the script is being checked with shellcheck for more strict validation.\n\nsymlinkJoin \n\nThis can be used to put many derivations into the same directory structure. It works by creating a new derivation and adding symlinks to each of the paths listed. It expects two arguments, name, and paths. name is the name used in the Nix store path for the created derivation. paths is a list of paths that will be symlinked. These paths can be to Nix store derivations or any other subdirectory contained within. Here is an example:\n\n# adds symlinks of hello and stack to current build and prints \"links added\"\nsymlinkJoin { name = \"myexample\"; paths = [ pkgs.hello pkgs.stack ]; postBuild = \"echo links added\"; }\n\n\nThis creates a derivation with a directory structure like the following:\n\n/nix/store/sglsr5g079a5235hy29da3mq3hv8sjmm-myexample\n|-- bin\n|   |-- hello -> /nix/store/qy93dp4a3rqyn2mz63fbxjg228hffwyw-hello-2.10/bin/hello\n|   `-- stack -> /nix/store/6lzdpxshx78281vy056lbk553ijsdr44-stack-2.1.3.1/bin/stack\n`-- share\n    |-- bash-completion\n    |   `-- completions\n    |       `-- stack -> /nix/store/6lzdpxshx78281vy056lbk553ijsdr44-stack-2.1.3.1/share/bash-completion/completions/stack\n    |-- fish\n    |   `-- vendor_completions.d\n    |       `-- stack.fish -> /nix/store/6lzdpxshx78281vy056lbk553ijsdr44-stack-2.1.3.1/share/fish/vendor_completions.d/stack.fish\n...\n\nwriteReferencesToFile \n\nWrites the closure of transitive dependencies to a file.\n\nThis produces the equivalent of nix-store -q --requisites.\n\nFor example,\n\nwriteReferencesToFile (writeScriptBin \"hi\" ''${hello}/bin/hello'')\n\n\nproduces an output path /nix/store/<hash>-runtime-deps containing\n\n/nix/store/<hash>-hello-2.10\n/nix/store/<hash>-hi\n/nix/store/<hash>-libidn2-2.3.0\n/nix/store/<hash>-libunistring-0.9.10\n/nix/store/<hash>-glibc-2.32-40\n\n\nYou can see that this includes hi, the original input path, hello, which is a direct reference, but also the other paths that are indirectly required to run hello.\n\nwriteDirectReferencesToFile \n\nWrites the set of references to the output file, that is, their immediate dependencies.\n\nThis produces the equivalent of nix-store -q --references.\n\nFor example,\n\nwriteDirectReferencesToFile (writeScriptBin \"hi\" ''${hello}/bin/hello'')\n\n\nproduces an output path /nix/store/<hash>-runtime-references containing\n\n/nix/store/<hash>-hello-2.10\n\n\nbut none of hello’s dependencies because those are not referenced directly by hi’s output.\n\nTesters \n\nTable of Contents\n\nhasPkgConfigModules\ntestVersion\ntestBuildFailure\ntestEqualContents\ntestEqualDerivation\ninvalidateFetcherByDrvHash\nrunNixOSTest\nnixosTest\n\nThis chapter describes several testing builders which are available in the testers namespace.\n\nhasPkgConfigModules \n\nChecks whether a package exposes a given list of pkg-config modules. If the moduleNames argument is omitted, hasPkgConfigModules will use meta.pkgConfigModules.\n\nExample:\n\npassthru.tests.pkg-config = testers.hasPkgConfigModules {\n  package = finalAttrs.finalPackage;\n  moduleNames = [ \"libfoo\" ];\n};\n\n\nIf the package in question has meta.pkgConfigModules set, it is even simpler:\n\npassthru.tests.pkg-config = testers.hasPkgConfigModules {\n  package = finalAttrs.finalPackage;\n};\n\nmeta.pkgConfigModules = [ \"libfoo\" ];\n\ntestVersion \n\nChecks the command output contains the specified version\n\nAlthough simplistic, this test assures that the main program can run. While there’s no substitute for a real test case, it does catch dynamic linking errors and such. It also provides some protection against accidentally building the wrong version, for example when using an ‘old’ hash in a fixed-output derivation.\n\nExamples:\n\npassthru.tests.version = testers.testVersion { package = hello; };\n\npassthru.tests.version = testers.testVersion {\n  package = seaweedfs;\n  command = \"weed version\";\n};\n\npassthru.tests.version = testers.testVersion {\n  package = key;\n  command = \"KeY --help\";\n  # Wrong '2.5' version in the code. Drop on next version.\n  version = \"2.5\";\n};\n\npassthru.tests.version = testers.testVersion {\n  package = ghr;\n  # The output needs to contain the 'version' string without any prefix or suffix.\n  version = \"v${version}\";\n};\n\ntestBuildFailure \n\nMake sure that a build does not succeed. This is useful for testing testers.\n\nThis returns a derivation with an override on the builder, with the following effects:\n\nFail the build when the original builder succeeds\n\nMove $out to $out/result, if it exists (assuming out is the default output)\n\nSave the build log to $out/testBuildFailure.log (same)\n\nExample:\n\nrunCommand \"example\" {\n  failed = testers.testBuildFailure (runCommand \"fail\" {} ''\n    echo ok-ish >$out\n    echo failing though\n    exit 3\n  '');\n} ''\n  grep -F 'ok-ish' $failed/result\n  grep -F 'failing though' $failed/testBuildFailure.log\n  [[ 3 = $(cat $failed/testBuildFailure.exit) ]]\n  touch $out\n'';\n\n\nWhile testBuildFailure is designed to keep changes to the original builder’s environment to a minimum, some small changes are inevitable.\n\nThe file $TMPDIR/testBuildFailure.log is present. It should not be deleted.\n\nstdout and stderr are a pipe instead of a tty. This could be improved.\n\nOne or two extra processes are present in the sandbox during the original builder’s execution.\n\nThe derivation and output hashes are different, but not unusual.\n\nThe derivation includes a dependency on buildPackages.bash and expect-failure.sh, which is built to include a transitive dependency on buildPackages.coreutils and possibly more. These are not added to PATH or any other environment variable, so they should be hard to observe.\n\ntestEqualContents \n\nCheck that two paths have the same contents.\n\nExample:\n\ntesters.testEqualContents {\n  assertion = \"sed -e performs replacement\";\n  expected = writeText \"expected\" ''\n    foo baz baz\n  '';\n  actual = runCommand \"actual\" {\n    # not really necessary for a package that's in stdenv\n    nativeBuildInputs = [ gnused ];\n    base = writeText \"base\" ''\n      foo bar baz\n    '';\n  } ''\n    sed -e 's/bar/baz/g' $base >$out\n  '';\n}\n\ntestEqualDerivation \n\nChecks that two packages produce the exact same build instructions.\n\nThis can be used to make sure that a certain difference of configuration, such as the presence of an overlay does not cause a cache miss.\n\nWhen the derivations are equal, the return value is an empty file. Otherwise, the build log explains the difference via nix-diff.\n\nExample:\n\ntesters.testEqualDerivation\n  \"The hello package must stay the same when enabling checks.\"\n  hello\n  (hello.overrideAttrs(o: { doCheck = true; }))\n\ninvalidateFetcherByDrvHash \n\nUse the derivation hash to invalidate the output via name, for testing.\n\nType: (a@{ name, ... } -> Derivation) -> a -> Derivation\n\nNormally, fixed output derivations can and should be cached by their output hash only, but for testing we want to re-fetch everytime the fetcher changes.\n\nChanges to the fetcher become apparent in the drvPath, which is a hash of how to fetch, rather than a fixed store path. By inserting this hash into the name, we can make sure to re-run the fetcher every time the fetcher changes.\n\nThis relies on the assumption that Nix isn’t clever enough to reuse its database of local store contents to optimize fetching.\n\nYou might notice that the “salted” name derives from the normal invocation, not the final derivation. invalidateFetcherByDrvHash has to invoke the fetcher function twice: once to get a derivation hash, and again to produce the final fixed output derivation.\n\nExample:\n\ntests.fetchgit = testers.invalidateFetcherByDrvHash fetchgit {\n  name = \"nix-source\";\n  url = \"https://github.com/NixOS/nix\";\n  rev = \"9d9dbe6ed05854e03811c361a3380e09183f4f4a\";\n  hash = \"sha256-7DszvbCNTjpzGRmpIVAWXk20P0/XTrWZ79KSOGLrUWY=\";\n};\n\nrunNixOSTest \n\nA helper function that behaves exactly like the NixOS runTest, except it also assigns this Nixpkgs package set as the pkgs of the test and makes the nixpkgs.* options read-only.\n\nIf your test is part of the Nixpkgs repository, or if you need a more general entrypoint, see “Calling a test” in the NixOS manual.\n\nExample:\n\npkgs.testers.runNixOSTest ({ lib, ... }: {\n  name = \"hello\";\n  nodes.machine = { pkgs, ... }: {\n    environment.systemPackages = [ pkgs.hello ];\n  };\n  testScript = ''\n    machine.succeed(\"hello\")\n  '';\n})\n\nnixosTest \nParameter\nResult\n\nRun a NixOS VM network test using this evaluation of Nixpkgs.\n\nNOTE: This function is primarily for external use. NixOS itself uses make-test-python.nix directly. Packages defined in Nixpkgs reuse NixOS tests via nixosTests, plural.\n\nIt is mostly equivalent to the function import ./make-test-python.nix from the NixOS manual, except that the current application of Nixpkgs (pkgs) will be used, instead of letting NixOS invoke Nixpkgs anew.\n\nIf a test machine needs to set NixOS options under nixpkgs, it must set only the nixpkgs.pkgs option.\n\nParameter \n\nA NixOS VM test network, or path to it. Example:\n\n{\n  name = \"my-test\";\n  nodes = {\n    machine1 = { lib, pkgs, nodes, ... }: {\n      environment.systemPackages = [ pkgs.hello ];\n      services.foo.enable = true;\n    };\n    # machine2 = ...;\n  };\n  testScript = ''\n    start_all()\n    machine1.wait_for_unit(\"foo.service\")\n    machine1.succeed(\"hello | foo-send\")\n  '';\n}\n\nResult \n\nA derivation that runs the VM test.\n\nNotable attributes:\n\nnodes: the evaluated NixOS configurations. Useful for debugging and exploring the configuration.\n\ndriverInteractive: a script that launches an interactive Python session in the context of the testScript.\n\nSpecial build helpers \n\nTable of Contents\n\nbuildFHSEnv\npkgs.makeSetupHook\npkgs.mkShell\nvmTools\n\nThis chapter describes several special build helpers.\n\nbuildFHSEnv \n\nbuildFHSEnv provides a way to build and run FHS-compatible lightweight sandboxes. It creates an isolated root filesystem with the host’s /nix/store, so its footprint in terms of disk space is quite small. This allows you to run software which is hard or unfeasible to patch for NixOS; 3rd-party source trees with FHS assumptions, games distributed as tarballs, software with integrity checking and/or external self-updated binaries for instance. It uses Linux’ namespaces feature to create temporary lightweight environments which are destroyed after all child processes exit, without requiring elevated privileges. It works similar to containerisation technology such as Docker or FlatPak but provides no security-relevant separation from the host system.\n\nAccepted arguments are:\n\nname The name of the environment and the wrapper executable.\n\ntargetPkgs Packages to be installed for the main host’s architecture (i.e. x86_64 on x86_64 installations). Along with libraries binaries are also installed.\n\nmultiPkgs Packages to be installed for all architectures supported by a host (i.e. i686 and x86_64 on x86_64 installations). Only libraries are installed by default.\n\nmultiArch Whether to install 32bit multiPkgs into the FHSEnv in 64bit environments\n\nextraBuildCommands Additional commands to be executed for finalizing the directory structure.\n\nextraBuildCommandsMulti Like extraBuildCommands, but executed only on multilib architectures.\n\nextraOutputsToInstall Additional derivation outputs to be linked for both target and multi-architecture packages.\n\nextraInstallCommands Additional commands to be executed for finalizing the derivation with runner script.\n\nrunScript A shell command to be executed inside the sandbox. It defaults to bash. Command line arguments passed to the resulting wrapper are appended to this command by default. This command must be escaped; i.e. \"foo app\" --do-stuff --with \"some file\". See lib.escapeShellArgs.\n\nprofile Optional script for /etc/profile within the sandbox.\n\nYou can create a simple environment using a shell.nix like this:\n\n{ pkgs ? import <nixpkgs> {} }:\n\n(pkgs.buildFHSEnv {\n  name = \"simple-x11-env\";\n  targetPkgs = pkgs: (with pkgs; [\n    udev\n    alsa-lib\n  ]) ++ (with pkgs.xorg; [\n    libX11\n    libXcursor\n    libXrandr\n  ]);\n  multiPkgs = pkgs: (with pkgs; [\n    udev\n    alsa-lib\n  ]);\n  runScript = \"bash\";\n}).env\n\n\nRunning nix-shell on it would drop you into a shell inside an FHS env where those libraries and binaries are available in FHS-compliant paths. Applications that expect an FHS structure (i.e. proprietary binaries) can run inside this environment without modification. You can build a wrapper by running your binary in runScript, e.g. ./bin/start.sh. Relative paths work as expected.\n\nAdditionally, the FHS builder links all relocated gsettings-schemas (the glib setup-hook moves them to share/gsettings-schemas/${name}/glib-2.0/schemas) to their standard FHS location. This means you don’t need to wrap binaries with wrapGAppsHook.\n\npkgs.makeSetupHook \nUsage\nAttributes\n\npkgs.makeSetupHook is a build helper that produces hooks that go in to nativeBuildInputs\n\nUsage \npkgs.makeSetupHook {\n  name = \"something-hook\";\n  propagatedBuildInputs = [ pkgs.commandsomething ];\n  depsTargetTargetPropagated = [ pkgs.libsomething ];\n} ./script.sh\n\nsetup hook that depends on the hello package and runs hello and @shell@ is substituted with path to bash\npkgs.makeSetupHook {\n    name = \"run-hello-hook\";\n    propagatedBuildInputs = [ pkgs.hello ];\n    substitutions = { shell = \"${pkgs.bash}/bin/bash\"; };\n    passthru.tests.greeting = callPackage ./test { };\n    meta.platforms = lib.platforms.linux;\n} (writeScript \"run-hello-hook.sh\" ''\n    #!@shell@\n    hello\n'')\n\nAttributes \n\nname Set the name of the hook.\n\npropagatedBuildInputs Runtime dependencies (such as binaries) of the hook.\n\ndepsTargetTargetPropagated Non-binary dependencies.\n\nmeta\n\npassthru\n\nsubstitutions Variables for substituteAll\n\npkgs.mkShell \nUsage\nAttributes\nBuilding the shell\n\npkgs.mkShell is a specialized stdenv.mkDerivation that removes some repetition when using it with nix-shell (or nix develop).\n\nUsage \n\nHere is a common usage example:\n\n{ pkgs ? import <nixpkgs> {} }:\npkgs.mkShell {\n  packages = [ pkgs.gnumake ];\n\n  inputsFrom = [ pkgs.hello pkgs.gnutar ];\n\n  shellHook = ''\n    export DEBUG=1\n  '';\n}\n\nAttributes \n\nname (default: nix-shell). Set the name of the derivation.\n\npackages (default: []). Add executable packages to the nix-shell environment.\n\ninputsFrom (default: []). Add build dependencies of the listed derivations to the nix-shell environment.\n\nshellHook (default: \"\"). Bash statements that are executed by nix-shell.\n\n… all the attributes of stdenv.mkDerivation.\n\nBuilding the shell \n\nThis derivation output will contain a text file that contains a reference to all the build inputs. This is useful in CI where we want to make sure that every derivation, and its dependencies, build properly. Or when creating a GC root so that the build dependencies don’t get garbage-collected.\n\nvmTools \nvmTools.createEmptyImage\nvmTools.runInLinuxVM\nvmTools.extractFs\nvmTools.extractMTDfs\nvmTools.runInLinuxImage\nvmTools.makeImageTestScript\nvmTools.diskImageFuns\nvmTools.diskImageExtraFuns\nvmTools.diskImages\n\nA set of VM related utilities, that help in building some packages in more advanced scenarios.\n\nvmTools.createEmptyImage \n\nA bash script fragment that produces a disk image at destination.\n\nAttributes\n\nsize. The disk size, in MiB.\n\nfullName. Name that will be written to ${destination}/nix-support/full-name.\n\ndestination (optional, default $out). Where to write the image files.\n\nvmTools.runInLinuxVM \n\nRun a derivation in a Linux virtual machine (using Qemu/KVM). By default, there is no disk image; the root filesystem is a tmpfs, and the Nix store is shared with the host (via the 9P protocol). Thus, any pure Nix derivation should run unmodified.\n\nIf the build fails and Nix is run with the -K/--keep-failed option, a script run-vm will be left behind in the temporary build directory that allows you to boot into the VM and debug it interactively.\n\nAttributes\n\npreVM (optional). Shell command to be evaluated before the VM is started (i.e., on the host).\n\nmemSize (optional, default 512). The memory size of the VM in MiB.\n\ndiskImage (optional). A file system image to be attached to /dev/sda. Note that currently we expect the image to contain a filesystem, not a full disk image with a partition table etc.\n\nExamples\n\nBuild the derivation hello inside a VM:\n\n{ pkgs }: with pkgs; with vmTools;\nrunInLinuxVM hello\n\n\nBuild inside a VM with extra memory:\n\n{ pkgs }: with pkgs; with vmTools;\nrunInLinuxVM (hello.overrideAttrs (_: { memSize = 1024; }))\n\n\nUse VM with a disk image (implicitly sets diskImage, see vmTools.createEmptyImage):\n\n{ pkgs }: with pkgs; with vmTools;\nrunInLinuxVM (hello.overrideAttrs (_: {\n  preVM = createEmptyImage {\n    size = 1024;\n    fullName = \"vm-image\";\n  };\n}))\n\nvmTools.extractFs \n\nTakes a file, such as an ISO, and extracts its contents into the store.\n\nAttributes\n\nfile. Path to the file to be extracted. Note that currently we expect the image to contain a filesystem, not a full disk image with a partition table etc.\n\nfs (optional). Filesystem of the contents of the file.\n\nExamples\n\nExtract the contents of an ISO file:\n\n{ pkgs }: with pkgs; with vmTools;\nextractFs { file = ./image.iso; }\n\nvmTools.extractMTDfs \n\nLike the section called “vmTools.extractFs”, but it makes use of a Memory Technology Device (MTD).\n\nvmTools.runInLinuxImage \n\nLike the section called “vmTools.runInLinuxVM”, but instead of using stdenv from the Nix store, run the build using the tools provided by /bin, /usr/bin, etc. from the specified filesystem image, which typically is a filesystem containing a FHS-based Linux distribution.\n\nvmTools.makeImageTestScript \n\nGenerate a script that can be used to run an interactive session in the given image.\n\nExamples\n\nCreate a script for running a Fedora 27 VM:\n\n{ pkgs }: with pkgs; with vmTools;\nmakeImageTestScript diskImages.fedora27x86_64\n\n\nCreate a script for running an Ubuntu 20.04 VM:\n\n{ pkgs }: with pkgs; with vmTools;\nmakeImageTestScript diskImages.ubuntu2004x86_64\n\nvmTools.diskImageFuns \n\nA set of functions that build a predefined set of minimal Linux distributions images.\n\nImages\n\nFedora\n\nfedora26x86_64\n\nfedora27x86_64\n\nCentOS\n\ncentos6i386\n\ncentos6x86_64\n\ncentos7x86_64\n\nUbuntu\n\nubuntu1404i386\n\nubuntu1404x86_64\n\nubuntu1604i386\n\nubuntu1604x86_64\n\nubuntu1804i386\n\nubuntu1804x86_64\n\nubuntu2004i386\n\nubuntu2004x86_64\n\nubuntu2204i386\n\nubuntu2204x86_64\n\nDebian\n\ndebian10i386\n\ndebian10x86_64\n\ndebian11i386\n\ndebian11x86_64\n\nAttributes\n\nsize (optional, defaults to 4096). The size of the image, in MiB.\n\nextraPackages (optional). A list names of additional packages from the distribution that should be included in the image.\n\nExamples\n\n8GiB image containing Firefox in addition to the default packages:\n\n{ pkgs }: with pkgs; with vmTools;\ndiskImageFuns.ubuntu2004x86_64 { extraPackages = [ \"firefox\" ]; size = 8192; }\n\nvmTools.diskImageExtraFuns \n\nShorthand for vmTools.diskImageFuns.<attr> { extraPackages = ... }.\n\nvmTools.diskImages \n\nShorthand for vmTools.diskImageFuns.<attr> { }.\n\nImages \n\nTable of Contents\n\npkgs.appimageTools\npkgs.dockerTools\npkgs.ociTools\npkgs.snapTools\npkgs.portableService\n<nixpkgs/nixos/lib/make-disk-image.nix>\npkgs.mkBinaryCache\n\nThis chapter describes tools for creating various types of images.\n\npkgs.appimageTools \nAppImage formats\nWrapping\n\npkgs.appimageTools is a set of functions for extracting and wrapping AppImage files. They are meant to be used if traditional packaging from source is infeasible, or it would take too long. To quickly run an AppImage file, pkgs.appimage-run can be used as well.\n\nWarning\n\nThe appimageTools API is unstable and may be subject to backwards-incompatible changes in the future.\n\nAppImage formats \n\nThere are different formats for AppImages, see the specification for details.\n\nType 1 images are ISO 9660 files that are also ELF executables.\n\nType 2 images are ELF executables with an appended filesystem.\n\nThey can be told apart with file -k:\n\n$ file -k type1.AppImage\ntype1.AppImage: ELF 64-bit LSB executable, x86-64, version 1 (SYSV) ISO 9660 CD-ROM filesystem data 'AppImage' (Lepton 3.x), scale 0-0,\nspot sensor temperature 0.000000, unit celsius, color scheme 0, calibration: offset 0.000000, slope 0.000000, dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.18, BuildID[sha1]=d629f6099d2344ad82818172add1d38c5e11bc6d, stripped\\012- data\n\n$ file -k type2.AppImage\ntype2.AppImage: ELF 64-bit LSB executable, x86-64, version 1 (SYSV) (Lepton 3.x), scale 232-60668, spot sensor temperature -4.187500, color scheme 15, show scale bar, calibration: offset -0.000000, slope 0.000000 (Lepton 2.x), scale 4111-45000, spot sensor temperature 412442.250000, color scheme 3, minimum point enabled, calibration: offset -75402534979642766821519867692934234112.000000, slope 5815371847733706829839455140374904832.000000, dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.18, BuildID[sha1]=79dcc4e55a61c293c5e19edbd8d65b202842579f, stripped\\012- data\n\n\nNote how the type 1 AppImage is described as an ISO 9660 CD-ROM filesystem, and the type 2 AppImage is not.\n\nWrapping \n\nDepending on the type of AppImage you’re wrapping, you’ll have to use wrapType1 or wrapType2.\n\nappimageTools.wrapType2 { # or wrapType1\n  name = \"patchwork\";\n  src = fetchurl {\n    url = \"https://github.com/ssbc/patchwork/releases/download/v3.11.4/Patchwork-3.11.4-linux-x86_64.AppImage\";\n    hash = \"sha256-OqTitCeZ6xmWbqYTXp8sDrmVgTNjPZNW0hzUPW++mq4=\";\n  };\n  extraPkgs = pkgs: with pkgs; [ ];\n}\n\n\nname specifies the name of the resulting image.\n\nsrc specifies the AppImage file to extract.\n\nextraPkgs allows you to pass a function to include additional packages inside the FHS environment your AppImage is going to run in. There are a few ways to learn which dependencies an application needs:\n\nLooking through the extracted AppImage files, reading its scripts and running patchelf and ldd on its executables. This can also be done in appimage-run, by setting APPIMAGE_DEBUG_EXEC=bash.\n\nRunning strace -vfefile on the wrapped executable, looking for libraries that can’t be found.\n\npkgs.dockerTools \nbuildImage\nbuildLayeredImage\nstreamLayeredImage\npullImage\nexportImage\nEnvironment Helpers\nfakeNss\nbuildNixShellImage\n\npkgs.dockerTools is a set of functions for creating and manipulating Docker images according to the Docker Image Specification v1.2.0. Docker itself is not used to perform any of the operations done by these functions.\n\nbuildImage \n\nThis function is analogous to the docker build command, in that it can be used to build a Docker-compatible repository tarball containing a single image with one or multiple layers. As such, the result is suitable for being loaded in Docker with docker load.\n\nThe parameters of buildImage with relative example values are described below:\n\nbuildImage {\n  name = \"redis\";\n  tag = \"latest\";\n\n  fromImage = someBaseImage;\n  fromImageName = null;\n  fromImageTag = \"latest\";\n\n  copyToRoot = pkgs.buildEnv {\n    name = \"image-root\";\n    paths = [ pkgs.redis ];\n    pathsToLink = [ \"/bin\" ];\n  };\n\n  runAsRoot = ''\n    #!${pkgs.runtimeShell}\n    mkdir -p /data\n  '';\n\n  config = {\n    Cmd = [ \"/bin/redis-server\" ];\n    WorkingDir = \"/data\";\n    Volumes = { \"/data\" = { }; };\n  };\n\n  diskSize = 1024;\n  buildVMMemorySize = 512;\n}\n\n\nThe above example will build a Docker image redis/latest from the given base image. Loading and running this image in Docker results in redis-server being started automatically.\n\nname specifies the name of the resulting image. This is the only required argument for buildImage.\n\ntag specifies the tag of the resulting image. By default it’s null, which indicates that the nix output hash will be used as tag.\n\nfromImage is the repository tarball containing the base image. It must be a valid Docker image, such as exported by docker save. By default it’s null, which can be seen as equivalent to FROM scratch of a Dockerfile.\n\nfromImageName can be used to further specify the base image within the repository, in case it contains multiple images. By default it’s null, in which case buildImage will peek the first image available in the repository.\n\nfromImageTag can be used to further specify the tag of the base image within the repository, in case an image contains multiple tags. By default it’s null, in which case buildImage will peek the first tag available for the base image.\n\ncopyToRoot is a derivation that will be copied in the new layer of the resulting image. This can be similarly seen as ADD contents/ / in a Dockerfile. By default it’s null.\n\nrunAsRoot is a bash script that will run as root in an environment that overlays the existing layers of the base image with the new resulting layer, including the previously copied contents derivation. This can be similarly seen as RUN ... in a Dockerfile.\n\nNOTE: Using this parameter requires the kvm device to be available.\n\nconfig is used to specify the configuration of the containers that will be started off the built image in Docker. The available options are listed in the Docker Image Specification v1.2.0.\n\narchitecture is optional and used to specify the image architecture, this is useful for multi-architecture builds that don’t need cross compiling. If not specified it will default to hostPlatform.\n\ndiskSize is used to specify the disk size of the VM used to build the image in megabytes. By default it’s 1024 MiB.\n\nbuildVMMemorySize is used to specify the memory size of the VM to build the image in megabytes. By default it’s 512 MiB.\n\nAfter the new layer has been created, its closure (to which contents, config and runAsRoot contribute) will be copied in the layer itself. Only new dependencies that are not already in the existing layers will be copied.\n\nAt the end of the process, only one new single layer will be produced and added to the resulting image.\n\nThe resulting repository will only list the single image image/tag. In the case of the buildImage example, it would be redis/latest.\n\nIt is possible to inspect the arguments with which an image was built using its buildArgs attribute.\n\nNOTE: If you see errors similar to getProtocolByName: does not exist (no such protocol name: tcp) you may need to add pkgs.iana-etc to contents.\n\nNOTE: If you see errors similar to Error_Protocol (\"certificate has unknown CA\",True,UnknownCa) you may need to add pkgs.cacert to contents.\n\nBy default buildImage will use a static date of one second past the UNIX Epoch. This allows buildImage to produce binary reproducible images. When listing images with docker images, the newly created images will be listed like this:\n\n$ docker images\nREPOSITORY   TAG      IMAGE ID       CREATED        SIZE\nhello        latest   08c791c7846e   48 years ago   25.2MB\n\n\nYou can break binary reproducibility but have a sorted, meaningful CREATED column by setting created to now.\n\npkgs.dockerTools.buildImage {\n  name = \"hello\";\n  tag = \"latest\";\n  created = \"now\";\n  copyToRoot = pkgs.buildEnv {\n    name = \"image-root\";\n    paths = [ pkgs.hello ];\n    pathsToLink = [ \"/bin\" ];\n  };\n\n  config.Cmd = [ \"/bin/hello\" ];\n}\n\n\nNow the Docker CLI will display a reasonable date and sort the images as expected:\n\n$ docker images\nREPOSITORY   TAG      IMAGE ID       CREATED              SIZE\nhello        latest   de2bf4786de6   About a minute ago   25.2MB\n\n\nHowever, the produced images will not be binary reproducible.\n\nbuildLayeredImage \n\nCreate a Docker image with many of the store paths being on their own layer to improve sharing between images. The image is realized into the Nix store as a gzipped tarball. Depending on the intended usage, many users might prefer to use streamLayeredImage instead, which this function uses internally.\n\nname\n\nThe name of the resulting image.\n\ntag optional\n\nTag of the generated image.\n\nDefault: the output path’s hash\n\nfromImage optional\n\nThe repository tarball containing the base image. It must be a valid Docker image, such as one exported by docker save.\n\nDefault: null, which can be seen as equivalent to FROM scratch of a Dockerfile.\n\ncontents optional\n\nTop-level paths in the container. Either a single derivation, or a list of derivations.\n\nDefault: []\n\nconfig optional\n\narchitecture is optional and used to specify the image architecture, this is useful for multi-architecture builds that don’t need cross compiling. If not specified it will default to hostPlatform.\n\nRun-time configuration of the container. A full list of the options available is in the Docker Image Specification v1.2.0.\n\nDefault: {}\n\ncreated optional\n\nDate and time the layers were created. Follows the same now exception supported by buildImage.\n\nDefault: 1970-01-01T00:00:01Z\n\nmaxLayers optional\n\nMaximum number of layers to create.\n\nDefault: 100\n\nMaximum: 125\n\nextraCommands optional\n\nShell commands to run while building the final layer, without access to most of the layer contents. Changes to this layer are “on top” of all the other layers, so can create additional directories and files.\n\nfakeRootCommands optional\n\nShell commands to run while creating the archive for the final layer in a fakeroot environment. Unlike extraCommands, you can run chown to change the owners of the files in the archive, changing fakeroot’s state instead of the real filesystem. The latter would require privileges that the build user does not have. Static binaries do not interact with the fakeroot environment. By default all files in the archive will be owned by root.\n\nenableFakechroot optional\n\nWhether to run in fakeRootCommands in fakechroot, making programs behave as though / is the root of the image being created, while files in the Nix store are available as usual. This allows scripts that perform installation in / to work as expected. Considering that fakechroot is implemented via the same mechanism as fakeroot, the same caveats apply.\n\nDefault: false\n\nBehavior of contents in the final image\n\nEach path directly listed in contents will have a symlink in the root of the image.\n\nFor example:\n\npkgs.dockerTools.buildLayeredImage {\n  name = \"hello\";\n  contents = [ pkgs.hello ];\n}\n\n\nwill create symlinks for all the paths in the hello package:\n\n/bin/hello -> /nix/store/h1zb1padqbbb7jicsvkmrym3r6snphxg-hello-2.10/bin/hello\n/share/info/hello.info -> /nix/store/h1zb1padqbbb7jicsvkmrym3r6snphxg-hello-2.10/share/info/hello.info\n/share/locale/bg/LC_MESSAGES/hello.mo -> /nix/store/h1zb1padqbbb7jicsvkmrym3r6snphxg-hello-2.10/share/locale/bg/LC_MESSAGES/hello.mo\n\nAutomatic inclusion of config references\n\nThe closure of config is automatically included in the closure of the final image.\n\nThis allows you to make very simple Docker images with very little code. This container will start up and run hello:\n\npkgs.dockerTools.buildLayeredImage {\n  name = \"hello\";\n  config.Cmd = [ \"${pkgs.hello}/bin/hello\" ];\n}\n\nAdjusting maxLayers\n\nIncreasing the maxLayers increases the number of layers which have a chance to be shared between different images.\n\nModern Docker installations support up to 128 layers, but older versions support as few as 42.\n\nIf the produced image will not be extended by other Docker builds, it is safe to set maxLayers to 128. However, it will be impossible to extend the image further.\n\nThe first (maxLayers-2) most “popular” paths will have their own individual layers, then layer #maxLayers-1 will contain all the remaining “unpopular” paths, and finally layer #maxLayers will contain the Image configuration.\n\nDocker’s Layers are not inherently ordered, they are content-addressable and are not explicitly layered until they are composed in to an Image.\n\nstreamLayeredImage \n\nBuilds a script which, when run, will stream an uncompressed tarball of a Docker image to stdout. The arguments to this function are as for buildLayeredImage. This method of constructing an image does not realize the image into the Nix store, so it saves on IO and disk/cache space, particularly with large images.\n\nThe image produced by running the output script can be piped directly into docker load, to load it into the local docker daemon:\n\n$(nix-build) | docker load\n\n\nAlternatively, the image be piped via gzip into skopeo, e.g., to copy it into a registry:\n\n$(nix-build) | gzip --fast | skopeo copy docker-archive:/dev/stdin docker://some_docker_registry/myimage:tag\n\npullImage \n\nThis function is analogous to the docker pull command, in that it can be used to pull a Docker image from a Docker registry. By default Docker Hub is used to pull images.\n\nIts parameters are described in the example below:\n\npullImage {\n  imageName = \"nixos/nix\";\n  imageDigest =\n    \"sha256:473a2b527958665554806aea24d0131bacec46d23af09fef4598eeab331850fa\";\n  finalImageName = \"nix\";\n  finalImageTag = \"2.11.1\";\n  sha256 = \"sha256-qvhj+Hlmviz+KEBVmsyPIzTB3QlVAFzwAY1zDPIBGxc=\";\n  os = \"linux\";\n  arch = \"x86_64\";\n}\n\n\nimageName specifies the name of the image to be downloaded, which can also include the registry namespace (e.g. nixos). This argument is required.\n\nimageDigest specifies the digest of the image to be downloaded. This argument is required.\n\nfinalImageName, if specified, this is the name of the image to be created. Note it is never used to fetch the image since we prefer to rely on the immutable digest ID. By default it’s equal to imageName.\n\nfinalImageTag, if specified, this is the tag of the image to be created. Note it is never used to fetch the image since we prefer to rely on the immutable digest ID. By default it’s latest.\n\nsha256 is the checksum of the whole fetched image. This argument is required.\n\nos, if specified, is the operating system of the fetched image. By default it’s linux.\n\narch, if specified, is the cpu architecture of the fetched image. By default it’s x86_64.\n\nnix-prefetch-docker command can be used to get required image parameters:\n\n$ nix run nixpkgs#nix-prefetch-docker -- --image-name mysql --image-tag 5\n\n\nSince a given imageName may transparently refer to a manifest list of images which support multiple architectures and/or operating systems, you can supply the --os and --arch arguments to specify exactly which image you want. By default it will match the OS and architecture of the host the command is run on.\n\n$ nix-prefetch-docker --image-name mysql --image-tag 5 --arch x86_64 --os linux\n\n\nDesired image name and tag can be set using --final-image-name and --final-image-tag arguments:\n\n$ nix-prefetch-docker --image-name mysql --image-tag 5 --final-image-name eu.gcr.io/my-project/mysql --final-image-tag prod\n\nexportImage \n\nThis function is analogous to the docker export command, in that it can be used to flatten a Docker image that contains multiple layers. It is in fact the result of the merge of all the layers of the image. As such, the result is suitable for being imported in Docker with docker import.\n\nNOTE: Using this function requires the kvm device to be available.\n\nThe parameters of exportImage are the following:\n\nexportImage {\n  fromImage = someLayeredImage;\n  fromImageName = null;\n  fromImageTag = null;\n\n  name = someLayeredImage.name;\n}\n\n\nThe parameters relative to the base image have the same synopsis as described in buildImage, except that fromImage is the only required argument in this case.\n\nThe name argument is the name of the derivation output, which defaults to fromImage.name.\n\nEnvironment Helpers \n\nSome packages expect certain files to be available globally. When building an image from scratch (i.e. without fromImage), these files are missing. pkgs.dockerTools provides some helpers to set up an environment with the necessary files. You can include them in copyToRoot like this:\n\nbuildImage {\n  name = \"environment-example\";\n  copyToRoot = with pkgs.dockerTools; [\n    usrBinEnv\n    binSh\n    caCertificates\n    fakeNss\n  ];\n}\n\nusrBinEnv\n\nThis provides the env utility at /usr/bin/env.\n\nbinSh\n\nThis provides bashInteractive at /bin/sh.\n\ncaCertificates\n\nThis sets up /etc/ssl/certs/ca-certificates.crt.\n\nfakeNss\n\nProvides /etc/passwd and /etc/group that contain root and nobody. Useful when packaging binaries that insist on using nss to look up username/groups (like nginx).\n\nshadowSetup\n\nThis constant string is a helper for setting up the base files for managing users and groups, only if such files don’t exist already. It is suitable for being used in a buildImage runAsRoot script for cases like in the example below:\n\nbuildImage {\n  name = \"shadow-basic\";\n\n  runAsRoot = ''\n    #!${pkgs.runtimeShell}\n    ${pkgs.dockerTools.shadowSetup}\n    groupadd -r redis\n    useradd -r -g redis redis\n    mkdir /data\n    chown redis:redis /data\n  '';\n}\n\n\nCreating base files like /etc/passwd or /etc/login.defs is necessary for shadow-utils to manipulate users and groups.\n\nfakeNss \n\nIf your primary goal is providing a basic skeleton for user lookups to work, and/or a lesser privileged user, adding pkgs.fakeNss to the container image root might be the better choice than a custom script running useradd and friends.\n\nIt provides a /etc/passwd and /etc/group, containing root and nobody users and groups.\n\nIt also provides a /etc/nsswitch.conf, configuring NSS host resolution to first check /etc/hosts, before checking DNS, as the default in the absence of a config file (dns [!UNAVAIL=return] files) is quite unexpected.\n\nYou can pair it with binSh, which provides bin/sh as a symlink to bashInteractive (as /bin/sh is configured as a shell).\n\nbuildImage {\n  name = \"shadow-basic\";\n\n  copyToRoot = pkgs.buildEnv {\n    name = \"image-root\";\n    paths = [ binSh pkgs.fakeNss ];\n    pathsToLink = [ \"/bin\" \"/etc\" \"/var\" ];\n  };\n}\n\nbuildNixShellImage \n\nCreate a Docker image that sets up an environment similar to that of running nix-shell on a derivation. When run in Docker, this environment somewhat resembles the Nix sandbox typically used by nix-build, with a major difference being that access to the internet is allowed. It additionally also behaves like an interactive nix-shell, running things like shellHook and setting an interactive prompt. If the derivation is fully buildable (i.e. nix-build can be used on it), running buildDerivation inside such a Docker image will build the derivation, with all its outputs being available in the correct /nix/store paths, pointed to by the respective environment variables like $out, etc.\n\nWarning\n\nThe behavior doesn’t match nix-shell or nix-build exactly and this function is known not to work correctly for e.g. fixed-output derivations, content-addressed derivations, impure derivations and other special types of derivations.\n\nArguments\ndrv\n\nThe derivation on which to base the Docker image.\n\nAdding packages to the Docker image is possible by e.g. extending the list of nativeBuildInputs of this derivation like\n\nbuildNixShellImage {\n  drv = someDrv.overrideAttrs (old: {\n    nativeBuildInputs = old.nativeBuildInputs or [] ++ [\n      somethingExtra\n    ];\n  });\n  # ...\n}\n\n\nSimilarly, you can extend the image initialization script by extending shellHook\n\nname optional\n\nThe name of the resulting image.\n\nDefault: drv.name + \"-env\"\n\ntag optional\n\nTag of the generated image.\n\nDefault: the resulting image derivation output path’s hash\n\nuid/gid optional\n\nThe user/group ID to run the container as. This is like a nixbld build user.\n\nDefault: 1000/1000\n\nhomeDirectory optional\n\nThe home directory of the user the container is running as\n\nDefault: /build\n\nshell optional\n\nThe path to the bash binary to use as the shell. This shell is started when running the image.\n\nDefault: pkgs.bashInteractive + \"/bin/bash\"\n\ncommand optional\n\nRun this command in the environment of the derivation, in an interactive shell. See the --command option in the nix-shell documentation.\n\nDefault: (none)\n\nrun optional\n\nSame as command, but runs the command in a non-interactive shell instead. See the --run option in the nix-shell documentation.\n\nDefault: (none)\n\nExample\n\nThe following shows how to build the pkgs.hello package inside a Docker container built with buildNixShellImage.\n\nwith import <nixpkgs> {};\ndockerTools.buildNixShellImage {\n  drv = hello;\n}\n\n\nBuild the derivation:\n\nnix-build hello.nix\n\nthese 8 derivations will be built:\n  /nix/store/xmw3a5ln29rdalavcxk1w3m4zb2n7kk6-nix-shell-rc.drv\n...\nCreating layer 56 from paths: ['/nix/store/crpnj8ssz0va2q0p5ibv9i6k6n52gcya-stdenv-linux']\nCreating layer 57 with customisation...\nAdding manifests...\nDone.\n/nix/store/cpyn1lc897ghx0rhr2xy49jvyn52bazv-hello-2.12-env.tar.gz\n\n\nLoad the image:\n\ndocker load -i result\n\n0d9f4c4cd109: Loading layer [==================================================>]   2.56MB/2.56MB\n...\nab1d897c0697: Loading layer [==================================================>]  10.24kB/10.24kB\nLoaded image: hello-2.12-env:pgj9h98nal555415faa43vsydg161bdz\n\n\nRun the container:\n\ndocker run -it hello-2.12-env:pgj9h98nal555415faa43vsydg161bdz\n\n[nix-shell:/build]$\n\n\nIn the running container, run the build:\n\nbuildDerivation\n\nunpacking sources\nunpacking source archive /nix/store/8nqv6kshb3vs5q5bs2k600xpj5bkavkc-hello-2.12.tar.gz\n...\npatching script interpreter paths in /nix/store/z5wwy5nagzy15gag42vv61c2agdpz2f2-hello-2.12\nchecking for references to /build/ in /nix/store/z5wwy5nagzy15gag42vv61c2agdpz2f2-hello-2.12...\n\n\nCheck the build result:\n\n$out/bin/hello\n\nHello, world!\n\npkgs.ociTools \nbuildContainer\n\npkgs.ociTools is a set of functions for creating containers according to the OCI container specification v1.0.0. Beyond that, it makes no assumptions about the container runner you choose to use to run the created container.\n\nbuildContainer \n\nThis function creates a simple OCI container that runs a single command inside of it. An OCI container consists of a config.json and a rootfs directory. The nix store of the container will contain all referenced dependencies of the given command.\n\nThe parameters of buildContainer with an example value are described below:\n\nbuildContainer {\n  args = [\n    (with pkgs;\n      writeScript \"run.sh\" ''\n        #!${bash}/bin/bash\n        exec ${bash}/bin/bash\n      '').outPath\n  ];\n\n  mounts = {\n    \"/data\" = {\n      type = \"none\";\n      source = \"/var/lib/mydata\";\n      options = [ \"bind\" ];\n    };\n  };\n\n  readonly = false;\n}\n\n\nargs specifies a set of arguments to run inside the container. This is the only required argument for buildContainer. All referenced packages inside the derivation will be made available inside the container.\n\nmounts specifies additional mount points chosen by the user. By default only a minimal set of necessary filesystems are mounted into the container (e.g procfs, cgroupfs)\n\nreadonly makes the container’s rootfs read-only if it is set to true. The default value is false false.\n\npkgs.snapTools \nThe makeSnap Function\nBuild a Hello World Snap\nBuild a Graphical Snap\n\npkgs.snapTools is a set of functions for creating Snapcraft images. Snap and Snapcraft is not used to perform these operations.\n\nThe makeSnap Function \n\nmakeSnap takes a single named argument, meta. This argument mirrors the upstream snap.yaml format exactly.\n\nThe base should not be specified, as makeSnap will force set it.\n\nCurrently, makeSnap does not support creating GUI stubs.\n\nBuild a Hello World Snap \n\nThe following expression packages GNU Hello as a Snapcraft snap.\n\nlet\n  inherit (import <nixpkgs> { }) snapTools hello;\nin snapTools.makeSnap {\n  meta = {\n    name = \"hello\";\n    summary = hello.meta.description;\n    description = hello.meta.longDescription;\n    architectures = [ \"amd64\" ];\n    confinement = \"strict\";\n    apps.hello.command = \"${hello}/bin/hello\";\n  };\n}\n\n\nnix-build this expression and install it with snap install ./result --dangerous. hello will now be the Snapcraft version of the package.\n\nBuild a Graphical Snap \n\nGraphical programs require many more integrations with the host. This example uses Firefox as an example because it is one of the most complicated programs we could package.\n\nlet\n  inherit (import <nixpkgs> { }) snapTools firefox;\nin snapTools.makeSnap {\n  meta = {\n    name = \"nix-example-firefox\";\n    summary = firefox.meta.description;\n    architectures = [ \"amd64\" ];\n    apps.nix-example-firefox = {\n      command = \"${firefox}/bin/firefox\";\n      plugs = [\n        \"pulseaudio\"\n        \"camera\"\n        \"browser-support\"\n        \"avahi-observe\"\n        \"cups-control\"\n        \"desktop\"\n        \"desktop-legacy\"\n        \"gsettings\"\n        \"home\"\n        \"network\"\n        \"mount-observe\"\n        \"removable-media\"\n        \"x11\"\n      ];\n    };\n    confinement = \"strict\";\n  };\n}\n\n\nnix-build this expression and install it with snap install ./result --dangerous. nix-example-firefox will now be the Snapcraft version of the Firefox package.\n\nThe specific meaning behind plugs can be looked up in the Snapcraft interface documentation.\n\npkgs.portableService \n\npkgs.portableService is a function to create portable service images, as read-only, immutable, squashfs archives.\n\nsystemd supports a concept of Portable Services. Portable Services are a delivery method for system services that uses two specific features of container management:\n\nApplications are bundled. I.e. multiple services, their binaries and all their dependencies are packaged in an image, and are run directly from it.\n\nStricter default security policies, i.e. sandboxing of applications.\n\nThis allows using Nix to build images which can be run on many recent Linux distributions.\n\nThe primary tool for interacting with Portable Services is portablectl, and they are managed by the systemd-portabled system service.\n\nNote\n\nPortable services are supported starting with systemd 239 (released on 2018-06-22).\n\nA very simple example of using portableService is described below:\n\npkgs.portableService {\n  pname = \"demo\";\n  version = \"1.0\";\n  units = [ demo-service demo-socket ];\n}\n\n\nThe above example will build an squashfs archive image in result/$pname_$version.raw. The image will contain the file system structure as required by the portable service specification, and a subset of the Nix store with all the dependencies of the two derivations in the units list. units must be a list of derivations, and their names must be prefixed with the service name (\"demo\" in this case). Otherwise systemd-portabled will ignore them.\n\nNote\n\nThe .raw file extension of the image is required by the portable services specification.\n\nSome other options available are:\n\ndescription, homepage\n\nAre added to the /etc/os-release in the image and are shown by the portable services tooling. Default to empty values, not added to os-release.\n\nsymlinks\n\nA list of attribute sets {object, symlink}. Symlinks will be created in the root filesystem of the image to objects in the Nix store. Defaults to an empty list.\n\ncontents\n\nA list of additional derivations to be included in the image Nix store, as-is. Defaults to an empty list.\n\nsquashfsTools\n\nDefaults to pkgs.squashfsTools, allows you to override the package that provides mksquashfs.\n\nsquash-compression, squash-block-size\n\nOptions to mksquashfs. Default to \"xz -Xdict-size 100%\" and \"1M\" respectively.\n\nA typical usage of symlinks would be:\n\n  symlinks = [\n    { object = \"${pkgs.cacert}/etc/ssl\"; symlink = \"/etc/ssl\"; }\n    { object = \"${pkgs.bash}/bin/bash\"; symlink = \"/bin/sh\"; }\n    { object = \"${pkgs.php}/bin/php\"; symlink = \"/usr/bin/php\"; }\n  ];\n\n\nto create these symlinks for legacy applications that assume them existing globally.\n\nOnce the image is created, and deployed on a host in /var/lib/portables/, you can attach the image and run the service. As root run:\n\nportablectl attach demo_1.0.raw\nsystemctl enable --now demo.socket\nsystemctl enable --now demo.service\n\nNote\n\nSee the man page of portablectl for more info on its usage.\n\n<nixpkgs/nixos/lib/make-disk-image.nix> \nFeatures\nUsage\n\n<nixpkgs/nixos/lib/make-disk-image.nix> is a function to create disk images in multiple formats: raw, QCOW2 (QEMU), QCOW2-Compressed (compressed version), VDI (VirtualBox), VPC (VirtualPC).\n\nThis function can create images in two ways:\n\nusing cptofs without any virtual machine to create a Nix store disk image,\n\nusing a virtual machine to create a full NixOS installation.\n\nWhen testing early-boot or lifecycle parts of NixOS such as a bootloader or multiple generations, it is necessary to opt for a full NixOS system installation. Whereas for many web servers, applications, it is possible to work with a Nix store only disk image and is faster to build.\n\nNixOS tests also use this function when preparing the VM. The cptofs method is used when virtualisation.useBootLoader is false (the default). Otherwise the second method is used.\n\nFeatures \n\nFor reference, read the function signature source code for documentation on arguments: https://github.com/NixOS/nixpkgs/blob/master/nixos/lib/make-disk-image.nix. Features are separated in various sections depending on if you opt for a Nix-store only image or a full NixOS image.\n\nCommon\n\narbitrary NixOS configuration\n\nautomatic or bound disk size: diskSize parameter, additionalSpace can be set when diskSize is auto to add a constant of disk space\n\nmultiple partition table layouts: EFI, legacy, legacy + GPT, hybrid, none through partitionTableType parameter\n\nOVMF or EFI firmwares and variables templates can be customized\n\nroot filesystem fsType can be customized to whatever mkfs.${fsType} exist during operations\n\nroot filesystem label can be customized, defaults to nix-store if it’s a Nix store image, otherwise nixpkgs/nixos\n\narbitrary code can be executed after disk image was produced with postVM\n\nthe current nixpkgs can be realized as a channel in the disk image, which will change the hash of the image when the sources are updated\n\nadditional store paths can be provided through additionalPaths\n\nFull NixOS image\n\narbitrary contents with permissions can be placed in the target filesystem using contents\n\na /etc/nixpkgs/nixos/configuration.nix can be provided through configFile\n\nbootloaders are supported\n\nEFI variables can be mutated during image production and the result is exposed in $out\n\nboot partition size when partition table is efi or hybrid\n\nOn bit-to-bit reproducibility\n\nImages are NOT deterministic, please do not hesitate to try to fix this, source of determinisms are (not exhaustive) :\n\nbootloader installation have timestamps\n\nSQLite Nix store database contain registration times\n\n/etc/shadow is in a non-deterministic order\n\nA deterministic flag is available for best efforts determinism.\n\nUsage \n\nTo produce a Nix-store only image:\n\nlet\n  pkgs = import <nixpkgs> {};\n  lib = pkgs.lib;\n  make-disk-image = import <nixpkgs/nixos/lib/make-disk-image.nix>;\nin\n  make-disk-image {\n    inherit pkgs lib;\n    config = {};\n    additionalPaths = [ ];\n    format = \"qcow2\";\n    onlyNixStore = true;\n    partitionTableType = \"none\";\n    installBootLoader = false;\n    touchEFIVars = false;\n    diskSize = \"auto\";\n    additionalSpace = \"0M\"; # Defaults to 512M.\n    copyChannel = false;\n  }\n\n\nSome arguments can be left out, they are shown explicitly for the sake of the example.\n\nBuilding this derivation will provide a QCOW2 disk image containing only the Nix store and its registration information.\n\nTo produce a NixOS installation image disk with UEFI and bootloader installed:\n\nlet\n  pkgs = import <nixpkgs> {};\n  lib = pkgs.lib;\n  make-disk-image = import <nixpkgs/nixos/lib/make-disk-image.nix>;\n  evalConfig = import <nixpkgs/nixos/lib/eval-config.nix>;\nin\n  make-disk-image {\n    inherit pkgs lib;\n    config = evalConfig {\n      modules = [\n        {\n          fileSystems.\"/\" = { device = \"/dev/vda\"; fsType = \"ext4\"; autoFormat = true; };\n          boot.grub.device = \"/dev/vda\";\n        }\n      ];\n    };\n    format = \"qcow2\";\n    onlyNixStore = false;\n    partitionTableType = \"legacy+gpt\";\n    installBootLoader = true;\n    touchEFIVars = true;\n    diskSize = \"auto\";\n    additionalSpace = \"0M\"; # Defaults to 512M.\n    copyChannel = false;\n    memSize = 2048; # Qemu VM memory size in megabytes. Defaults to 1024M.\n  }\n\npkgs.mkBinaryCache \nExample\n\npkgs.mkBinaryCache is a function for creating Nix flat-file binary caches. Such a cache exists as a directory on disk, and can be used as a Nix substituter by passing --substituter file:///path/to/cache to Nix commands.\n\nNix packages are most commonly shared between machines using HTTP, SSH, or S3, but a flat-file binary cache can still be useful in some situations. For example, you can copy it directly to another machine, or make it available on a network file system. It can also be a convenient way to make some Nix packages available inside a container via bind-mounting.\n\nNote that this function is meant for advanced use-cases. The more idiomatic way to work with flat-file binary caches is via the nix-copy-closure command. You may also want to consider dockerTools for your containerization needs.\n\nExample \n\nThe following derivation will construct a flat-file binary cache containing the closure of hello.\n\nmkBinaryCache {\n  rootPaths = [hello];\n}\n\n\nrootPaths specifies a list of root derivations. The transitive closure of these derivations’ outputs will be copied into the cache.\n\nHere’s an example of building and using the cache.\n\nBuild the cache on one machine, host1:\n\nnix-build -E 'with import <nixpkgs> {}; mkBinaryCache { rootPaths = [hello]; }'\n\n/nix/store/cc0562q828rnjqjyfj23d5q162gb424g-binary-cache\n\n\nCopy the resulting directory to the other machine, host2:\n\nscp result host2:/tmp/hello-cache\n\n\nSubstitute the derivation using the flat-file binary cache on the other machine, host2:\n\nnix-build -A hello '<nixpkgs>' \\\n  --option require-sigs false \\\n  --option trusted-substituters file:///tmp/hello-cache \\\n  --option substituters file:///tmp/hello-cache\n\n/nix/store/gl5a41azbpsadfkfmbilh9yk40dh5dl0-hello-2.12.1\n\nHooks reference \n\nTable of Contents\n\nAutoconf\nAutomake\nautoPatchelfHook\nbmake\nbreakpointHook\ncmake\ngdk-pixbuf\nGHC\nGNOME platform\ninstallShellFiles\nlibiconv, libintl\nlibxml2\nMeson\nmpiCheckPhaseHook\nninja\npatchRcPath hooks\nPerl\npkg-config\npostgresqlTestHook\nPython\nQt 4\nscons\nteTeX / TeX Live\nunzip\nvalidatePkgConfig\nwafHook\nzig.hook\nxcbuildHook\n\nNixpkgs has several hook packages that augment the stdenv phases.\n\nThe stdenv built-in hooks are documented in the section called “Package setup hooks”.\n\nAutoconf \n\nThe autoreconfHook derivation adds autoreconfPhase, which runs autoreconf, libtoolize and automake, essentially preparing the configure script in autotools-based builds. Most autotools-based packages come with the configure script pre-generated, but this hook is necessary for a few packages and when you need to patch the package’s configure scripts.\n\nAutomake \n\nAdds the share/aclocal subdirectory of each build input to the ACLOCAL_PATH environment variable.\n\nautoPatchelfHook \n\nThis is a special setup hook which helps in packaging proprietary software in that it automatically tries to find missing shared library dependencies of ELF files based on the given buildInputs and nativeBuildInputs.\n\nYou can also specify a runtimeDependencies variable which lists dependencies to be unconditionally added to rpath of all executables. This is useful for programs that use dlopen 3 to load libraries at runtime.\n\nIn certain situations you may want to run the main command (autoPatchelf) of the setup hook on a file or a set of directories instead of unconditionally patching all outputs. This can be done by setting the dontAutoPatchelf environment variable to a non-empty value.\n\nBy default autoPatchelf will fail as soon as any ELF file requires a dependency which cannot be resolved via the given build inputs. In some situations you might prefer to just leave missing dependencies unpatched and continue to patch the rest. This can be achieved by setting the autoPatchelfIgnoreMissingDeps environment variable to a non-empty value. autoPatchelfIgnoreMissingDeps can be set to a list like autoPatchelfIgnoreMissingDeps = [ \"libcuda.so.1\" \"libcudart.so.1\" ]; or to [ \"*\" ] to ignore all missing dependencies.\n\nThe autoPatchelf command also recognizes a --no-recurse command line flag, which prevents it from recursing into subdirectories.\n\nbmake \n\nbmake is the portable variant of NetBSD make utility.\n\nIn Nixpkgs, bmake comes with a hook that overrides the default build, check, install and dist phases.\n\nbreakpointHook \n\nThis hook will make a build pause instead of stopping when a failure happens. It prevents nix from cleaning up the build environment immediately and allows the user to attach to a build environment using the cntr command. Upon build error it will print instructions on how to use cntr, which can be used to enter the environment for debugging. Installing cntr and running the command will provide shell access to the build sandbox of failed build. At /var/lib/cntr the sandboxed filesystem is mounted. All commands and files of the system are still accessible within the shell. To execute commands from the sandbox use the cntr exec subcommand. cntr is only supported on Linux-based platforms. To use it first add cntr to your environment.systemPackages on NixOS or alternatively to the root user on non-NixOS systems. Then in the package that is supposed to be inspected, add breakpointHook to nativeBuildInputs.\n\nnativeBuildInputs = [ breakpointHook ];\n\n\nWhen a build failure happens there will be an instruction printed that shows how to attach with cntr to the build sandbox.\n\nNote\n\nCaution with remote builds\n\nThis won’t work with remote builds as the build environment is on a different machine and can’t be accessed by cntr. Remote builds can be turned off by setting --option builders '' for nix-build or --builders '' for nix build.\n\ncmake \n\nOverrides the default configure phase to run the CMake command. By default, we use the Make generator of CMake. In addition, dependencies are added automatically to CMAKE_PREFIX_PATH so that packages are correctly detected by CMake. Some additional flags are passed in to give similar behavior to configure-based packages. You can disable this hook’s behavior by setting configurePhase to a custom value, or by setting dontUseCmakeConfigure. cmakeFlags controls flags passed only to CMake. By default, parallel building is enabled as CMake supports parallel building almost everywhere. When Ninja is also in use, CMake will detect that and use the ninja generator.\n\ngdk-pixbuf \n\nExports GDK_PIXBUF_MODULE_FILE environment variable to the builder. Add librsvg package to buildInputs to get svg support. See also the setup hook description in GNOME platform docs.\n\nGHC \n\nCreates a temporary package database and registers every Haskell build input in it (TODO: how?).\n\nGNOME platform \n\nHooks related to GNOME platform and related libraries like GLib, GTK and GStreamer are described in the section called “GNOME”.\n\ninstallShellFiles \n\nThis hook helps with installing manpages and shell completion files. It exposes 2 shell functions installManPage and installShellCompletion that can be used from your postInstall hook.\n\nThe installManPage function takes one or more paths to manpages to install. The manpages must have a section suffix, and may optionally be compressed (with .gz suffix). This function will place them into the correct directory.\n\nThe installShellCompletion function takes one or more paths to shell completion files. By default it will autodetect the shell type from the completion file extension, but you may also specify it by passing one of --bash, --fish, or --zsh. These flags apply to all paths listed after them (up until another shell flag is given). Each path may also have a custom installation name provided by providing a flag --name NAME before the path. If this flag is not provided, zsh completions will be renamed automatically such that foobar.zsh becomes _foobar. A root name may be provided for all paths using the flag --cmd NAME; this synthesizes the appropriate name depending on the shell (e.g. --cmd foo will synthesize the name foo.bash for bash and _foo for zsh). The path may also be a fifo or named fd (such as produced by <(cmd)), in which case the shell and name must be provided.\n\nnativeBuildInputs = [ installShellFiles ];\npostInstall = ''\n  installManPage doc/foobar.1 doc/barfoo.3\n  # explicit behavior\n  installShellCompletion --bash --name foobar.bash share/completions.bash\n  installShellCompletion --fish --name foobar.fish share/completions.fish\n  installShellCompletion --zsh --name _foobar share/completions.zsh\n  # implicit behavior\n  installShellCompletion share/completions/foobar.{bash,fish,zsh}\n  # using named fd\n  installShellCompletion --cmd foobar \\\n    --bash <($out/bin/foobar --bash-completion) \\\n    --fish <($out/bin/foobar --fish-completion) \\\n    --zsh <($out/bin/foobar --zsh-completion)\n'';\n\nlibiconv, libintl \n\nA few libraries automatically add to NIX_LDFLAGS their library, making their symbols automatically available to the linker. This includes libiconv and libintl (gettext). This is done to provide compatibility between GNU Linux, where libiconv and libintl are bundled in, and other systems where that might not be the case. Sometimes, this behavior is not desired. To disable this behavior, set dontAddExtraLibs.\n\nlibxml2 \n\nAdds every file named catalog.xml found under the xml/dtd and xml/xsl subdirectories of each build input to the XML_CATALOG_FILES environment variable.\n\nMeson \nVariables controlling Meson\n\nOverrides the configure phase to run meson to generate Ninja files. To run these files, you should accompany Meson with ninja. By default, enableParallelBuilding is enabled as Meson supports parallel building almost everywhere.\n\nVariables controlling Meson \nmesonFlags\n\nControls the flags passed to meson.\n\nmesonBuildType\n\nWhich --buildtype to pass to Meson. We default to plain.\n\nmesonAutoFeatures\n\nWhat value to set -Dauto_features= to. We default to enabled.\n\nmesonWrapMode\n\nWhat value to set -Dwrap_mode= to. We default to nodownload as we disallow network access.\n\ndontUseMesonConfigure\n\nDisables using Meson’s configurePhase.\n\nmpiCheckPhaseHook \n\nThis hook can be used to setup a check phase that requires running a MPI application. It detects the used present MPI implementation type and exports the neceesary environment variables to use mpirun and mpiexec in a Nix sandbox.\n\nExample:\n\n  { mpiCheckPhaseHook, mpi, ... }:\n\n  ...\n\n  nativeCheckInputs = [\n    openssh\n    mpiCheckPhaseHook\n  ];\n\nninja \n\nOverrides the build, install, and check phase to run ninja instead of make. You can disable this behavior with the dontUseNinjaBuild, dontUseNinjaInstall, and dontUseNinjaCheck, respectively. Parallel building is enabled by default in Ninja.\n\npatchRcPath hooks \n\nThese hooks provide shell-specific utilities (with the same name as the hook) to patch shell scripts meant to be sourced by software users.\n\nThe typical usage is to patch initialisation or rc scripts inside $out/bin or $out/etc. Such scripts, when being sourced, would insert the binary locations of certain commands into PATH, modify other environment variables or run a series of start-up commands. When shipped from the upstream, they sometimes use commands that might not be available in the environment they are getting sourced in.\n\nThe compatible shells for each hook are:\n\npatchRcPathBash: Bash, ksh, zsh and other shells supporting the Bash-like parameter expansions.\n\npatchRcPathCsh: Csh scripts, such as those targeting tcsh.\n\npatchRcPathFish: Fish scripts.\n\npatchRcPathPosix: POSIX-conformant shells supporting the limited parameter expansions specified by the POSIX standard. Current implementation uses the parameter expansion ${foo-} only.\n\nFor each supported shell, it modifies the script with a PATH prefix that is later removed when the script ends. It allows nested patching, which guarantees that a patched script may source another patched script.\n\nSyntax to apply the utility to a script:\n\npatchRcPath<shell> <file> <PATH-prefix>\n\n\nExample usage:\n\nGiven a package foo containing an init script this-foo.fish that depends on coreutils, man and which, patch the init script for users to source without having the above dependencies in their PATH:\n\n{ lib, stdenv, patchRcPathFish}:\nstdenv.mkDerivation {\n\n  # ...\n\n  nativeBuildInputs = [\n    patchRcPathFish\n  ];\n\n  postFixup = ''\n    patchRcPathFish $out/bin/this-foo.fish ${lib.makeBinPath [ coreutils man which ]}\n  '';\n}\n\nNote\n\npatchRcPathCsh and patchRcPathPosix implementation depends on sed to do the string processing. The others are in vanilla shell and have no third-party dependencies.\n\nPerl \n\nAdds the lib/site_perl subdirectory of each build input to the PERL5LIB environment variable. For instance, if buildInputs contains Perl, then the lib/site_perl subdirectory of each input is added to the PERL5LIB environment variable.\n\npkg-config \n\nAdds the lib/pkgconfig and share/pkgconfig subdirectories of each build input to the PKG_CONFIG_PATH environment variable.\n\npostgresqlTestHook \nVariables\nHooks\nTCP and the Nix sandbox\n\nThis hook starts a PostgreSQL server during the checkPhase. Example:\n\n{ stdenv, postgresql, postgresqlTestHook }:\nstdenv.mkDerivation {\n\n  # ...\n\n  nativeCheckInputs = [\n    postgresql\n    postgresqlTestHook\n  ];\n}\n\n\nIf you use a custom checkPhase, remember to add the runHook calls:\n\n  checkPhase ''\n    runHook preCheck\n\n    # ... your tests\n\n    runHook postCheck\n  ''\n\nVariables \n\nThe hook logic will read a number of variables and set them to a default value if unset or empty.\n\nExported variables:\n\nPGDATA: location of server files.\n\nPGHOST: location of UNIX domain socket directory; the default host in a connection string.\n\nPGUSER: user to create / log in with, default: test_user.\n\nPGDATABASE: database name, default: test_db.\n\nBash-only variables:\n\npostgresqlTestUserOptions: SQL options to use when creating the $PGUSER role, default: \"LOGIN\". Example: \"LOGIN SUPERUSER\"\n\npostgresqlTestSetupSQL: SQL commands to run as database administrator after startup, default: statements that create $PGUSER and $PGDATABASE.\n\npostgresqlTestSetupCommands: bash commands to run after database start, defaults to running $postgresqlTestSetupSQL as database administrator.\n\npostgresqlEnableTCP: set to 1 to enable TCP listening. Flaky; not recommended.\n\npostgresqlStartCommands: defaults to pg_ctl start.\n\nHooks \n\nA number of additional hooks are ran in postgresqlTestHook\n\npostgresqlTestSetupPost: ran after postgresql has been set up.\n\nTCP and the Nix sandbox \n\npostgresqlEnableTCP relies on network sandboxing, which is not available on macOS and some custom Nix installations, resulting in flaky tests. For this reason, it is disabled by default.\n\nThe preferred solution is to make the test suite use a UNIX domain socket connection. This is the default behavior when no host connection parameter is provided. Some test suites hardcode a value for host though, so a patch may be required. If you can upstream the patch, you can make host default to the PGHOST environment variable when set. Otherwise, you can patch it locally to omit the host connection string parameter altogether.\n\nNote\n\nThe error libpq: failed (could not receive data from server: Connection refused is generally an indication that the test suite is trying to connect through TCP.\n\nPython \n\nAdds the lib/${python.libPrefix}/site-packages subdirectory of each build input to the PYTHONPATH environment variable.\n\nQt 4 \n\nSets the QTDIR environment variable to Qt’s path.\n\nscons \n\nOverrides the build, install, and check phases. This uses the scons build system as a replacement for make. scons does not provide a configure phase, so everything is managed at build and install time.\n\nteTeX / TeX Live \n\nAdds the share/texmf-nix subdirectory of each build input to the TEXINPUTS environment variable.\n\nunzip \n\nThis setup hook will allow you to unzip .zip files specified in $src. There are many similar packages like unrar, undmg, etc.\n\nvalidatePkgConfig \n\nThe validatePkgConfig hook validates all pkg-config (.pc) files in a package. This helps catching some common errors in pkg-config files, such as undefined variables.\n\nwafHook \nVariables controlling wafHook\n\nWaf is a Python-based software building system.\n\nIn Nixpkgs, wafHook overrides the default configure, build, and install phases.\n\nVariables controlling wafHook \nwafHook Exclusive Variables\n\nThe variables below are exclusive of wafHook.\n\nwafPath\n\nLocation of the waf tool. It defaults to ./waf, to honor software projects that include it directly inside their source trees.\n\nIf wafPath doesn’t exist, then wafHook will copy the waf provided from Nixpkgs to it.\n\nwafFlags\n\nControls the flags passed to waf tool during build and install phases. For settings specific to build or install phases, use wafBuildFlags or wafInstallFlags respectively.\n\ndontAddWafCrossFlags\n\nWhen set to true, don’t add cross compilation flags during configure phase.\n\ndontUseWafConfigure\n\nWhen set to true, don’t use the predefined wafConfigurePhase.\n\ndontUseWafBuild\n\nWhen set to true, don’t use the predefined wafBuildPhase.\n\ndontUseWafInstall\n\nWhen set to true, don’t use the predefined wafInstallPhase.\n\nSimilar variables\n\nThe following variables are similar to their stdenv.mkDerivation counterparts.\n\nwafHook Variable\tstdenv.mkDerivation Counterpart\nwafConfigureFlags\tconfigureFlags\nwafConfigureTargets\tconfigureTargets\nwafBuildFlags\tbuildFlags\nwafBuildTargets\tbuildTargets\nwafInstallFlags\tinstallFlags\nwafInstallTargets\tinstallTargets\nHonored variables\n\nThe following variables commonly used by stdenv.mkDerivation are honored by wafHook.\n\nprefixKey\n\nenableParallelBuilding\n\nenableParallelInstalling\n\nzig.hook \nExample code snippet\nVariables controlling zig.hook\n\nZig is a general-purpose programming language and toolchain for maintaining robust, optimal and reusable software.\n\nIn Nixpkgs, zig.hook overrides the default build, check and install phases.\n\nExample code snippet \n{ lib\n, stdenv\n, zig_0_11\n}:\n\nstdenv.mkDerivation {\n  # . . .\n\n  nativeBuildInputs = [\n    zig_0_11.hook\n  ];\n\n  zigBuildFlags = [ \"-Dman-pages=true\" ];\n\n  dontUseZigCheck = true;\n\n  # . . .\n}\n\nVariables controlling zig.hook \nzig.hook Exclusive Variables\n\nThe variables below are exclusive to zig.hook.\n\ndontUseZigBuild\n\nDisables using zigBuildPhase.\n\ndontUseZigCheck\n\nDisables using zigCheckPhase.\n\ndontUseZigInstall\n\nDisables using zigInstallPhase.\n\nSimilar variables\n\nThe following variables are similar to their stdenv.mkDerivation counterparts.\n\nzig.hook Variable\tstdenv.mkDerivation Counterpart\nzigBuildFlags\tbuildFlags\nzigCheckFlags\tcheckFlags\nzigInstallFlags\tinstallFlags\nVariables honored by zig.hook\n\nThe following variables commonly used by stdenv.mkDerivation are honored by zig.hook.\n\nprefixKey\n\ndontAddPrefix\n\nxcbuildHook \n\nOverrides the build and install phases to run the “xcbuild” command. This hook is needed when a project only comes with build files for the XCode build system. You can disable this behavior by setting buildPhase and configurePhase to a custom value. xcbuildFlags controls flags passed only to xcbuild.\n\nLanguages and frameworks \n\nTable of Contents\n\nAgda\nAndroid\nBEAM Languages (Erlang, Elixir & LFE)\nBower\nCHICKEN\nCoq and coq packages\nCrystal\nCUDA\nCue (Cuelang)\nDart\nDhall\nDotnet\nEmscripten\nGNOME\nGo\nHaskell\nHy\nIdris\niOS\nJava\nJavascript\nlisp-modules\nUser’s Guide to Lua Infrastructure\nMaven\nNim\nOCaml\nOctave\nPerl\nPHP\npkg-config\nPython\nQt\nR\nRuby\nRust\nSwift\nTeX Live\nTitanium\nVim\n\nThe standard build environment makes it easy to build typical Autotools-based packages with very little code. Any other kind of package can be accommodated by overriding the appropriate phases of stdenv. However, there are specialised functions in Nixpkgs to easily build packages for other programming languages, such as Perl or Haskell. These are described in this chapter.\n\nAgda \nHow to use Agda\nCompiling Agda\nWriting Agda packages\nMaintaining the Agda package set on Nixpkgs\nHow to use Agda \n\nAgda is available as the agda package.\n\nThe agda package installs an Agda-wrapper, which calls agda with --library-file set to a generated library-file within the nix store, this means your library-file in $HOME/.agda/libraries will be ignored. By default the agda package installs Agda with no libraries, i.e. the generated library-file is empty. To use Agda with libraries, the agda.withPackages function can be used. This function either takes:\n\nA list of packages,\n\nor a function which returns a list of packages when given the agdaPackages attribute set,\n\nor an attribute set containing a list of packages and a GHC derivation for compilation (see below).\n\nor an attribute set containing a function which returns a list of packages when given the agdaPackages attribute set and a GHC derivation for compilation (see below).\n\nFor example, suppose we wanted a version of Agda which has access to the standard library. This can be obtained with the expressions:\n\nagda.withPackages [ agdaPackages.standard-library ]\n\n\nor\n\nagda.withPackages (p: [ p.standard-library ])\n\n\nor can be called as in the Compiling Agda section.\n\nIf you want to use a different version of a library (for instance a development version) override the src attribute of the package to point to your local repository\n\nagda.withPackages (p: [\n  (p.standard-library.overrideAttrs (oldAttrs: {\n    version = \"local version\";\n    src = /path/to/local/repo/agda-stdlib;\n  }))\n])\n\n\nYou can also reference a GitHub repository\n\nagda.withPackages (p: [\n  (p.standard-library.overrideAttrs (oldAttrs: {\n    version = \"1.5\";\n    src =  fetchFromGitHub {\n      repo = \"agda-stdlib\";\n      owner = \"agda\";\n      rev = \"v1.5\";\n      hash = \"sha256-nEyxYGSWIDNJqBfGpRDLiOAnlHJKEKAOMnIaqfVZzJk=\";\n    };\n  }))\n])\n\n\nIf you want to use a library not added to Nixpkgs, you can add a dependency to a local library by calling agdaPackages.mkDerivation.\n\nagda.withPackages (p: [\n  (p.mkDerivation {\n    pname = \"your-agda-lib\";\n    version = \"1.0.0\";\n    src = /path/to/your-agda-lib;\n  })\n])\n\n\nAgain you can reference GitHub\n\nagda.withPackages (p: [\n  (p.mkDerivation {\n    pname = \"your-agda-lib\";\n    version = \"1.0.0\";\n    src = fetchFromGitHub {\n      repo = \"repo\";\n      owner = \"owner\";\n      version = \"...\";\n      rev = \"...\";\n      hash = \"...\";\n    };\n  })\n])\n\n\nSee Building Agda Packages for more information on mkDerivation.\n\nAgda will not by default use these libraries. To tell Agda to use a library we have some options:\n\nCall agda with the library flag:\n\n$ agda -l standard-library -i . MyFile.agda\n\n\nWrite a my-library.agda-lib file for the project you are working on which may look like:\n\nname: my-library\ninclude: .\ndepend: standard-library\n\n\nCreate the file ~/.agda/defaults and add any libraries you want to use by default.\n\nMore information can be found in the official Agda documentation on library management.\n\nCompiling Agda \n\nAgda modules can be compiled using the GHC backend with the --compile flag. A version of ghc with ieee754 is made available to the Agda program via the --with-compiler flag. This can be overridden by a different version of ghc as follows:\n\nagda.withPackages {\n  pkgs = [ ... ];\n  ghc = haskell.compiler.ghcHEAD;\n}\n\nWriting Agda packages \n\nTo write a nix derivation for an Agda library, first check that the library has a *.agda-lib file.\n\nA derivation can then be written using agdaPackages.mkDerivation. This has similar arguments to stdenv.mkDerivation with the following additions:\n\neverythingFile can be used to specify the location of the Everything.agda file, defaulting to ./Everything.agda. If this file does not exist then either it should be patched in or the buildPhase should be overridden (see below).\n\nlibraryName should be the name that appears in the *.agda-lib file, defaulting to pname.\n\nlibraryFile should be the file name of the *.agda-lib file, defaulting to ${libraryName}.agda-lib.\n\nHere is an example default.nix\n\n{ nixpkgs ?  <nixpkgs> }:\nwith (import nixpkgs {});\nagdaPackages.mkDerivation {\n  version = \"1.0\";\n  pname = \"my-agda-lib\";\n  src = ./.;\n  buildInputs = [\n    agdaPackages.standard-library\n  ];\n}\n\nBuilding Agda packages\n\nThe default build phase for agdaPackages.mkDerivation runs agda on the Everything.agda file. If something else is needed to build the package (e.g. make) then the buildPhase should be overridden. Additionally, a preBuild or configurePhase can be used if there are steps that need to be done prior to checking the Everything.agda file. agda and the Agda libraries contained in buildInputs are made available during the build phase.\n\nInstalling Agda packages\n\nThe default install phase copies Agda source files, Agda interface files (*.agdai) and *.agda-lib files to the output directory. This can be overridden.\n\nBy default, Agda sources are files ending on .agda, or literate Agda files ending on .lagda, .lagda.tex, .lagda.org, .lagda.md, .lagda.rst. The list of recognised Agda source extensions can be extended by setting the extraExtensions config variable.\n\nMaintaining the Agda package set on Nixpkgs \n\nWe are aiming at providing all common Agda libraries as packages on nixpkgs, and keeping them up to date. Contributions and maintenance help is always appreciated, but the maintenance effort is typically low since the Agda ecosystem is quite small.\n\nThe nixpkgs Agda package set tries to take up a role similar to that of Stackage in the Haskell world. It is a curated set of libraries that:\n\nAlways work together.\n\nAre as up-to-date as possible.\n\nWhile the Haskell ecosystem is huge, and Stackage is highly automatised, the Agda package set is small and can (still) be maintained by hand.\n\nAdding Agda packages to Nixpkgs\n\nTo add an Agda package to nixpkgs, the derivation should be written to pkgs/development/libraries/agda/${library-name}/ and an entry should be added to pkgs/top-level/agda-packages.nix. Here it is called in a scope with access to all other Agda libraries, so the top line of the default.nix can look like:\n\n{ mkDerivation, standard-library, fetchFromGitHub }:\n\n\nNote that the derivation function is called with mkDerivation set to agdaPackages.mkDerivation, therefore you could use a similar set as in your default.nix from Writing Agda Packages with agdaPackages.mkDerivation replaced with mkDerivation.\n\nHere is an example skeleton derivation for iowa-stdlib:\n\nmkDerivation {\n  version = \"1.5.0\";\n  pname = \"iowa-stdlib\";\n\n  src = ...\n\n  libraryFile = \"\";\n  libraryName = \"IAL-1.3\";\n\n  buildPhase = ''\n    patchShebangs find-deps.sh\n    make\n  '';\n}\n\n\nThis library has a file called .agda-lib, and so we give an empty string to libraryFile as nothing precedes .agda-lib in the filename. This file contains name: IAL-1.3, and so we let libraryName = \"IAL-1.3\". This library does not use an Everything.agda file and instead has a Makefile, so there is no need to set everythingFile and we set a custom buildPhase.\n\nWhen writing an Agda package it is essential to make sure that no .agda-lib file gets added to the store as a single file (for example by using writeText). This causes Agda to think that the nix store is a Agda library and it will attempt to write to it whenever it typechecks something. See https://github.com/agda/agda/issues/4613.\n\nIn the pull request adding this library, you can test whether it builds correctly by writing in a comment:\n\n@ofborg build agdaPackages.iowa-stdlib\n\nMaintaining Agda packages\n\nAs mentioned before, the aim is to have a compatible, and up-to-date package set. These two conditions sometimes exclude each other: For example, if we update agdaPackages.standard-library because there was an upstream release, this will typically break many reverse dependencies, i.e. downstream Agda libraries that depend on the standard library. In nixpkgs we are typically among the first to notice this, since we have build tests in place to check this.\n\nIn a pull request updating e.g. the standard library, you should write the following comment:\n\n@ofborg build agdaPackages.standard-library.passthru.tests\n\n\nThis will build all reverse dependencies of the standard library, for example agdaPackages.agda-categories, or agdaPackages.generic.\n\nIn some cases it is useful to build all Agda packages. This can be done with the following Github comment:\n\n@ofborg build agda.passthru.tests.allPackages\n\n\nSometimes, the builds of the reverse dependencies fail because they have not yet been updated and released. You should drop the maintainers a quick issue notifying them of the breakage, citing the build error (which you can get from the ofborg logs). If you are motivated, you might even send a pull request that fixes it. Usually, the maintainers will answer within a week or two with a new release. Bumping the version of that reverse dependency should be a further commit on your PR.\n\nIn the rare case that a new release is not to be expected within an acceptable time, mark the broken package as broken by setting meta.broken = true;. This will exclude it from the build test. It can be added later when it is fixed, and does not hinder the advancement of the whole package set in the meantime.\n\nAndroid \nDeploying an Android SDK installation with plugins\nUsing predefined Android package compositions\nBuilding an Android application\nSpawning emulator instances\nNotes on environment variables in Android projects\nNotes on improving build.gradle compatibility\nQuerying the available versions of each plugin\nUpdating the generated expressions\n\nThe Android build environment provides three major features and a number of supporting features.\n\nDeploying an Android SDK installation with plugins \n\nThe first use case is deploying the SDK with a desired set of plugins or subsets of an SDK.\n\nwith import <nixpkgs> {};\n\nlet\n  androidComposition = androidenv.composeAndroidPackages {\n    cmdLineToolsVersion = \"8.0\";\n    toolsVersion = \"26.1.1\";\n    platformToolsVersion = \"30.0.5\";\n    buildToolsVersions = [ \"30.0.3\" ];\n    includeEmulator = false;\n    emulatorVersion = \"30.3.4\";\n    platformVersions = [ \"28\" \"29\" \"30\" ];\n    includeSources = false;\n    includeSystemImages = false;\n    systemImageTypes = [ \"google_apis_playstore\" ];\n    abiVersions = [ \"armeabi-v7a\" \"arm64-v8a\" ];\n    cmakeVersions = [ \"3.10.2\" ];\n    includeNDK = true;\n    ndkVersions = [\"22.0.7026061\"];\n    useGoogleAPIs = false;\n    useGoogleTVAddOns = false;\n    includeExtras = [\n      \"extras;google;gcm\"\n    ];\n  };\nin\nandroidComposition.androidsdk\n\n\nThe above function invocation states that we want an Android SDK with the above specified plugin versions. By default, most plugins are disabled. Notable exceptions are the tools, platform-tools and build-tools sub packages.\n\nThe following parameters are supported:\n\ncmdLineToolsVersion, specifies the version of the cmdline-tools package to use\n\ntoolsVersion, specifies the version of the tools package. Notice tools is obsolete, and currently only 26.1.1 is available, so there’s not a lot of options here, however, you can set it as null if you don’t want it.\n\nplatformsToolsVersion specifies the version of the platform-tools plugin\n\nbuildToolsVersions specifies the versions of the build-tools plugins to use.\n\nincludeEmulator specifies whether to deploy the emulator package (false by default). When enabled, the version of the emulator to deploy can be specified by setting the emulatorVersion parameter.\n\ncmakeVersions specifies which CMake versions should be deployed.\n\nincludeNDK specifies that the Android NDK bundle should be included. Defaults to: false.\n\nndkVersions specifies the NDK versions that we want to use. These are linked under the ndk directory of the SDK root, and the first is linked under the ndk-bundle directory.\n\nndkVersion is equivalent to specifying one entry in ndkVersions, and ndkVersions overrides this parameter if provided.\n\nincludeExtras is an array of identifier strings referring to arbitrary add-on packages that should be installed.\n\nplatformVersions specifies which platform SDK versions should be included.\n\nFor each platform version that has been specified, we can apply the following options:\n\nincludeSystemImages specifies whether a system image for each platform SDK should be included.\n\nincludeSources specifies whether the sources for each SDK version should be included.\n\nuseGoogleAPIs specifies that for each selected platform version the Google API should be included.\n\nuseGoogleTVAddOns specifies that for each selected platform version the Google TV add-on should be included.\n\nFor each requested system image we can specify the following options:\n\nsystemImageTypes specifies what kind of system images should be included. Defaults to: default.\n\nabiVersions specifies what kind of ABI version of each system image should be included. Defaults to: armeabi-v7a.\n\nMost of the function arguments have reasonable default settings.\n\nYou can specify license names:\n\nextraLicenses is a list of license names. You can get these names from repo.json or querypackages.sh licenses. The SDK license (android-sdk-license) is accepted for you if you set accept_license to true. If you are doing something like working with preview SDKs, you will want to add android-sdk-preview-license or whichever license applies here.\n\nAdditionally, you can override the repositories that composeAndroidPackages will pull from:\n\nrepoJson specifies a path to a generated repo.json file. You can generate this by running generate.sh, which in turn will call into mkrepo.rb.\n\nrepoXmls is an attribute set containing paths to repo XML files. If specified, it takes priority over repoJson, and will trigger a local build writing out a repo.json to the Nix store based on the given repository XMLs.\n\nrepoXmls = {\n  packages = [ ./xml/repository2-1.xml ];\n  images = [\n    ./xml/android-sys-img2-1.xml\n    ./xml/android-tv-sys-img2-1.xml\n    ./xml/android-wear-sys-img2-1.xml\n    ./xml/android-wear-cn-sys-img2-1.xml\n    ./xml/google_apis-sys-img2-1.xml\n    ./xml/google_apis_playstore-sys-img2-1.xml\n  ];\n  addons = [ ./xml/addon2-1.xml ];\n};\n\n\nWhen building the above expression with:\n\n$ nix-build\n\n\nThe Android SDK gets deployed with all desired plugin versions.\n\nWe can also deploy subsets of the Android SDK. For example, to only the platform-tools package, you can evaluate the following expression:\n\nwith import <nixpkgs> {};\n\nlet\n  androidComposition = androidenv.composeAndroidPackages {\n    # ...\n  };\nin\nandroidComposition.platform-tools\n\nUsing predefined Android package compositions \n\nIn addition to composing an Android package set manually, it is also possible to use a predefined composition that contains all basic packages for a specific Android version, such as version 9.0 (API-level 28).\n\nThe following Nix expression can be used to deploy the entire SDK with all basic plugins:\n\nwith import <nixpkgs> {};\n\nandroidenv.androidPkgs_9_0.androidsdk\n\n\nIt is also possible to use one plugin only:\n\nwith import <nixpkgs> {};\n\nandroidenv.androidPkgs_9_0.platform-tools\n\nBuilding an Android application \n\nIn addition to the SDK, it is also possible to build an Ant-based Android project and automatically deploy all the Android plugins that a project requires.\n\nwith import <nixpkgs> {};\n\nandroidenv.buildApp {\n  name = \"MyAndroidApp\";\n  src = ./myappsources;\n  release = true;\n\n  # If release is set to true, you need to specify the following parameters\n  keyStore = ./keystore;\n  keyAlias = \"myfirstapp\";\n  keyStorePassword = \"mykeystore\";\n  keyAliasPassword = \"myfirstapp\";\n\n  # Any Android SDK parameters that install all the relevant plugins that a\n  # build requires\n  platformVersions = [ \"24\" ];\n\n  # When we include the NDK, then ndk-build is invoked before Ant gets invoked\n  includeNDK = true;\n}\n\n\nAside from the app-specific build parameters (name, src, release and keystore parameters), the buildApp {} function supports all the function parameters that the SDK composition function (the function shown in the previous section) supports.\n\nThis build function is particularly useful when it is desired to use Hydra: the Nix-based continuous integration solution to build Android apps. An Android APK gets exposed as a build product and can be installed on any Android device with a web browser by navigating to the build result page.\n\nSpawning emulator instances \n\nFor testing purposes, it can also be quite convenient to automatically generate scripts that spawn emulator instances with all desired configuration settings.\n\nAn emulator spawn script can be configured by invoking the emulateApp {} function:\n\nwith import <nixpkgs> {};\n\nandroidenv.emulateApp {\n  name = \"emulate-MyAndroidApp\";\n  platformVersion = \"28\";\n  abiVersion = \"x86\"; # armeabi-v7a, mips, x86_64\n  systemImageType = \"google_apis_playstore\";\n}\n\n\nAdditional flags may be applied to the Android SDK’s emulator through the runtime environment variable $NIX_ANDROID_EMULATOR_FLAGS.\n\nIt is also possible to specify an APK to deploy inside the emulator and the package and activity names to launch it:\n\nwith import <nixpkgs> {};\n\nandroidenv.emulateApp {\n  name = \"emulate-MyAndroidApp\";\n  platformVersion = \"24\";\n  abiVersion = \"armeabi-v7a\"; # mips, x86, x86_64\n  systemImageType = \"default\";\n  app = ./MyApp.apk;\n  package = \"MyApp\";\n  activity = \"MainActivity\";\n}\n\n\nIn addition to prebuilt APKs, you can also bind the APK parameter to a buildApp {} function invocation shown in the previous example.\n\nNotes on environment variables in Android projects \n\nANDROID_SDK_ROOT should point to the Android SDK. In your Nix expressions, this should be ${androidComposition.androidsdk}/libexec/android-sdk. Note that ANDROID_HOME is deprecated, but if you rely on tools that need it, you can export it too.\n\nANDROID_NDK_ROOT should point to the Android NDK, if you’re doing NDK development. In your Nix expressions, this should be ${ANDROID_SDK_ROOT}/ndk-bundle.\n\nIf you are running the Android Gradle plugin, you need to export GRADLE_OPTS to override aapt2 to point to the aapt2 binary in the Nix store as well, or use a FHS environment so the packaged aapt2 can run. If you don’t want to use a FHS environment, something like this should work:\n\nlet\n  buildToolsVersion = \"30.0.3\";\n\n  # Use buildToolsVersion when you define androidComposition\n  androidComposition = <...>;\nin\npkgs.mkShell rec {\n  ANDROID_SDK_ROOT = \"${androidComposition.androidsdk}/libexec/android-sdk\";\n  ANDROID_NDK_ROOT = \"${ANDROID_SDK_ROOT}/ndk-bundle\";\n\n  # Use the same buildToolsVersion here\n  GRADLE_OPTS = \"-Dorg.gradle.project.android.aapt2FromMavenOverride=${ANDROID_SDK_ROOT}/build-tools/${buildToolsVersion}/aapt2\";\n}\n\n\nIf you are using cmake, you need to add it to PATH in a shell hook or FHS env profile. The path is suffixed with a build number, but properly prefixed with the version. So, something like this should suffice:\n\nlet\n  cmakeVersion = \"3.10.2\";\n\n  # Use cmakeVersion when you define androidComposition\n  androidComposition = <...>;\nin\npkgs.mkShell rec {\n  ANDROID_SDK_ROOT = \"${androidComposition.androidsdk}/libexec/android-sdk\";\n  ANDROID_NDK_ROOT = \"${ANDROID_SDK_ROOT}/ndk-bundle\";\n\n  # Use the same cmakeVersion here\n  shellHook = ''\n    export PATH=\"$(echo \"$ANDROID_SDK_ROOT/cmake/${cmakeVersion}\".*/bin):$PATH\"\n  '';\n}\n\n\nNote that running Android Studio with ANDROID_SDK_ROOT set will automatically write a local.properties file with sdk.dir set to $ANDROID_SDK_ROOT if one does not already exist. If you are using the NDK as well, you may have to add ndk.dir to this file.\n\nAn example shell.nix that does all this for you is provided in examples/shell.nix. This shell.nix includes a shell hook that overwrites local.properties with the correct sdk.dir and ndk.dir values. This will ensure that the SDK and NDK directories will both be correct when you run Android Studio inside nix-shell.\n\nNotes on improving build.gradle compatibility \n\nEnsure that your buildToolsVersion and ndkVersion match what is declared in androidenv. If you are using cmake, make sure its declared version is correct too.\n\nOtherwise, you may get cryptic errors from aapt2 and the Android Gradle plugin warning that it cannot install the build tools because the SDK directory is not writeable.\n\nandroid {\n    buildToolsVersion \"30.0.3\"\n    ndkVersion = \"22.0.7026061\"\n    externalNativeBuild {\n        cmake {\n            version \"3.10.2\"\n        }\n    }\n}\n\n\nQuerying the available versions of each plugin \n\nrepo.json provides all the options in one file now.\n\nA shell script in the pkgs/development/mobile/androidenv/ subdirectory can be used to retrieve all possible options:\n\n./querypackages.sh packages\n\n\nThe above command-line instruction queries all package versions in repo.json.\n\nUpdating the generated expressions \n\nrepo.json is generated from XML files that the Android Studio package manager uses. To update the expressions run the generate.sh script that is stored in the pkgs/development/mobile/androidenv/ subdirectory:\n\n./generate.sh\n\nBEAM Languages (Erlang, Elixir & LFE) \nIntroduction\nAvailable versions and deprecations schedule\nStructure\nBuild Tools\nHow to Install BEAM Packages\nPackaging BEAM Applications\nHow to Develop\nIntroduction \n\nIn this document and related Nix expressions, we use the term, BEAM, to describe the environment. BEAM is the name of the Erlang Virtual Machine and, as far as we’re concerned, from a packaging perspective, all languages that run on the BEAM are interchangeable. That which varies, like the build system, is transparent to users of any given BEAM package, so we make no distinction.\n\nAvailable versions and deprecations schedule \nElixir\n\nnixpkgs follows the official elixir deprecation schedule and keeps the last 5 released versions of Elixir available.\n\nStructure \n\nAll BEAM-related expressions are available via the top-level beam attribute, which includes:\n\ninterpreters: a set of compilers running on the BEAM, including multiple Erlang/OTP versions (beam.interpreters.erlang_22, etc), Elixir (beam.interpreters.elixir) and LFE (Lisp Flavoured Erlang) (beam.interpreters.lfe).\n\npackages: a set of package builders (Mix and rebar3), each compiled with a specific Erlang/OTP version, e.g. beam.packages.erlang22.\n\nThe default Erlang compiler, defined by beam.interpreters.erlang, is aliased as erlang. The default BEAM package set is defined by beam.packages.erlang and aliased at the top level as beamPackages.\n\nTo create a package builder built with a custom Erlang version, use the lambda, beam.packagesWith, which accepts an Erlang/OTP derivation and produces a package builder similar to beam.packages.erlang.\n\nMany Erlang/OTP distributions available in beam.interpreters have versions with ODBC and/or Java enabled or without wx (no observer support). For example, there’s beam.interpreters.erlang_22_odbc_javac, which corresponds to beam.interpreters.erlang_22 and beam.interpreters.erlang_22_nox, which corresponds to beam.interpreters.erlang_22.\n\nBuild Tools \nRebar3\n\nWe provide a version of Rebar3, under rebar3. We also provide a helper to fetch Rebar3 dependencies from a lockfile under fetchRebar3Deps.\n\nWe also provide a version on Rebar3 with plugins included, under rebar3WithPlugins. This package is a function which takes two arguments: plugins, a list of nix derivations to include as plugins (loaded only when specified in rebar.config), and globalPlugins, which should always be loaded by rebar3. Example: rebar3WithPlugins { globalPlugins = [beamPackages.pc]; }.\n\nWhen adding a new plugin it is important that the packageName attribute is the same as the atom used by rebar3 to refer to the plugin.\n\nMix & Erlang.mk\n\nErlang.mk works exactly as expected. There is a bootstrap process that needs to be run, which is supported by the buildErlangMk derivation.\n\nFor Elixir applications use mixRelease to make a release. See examples for more details.\n\nThere is also a buildMix helper, whose behavior is closer to that of buildErlangMk and buildRebar3. The primary difference is that mixRelease makes a release, while buildMix only builds the package, making it useful for libraries and other dependencies.\n\nHow to Install BEAM Packages \n\nBEAM builders are not registered at the top level, because they are not relevant to the vast majority of Nix users. To use any of those builders into your environment, refer to them by their attribute path under beamPackages, e.g. beamPackages.rebar3:\n\nExample 222. Ephemeral shell\n\n$ nix-shell -p beamPackages.rebar3\n\n\n\n\nExample 223. Declarative shell\n\nlet\n  pkgs = import <nixpkgs> { config = {}; overlays = []; };\nin\npkgs.mkShell {\n  packages = [ pkgs.beamPackages.rebar3 ];\n}\n\n\n\nPackaging BEAM Applications \nErlang Applications\nRebar3 Packages\n\nThe Nix function, buildRebar3, defined in beam.packages.erlang.buildRebar3 and aliased at the top level, can be used to build a derivation that understands how to build a Rebar3 project.\n\nIf a package needs to compile native code via Rebar3’s port compilation mechanism, add compilePort = true; to the derivation.\n\nErlang.mk Packages\n\nErlang.mk functions similarly to Rebar3, except we use buildErlangMk instead of buildRebar3.\n\nMix Packages\n\nmixRelease is used to make a release in the mix sense. Dependencies will need to be fetched with fetchMixDeps and passed to it.\n\nmixRelease - Elixir Phoenix example\n\nthere are 3 steps, frontend dependencies (javascript), backend dependencies (elixir) and the final derivation that puts both of those together\n\nmixRelease - Frontend dependencies (javascript)\n\nFor phoenix projects, inside of nixpkgs you can either use yarn2nix (mkYarnModule) or node2nix. An example with yarn2nix can be found here. An example with node2nix will follow. To package something outside of nixpkgs, you have alternatives like npmlock2nix or nix-npm-buildpackage\n\nmixRelease - backend dependencies (mix)\n\nThere are 2 ways to package backend dependencies. With mix2nix and with a fixed-output-derivation (FOD).\n\nmix2nix\n\nmix2nix is a cli tool available in nixpkgs. it will generate a nix expression from a mix.lock file. It is quite standard in the 2nix tool series.\n\nNote that currently mix2nix can’t handle git dependencies inside the mix.lock file. If you have git dependencies, you can either add them manually (see example) or use the FOD method.\n\nThe advantage of using mix2nix is that nix will know your whole dependency graph. On a dependency update, this won’t trigger a full rebuild and download of all the dependencies, where FOD will do so.\n\nPractical steps:\n\nrun mix2nix > mix_deps.nix in the upstream repo.\n\npass mixNixDeps = with pkgs; import ./mix_deps.nix { inherit lib beamPackages; }; as an argument to mixRelease.\n\nIf there are git dependencies.\n\nYou’ll need to fix the version artificially in mix.exs and regenerate the mix.lock with fixed version (on upstream). This will enable you to run mix2nix > mix_deps.nix.\n\nFrom the mix_deps.nix file, remove the dependencies that had git versions and pass them as an override to the import function.\n\n  mixNixDeps = import ./mix.nix {\n    inherit beamPackages lib;\n    overrides = (final: prev: {\n      # mix2nix does not support git dependencies yet,\n      # so we need to add them manually\n      prometheus_ex = beamPackages.buildMix rec {\n        name = \"prometheus_ex\";\n        version = \"3.0.5\";\n\n        # Change the argument src with the git src that you actually need\n        src = fetchFromGitLab {\n          domain = \"git.pleroma.social\";\n          group = \"pleroma\";\n          owner = \"elixir-libraries\";\n          repo = \"prometheus.ex\";\n          rev = \"a4e9beb3c1c479d14b352fd9d6dd7b1f6d7deee5\";\n          hash = \"sha256-U17LlN6aGUKUFnT4XyYXppRN+TvUBIBRHEUsfeIiGOw=\";\n        };\n        # you can re-use the same beamDeps argument as generated\n        beamDeps = with final; [ prometheus ];\n      };\n  });\n};\n\n\nYou will need to run the build process once to fix the hash to correspond to your new git src.\n\nFOD\n\nA fixed output derivation will download mix dependencies from the internet. To ensure reproducibility, a hash will be supplied. Note that mix is relatively reproducible. An FOD generating a different hash on each run hasn’t been observed (as opposed to npm where the chances are relatively high). See elixir-ls for a usage example of FOD.\n\nPractical steps\n\nstart with the following argument to mixRelease\n\n  mixFodDeps = fetchMixDeps {\n    pname = \"mix-deps-${pname}\";\n    inherit src version;\n    hash = lib.fakeHash;\n  };\n\n\nThe first build will complain about the hash value, you can replace with the suggested value after that.\n\nNote that if after you’ve replaced the value, nix suggests another hash, then mix is not fetching the dependencies reproducibly. An FOD will not work in that case and you will have to use mix2nix.\n\nmixRelease - example\n\nHere is how your default.nix file would look for a phoenix project.\n\nwith import <nixpkgs> { };\n\nlet\n  # beam.interpreters.erlang_26 is available if you need a particular version\n  packages = beam.packagesWith beam.interpreters.erlang;\n\n  pname = \"your_project\";\n  version = \"0.0.1\";\n\n  src = builtins.fetchgit {\n    url = \"ssh://git@github.com/your_id/your_repo\";\n    rev = \"replace_with_your_commit\";\n  };\n\n  # if using mix2nix you can use the mixNixDeps attribute\n  mixFodDeps = packages.fetchMixDeps {\n    pname = \"mix-deps-${pname}\";\n    inherit src version;\n    # nix will complain and tell you the right value to replace this with\n    hash = lib.fakeHash;\n    mixEnv = \"\"; # default is \"prod\", when empty includes all dependencies, such as \"dev\", \"test\".\n    # if you have build time environment variables add them here\n    MY_ENV_VAR=\"my_value\";\n  };\n\n  nodeDependencies = (pkgs.callPackage ./assets/default.nix { }).shell.nodeDependencies;\n\nin packages.mixRelease {\n  inherit src pname version mixFodDeps;\n  # if you have build time environment variables add them here\n  MY_ENV_VAR=\"my_value\";\n\n  postBuild = ''\n    ln -sf ${nodeDependencies}/lib/node_modules assets/node_modules\n    npm run deploy --prefix ./assets\n\n    # for external task you need a workaround for the no deps check flag\n    # https://github.com/phoenixframework/phoenix/issues/2690\n    mix do deps.loadpaths --no-deps-check, phx.digest\n    mix phx.digest --no-deps-check\n  '';\n}\n\n\nSetup will require the following steps:\n\nMove your secrets to runtime environment variables. For more information refer to the runtime.exs docs. On a fresh Phoenix build that would mean that both DATABASE_URL and SECRET_KEY need to be moved to runtime.exs.\n\ncd assets and nix-shell -p node2nix --run node2nix --development will generate a Nix expression containing your frontend dependencies\n\ncommit and push those changes\n\nyou can now nix-build .\n\nTo run the release, set the RELEASE_TMP environment variable to a directory that your program has write access to. It will be used to store the BEAM settings.\n\nExample of creating a service for an Elixir - Phoenix project\n\nIn order to create a service with your release, you could add a service.nix in your project with the following\n\n{config, pkgs, lib, ...}:\n\nlet\n  release = pkgs.callPackage ./default.nix;\n  release_name = \"app\";\n  working_directory = \"/home/app\";\nin\n{\n  systemd.services.${release_name} = {\n    wantedBy = [ \"multi-user.target\" ];\n    after = [ \"network.target\" \"postgresql.service\" ];\n    # note that if you are connecting to a postgres instance on a different host\n    # postgresql.service should not be included in the requires.\n    requires = [ \"network-online.target\" \"postgresql.service\" ];\n    description = \"my app\";\n    environment = {\n      # RELEASE_TMP is used to write the state of the\n      # VM configuration when the system is running\n      # it needs to be a writable directory\n      RELEASE_TMP = working_directory;\n      # can be generated in an elixir console with\n      # Base.encode32(:crypto.strong_rand_bytes(32))\n      RELEASE_COOKIE = \"my_cookie\";\n      MY_VAR = \"my_var\";\n    };\n    serviceConfig = {\n      Type = \"exec\";\n      DynamicUser = true;\n      WorkingDirectory = working_directory;\n      # Implied by DynamicUser, but just to emphasize due to RELEASE_TMP\n      PrivateTmp = true;\n      ExecStart = ''\n        ${release}/bin/${release_name} start\n      '';\n      ExecStop = ''\n        ${release}/bin/${release_name} stop\n      '';\n      ExecReload = ''\n        ${release}/bin/${release_name} restart\n      '';\n      Restart = \"on-failure\";\n      RestartSec = 5;\n      StartLimitBurst = 3;\n      StartLimitInterval = 10;\n    };\n    # disksup requires bash\n    path = [ pkgs.bash ];\n  };\n\n  # in case you have migration scripts or you want to use a remote shell\n  environment.systemPackages = [ release ];\n}\n\nHow to Develop \nCreating a Shell\n\nUsually, we need to create a shell.nix file and do our development inside of the environment specified therein. Just install your version of Erlang and any other interpreters, and then use your normal build tools. As an example with Elixir:\n\n{ pkgs ? import <nixpkgs> {} }:\n\nwith pkgs;\nlet\n  elixir = beam.packages.erlang_24.elixir_1_12;\nin\nmkShell {\n  buildInputs = [ elixir ];\n}\n\nUsing an overlay\n\nIf you need to use an overlay to change some attributes of a derivation, e.g. if you need a bugfix from a version that is not yet available in nixpkgs, you can override attributes such as version (and the corresponding hash) and then use this overlay in your development environment:\n\nshell.nix\nlet\n  elixir_1_13_1_overlay = (self: super: {\n      elixir_1_13 = super.elixir_1_13.override {\n        version = \"1.13.1\";\n        sha256 = \"sha256-t0ic1LcC7EV3avWGdR7VbyX7pGDpnJSW1ZvwvQUPC3w=\";\n      };\n    });\n  pkgs = import <nixpkgs> { overlays = [ elixir_1_13_1_overlay ]; };\nin\nwith pkgs;\nmkShell {\n  buildInputs = [\n    elixir_1_13\n  ];\n}\n\nElixir - Phoenix project\n\nHere is an example shell.nix.\n\nwith import <nixpkgs> { };\n\nlet\n  # define packages to install\n  basePackages = [\n    git\n    # replace with beam.packages.erlang.elixir_1_13 if you need\n    beam.packages.erlang.elixir\n    nodejs\n    postgresql_14\n    # only used for frontend dependencies\n    # you are free to use yarn2nix as well\n    nodePackages.node2nix\n    # formatting js file\n    nodePackages.prettier\n  ];\n\n  inputs = basePackages ++ lib.optionals stdenv.isLinux [ inotify-tools ]\n    ++ lib.optionals stdenv.isDarwin\n    (with darwin.apple_sdk.frameworks; [ CoreFoundation CoreServices ]);\n\n  # define shell startup command\n  hooks = ''\n    # this allows mix to work on the local directory\n    mkdir -p .nix-mix .nix-hex\n    export MIX_HOME=$PWD/.nix-mix\n    export HEX_HOME=$PWD/.nix-mix\n    # make hex from Nixpkgs available\n    # `mix local.hex` will install hex into MIX_HOME and should take precedence\n    export MIX_PATH=\"${beam.packages.erlang.hex}/lib/erlang/lib/hex/ebin\"\n    export PATH=$MIX_HOME/bin:$HEX_HOME/bin:$PATH\n    export LANG=C.UTF-8\n    # keep your shell history in iex\n    export ERL_AFLAGS=\"-kernel shell_history enabled\"\n\n    # postges related\n    # keep all your db data in a folder inside the project\n    export PGDATA=\"$PWD/db\"\n\n    # phoenix related env vars\n    export POOL_SIZE=15\n    export DB_URL=\"postgresql://postgres:postgres@localhost:5432/db\"\n    export PORT=4000\n    export MIX_ENV=dev\n    # add your project env vars here, word readable in the nix store.\n    export ENV_VAR=\"your_env_var\"\n  '';\n\nin mkShell {\n  buildInputs = inputs;\n  shellHook = hooks;\n}\n\n\nInitializing the project will require the following steps:\n\ncreate the db directory initdb ./db (inside your mix project folder)\n\ncreate the postgres user createuser postgres -ds\n\ncreate the db createdb db\n\nstart the postgres instance pg_ctl -l \"$PGDATA/server.log\" start\n\nadd the /db folder to your .gitignore\n\nyou can start your phoenix server and get a shell with iex -S mix phx.server\n\nBower \nbower2nix usage\nbuildBowerComponents function\nTroubleshooting\n\nBower is a package manager for web site front-end components. Bower packages (comprising of build artifacts and sometimes sources) are stored in git repositories, typically on Github. The package registry is run by the Bower team with package metadata coming from the bower.json file within each package.\n\nThe end result of running Bower is a bower_components directory which can be included in the web app’s build process.\n\nBower can be run interactively, by installing nodePackages.bower. More interestingly, the Bower components can be declared in a Nix derivation, with the help of nodePackages.bower2nix.\n\nbower2nix usage \n\nSuppose you have a bower.json with the following contents:\n\nExample bower.json\n  \"name\": \"my-web-app\",\n  \"dependencies\": {\n    \"angular\": \"~1.5.0\",\n    \"bootstrap\": \"~3.3.6\"\n  }\n\n\nRunning bower2nix will produce something like the following output:\n\n{ fetchbower, buildEnv }:\nbuildEnv { name = \"bower-env\"; ignoreCollisions = true; paths = [\n  (fetchbower \"angular\" \"1.5.3\" \"~1.5.0\" \"1749xb0firxdra4rzadm4q9x90v6pzkbd7xmcyjk6qfza09ykk9y\")\n  (fetchbower \"bootstrap\" \"3.3.6\" \"~3.3.6\" \"1vvqlpbfcy0k5pncfjaiskj3y6scwifxygfqnw393sjfxiviwmbv\")\n  (fetchbower \"jquery\" \"2.2.2\" \"1.9.1 - 2\" \"10sp5h98sqwk90y4k6hbdviwqzvzwqf47r3r51pakch5ii2y7js1\")\n];\n\n\nUsing the bower2nix command line arguments, the output can be redirected to a file. A name like bower-packages.nix would be fine.\n\nThe resulting derivation is a union of all the downloaded Bower packages (and their dependencies). To use it, they still need to be linked together by Bower, which is where buildBowerComponents is useful.\n\nbuildBowerComponents function \n\nThe function is implemented in pkgs/development/bower-modules/generic/default.nix.\n\nExample buildBowerComponents\nbowerComponents = buildBowerComponents {\n  name = \"my-web-app\";\n  generated = ./bower-packages.nix; # note 1\n  src = myWebApp; # note 2\n};\n\n\nIn “buildBowerComponents” example the following arguments are of special significance to the function:\n\ngenerated specifies the file which was created by bower2nix.\n\nsrc is your project’s sources. It needs to contain a bower.json file.\n\nbuildBowerComponents will run Bower to link together the output of bower2nix, resulting in a bower_components directory which can be used.\n\nHere is an example of a web frontend build process using gulp. You might use grunt, or anything else.\n\nExample build script (gulpfile.js)\nvar gulp = require('gulp');\n\ngulp.task('default', [], function () {\n  gulp.start('build');\n});\n\ngulp.task('build', [], function () {\n  console.log(\"Just a dummy gulp build\");\n  gulp\n    .src([\"./bower_components/**/*\"])\n    .pipe(gulp.dest(\"./gulpdist/\"));\n});\n\nExample Full example — default.nix\n{ myWebApp ? { outPath = ./.; name = \"myWebApp\"; }\n, pkgs ? import <nixpkgs> {}\n}:\n\npkgs.stdenv.mkDerivation {\n  name = \"my-web-app-frontend\";\n  src = myWebApp;\n\n  buildInputs = [ pkgs.nodePackages.gulp ];\n\n  bowerComponents = pkgs.buildBowerComponents { # note 1\n    name = \"my-web-app\";\n    generated = ./bower-packages.nix;\n    src = myWebApp;\n  };\n\n  buildPhase = ''\n    cp --reflink=auto --no-preserve=mode -R $bowerComponents/bower_components . # note 2\n    export HOME=$PWD # note 3\n    ${pkgs.nodePackages.gulp}/bin/gulp build # note 4\n  '';\n\n  installPhase = \"mv gulpdist $out\";\n}\n\n\nA few notes about Full example — default.nix:\n\nThe result of buildBowerComponents is an input to the frontend build.\n\nWhether to symlink or copy the bower_components directory depends on the build tool in use. In this case a copy is used to avoid gulp silliness with permissions.\n\ngulp requires HOME to refer to a writeable directory.\n\nThe actual build command in this example is gulp. Other tools could be used instead.\n\nTroubleshooting \nENOCACHE errors from buildBowerComponents\n\nThis means that Bower was looking for a package version which doesn’t exist in the generated bower-packages.nix.\n\nIf bower.json has been updated, then run bower2nix again.\n\nIt could also be a bug in bower2nix or fetchbower. If possible, try reformulating the version specification in bower.json.\n\nCHICKEN \nUsing Eggs\nUpdating Eggs\nAdding Eggs\nOverride Scope\n\nCHICKEN is a R⁵RS-compliant Scheme compiler. It includes an interactive mode and a custom package format, “eggs”.\n\nUsing Eggs \n\nEggs described in nixpkgs are available inside the chickenPackages.chickenEggs attrset. Including an egg as a build input is done in the typical Nix fashion. For example, to include support for SRFI 189 in a derivation, one might write:\n\n  buildInputs = [\n    chicken\n    chickenPackages.chickenEggs.srfi-189\n  ];\n\n\nBoth chicken and its eggs have a setup hook which configures the environment variables CHICKEN_INCLUDE_PATH and CHICKEN_REPOSITORY_PATH.\n\nUpdating Eggs \n\nnixpkgs only knows about a subset of all published eggs. It uses egg2nix to generate a package set from a list of eggs to include.\n\nThe package set is regenerated by running the following shell commands:\n\n$ nix-shell -p chickenPackages.egg2nix\n$ cd pkgs/development/compilers/chicken/5/\n$ egg2nix eggs.scm > eggs.nix\n\nAdding Eggs \n\nWhen we run egg2nix, we obtain one collection of eggs with mutually-compatible versions. This means that when we add new eggs, we may need to update existing eggs. To keep those separate, follow the procedure for updating eggs before including more eggs.\n\nTo include more eggs, edit pkgs/development/compilers/chicken/5/eggs.scm. The first section of this file lists eggs which are required by egg2nix itself; all other eggs go into the second section. After editing, follow the procedure for updating eggs.\n\nOverride Scope \n\nThe chicken package and its eggs, respectively, reside in a scope. This means, the scope can be overridden to effect other packages in it.\n\nThis example shows how to use a local copy of srfi-180 and have it affect all the other eggs:\n\nlet\n  myChickenPackages = pkgs.chickenPackages.overrideScope' (self: super: {\n      # The chicken package itself can be overridden to effect the whole ecosystem.\n      # chicken = super.chicken.overrideAttrs {\n      #   src = ...\n      # };\n\n      chickenEggs = super.chickenEggs.overrideScope' (eggself: eggsuper: {\n        srfi-180 = eggsuper.srfi-180.overrideAttrs {\n          # path to a local copy of srfi-180\n          src = ...\n        };\n      });\n  });\nin\n# Here, `myChickenPackages.chickenEggs.json-rpc`, which depends on `srfi-180` will use\n# the local copy of `srfi-180`.\n# ...\n\nCoq and coq packages \nCoq derivation: coq\nCoq packages attribute sets: coqPackages\nThree ways of overriding Coq packages\nCoq derivation: coq \n\nThe Coq derivation is overridable through the coq.override overrides, where overrides is an attribute set which contains the arguments to override. We recommend overriding either of the following\n\nversion (optional, defaults to the latest version of Coq selected for nixpkgs, see pkgs/top-level/coq-packages to witness this choice), which follows the conventions explained in the coqPackages section below,\n\ncustomOCamlPackages (optional, defaults to null, which lets Coq choose a version automatically), which can be set to any of the ocaml packages attribute of ocaml-ng (such as ocaml-ng.ocamlPackages_4_10 which is the default for Coq 8.11 for example).\n\ncoq-version (optional, defaults to the short version e.g. “8.10”), is a version number of the form “x.y” that indicates which Coq’s version build behavior to mimic when using a source which is not a release. E.g. coq.override { version = \"d370a9d1328a4e1cdb9d02ee032f605a9d94ec7a\"; coq-version = \"8.10\"; }.\n\nThe associated package set can be obtained using mkCoqPackages coq, where coq is the derivation to use.\n\nCoq packages attribute sets: coqPackages \n\nThe recommended way of defining a derivation for a Coq library, is to use the coqPackages.mkCoqDerivation function, which is essentially a specialization of mkDerivation taking into account most of the specifics of Coq libraries. The following attributes are supported:\n\npname (required) is the name of the package,\n\nversion (optional, defaults to null), is the version to fetch and build, this attribute is interpreted in several ways depending on its type and pattern:\n\nif it is a known released version string, i.e. from the release attribute below, the according release is picked, and the version attribute of the resulting derivation is set to this release string,\n\nif it is a majorMinor \"x.y\" prefix of a known released version (as defined above), then the latest \"x.y.z\" known released version is selected (for the ordering given by versionAtLeast),\n\nif it is a path or a string representing an absolute path (i.e. starting with \"/\"), the provided path is selected as a source, and the version attribute of the resulting derivation is set to \"dev\",\n\nif it is a string of the form owner:branch then it tries to download the branch of owner owner for a project of the same name using the same vcs, and the version attribute of the resulting derivation is set to \"dev\", additionally if the owner is not provided (i.e. if the owner: prefix is missing), it defaults to the original owner of the package (see below),\n\nif it is a string of the form \"#N\", and the domain is github, then it tries to download the current head of the pull request #N from github,\n\ndefaultVersion (optional). Coq libraries may be compatible with some specific versions of Coq only. The defaultVersion attribute is used when no version is provided (or if version = null) to select the version of the library to use by default, depending on the context. This selection will mainly depend on a coq version number but also possibly on other packages versions (e.g. mathcomp). If its value ends up to be null, the package is marked for removal in end-user coqPackages attribute set.\n\nrelease (optional, defaults to {}), lists all the known releases of the library and for each of them provides an attribute set with at least a sha256 attribute (you may put the empty string \"\" in order to automatically insert a fake sha256, this will trigger an error which will allow you to find the correct sha256), each attribute set of the list of releases also takes optional overloading arguments for the fetcher as below (i.e.domain, owner, repo, rev assuming the default fetcher is used) and optional overrides for the result of the fetcher (i.e. version and src).\n\nfetcher (optional, defaults to a generic fetching mechanism supporting github or gitlab based infrastructures), is a function that takes at least an owner, a repo, a rev, and a hash and returns an attribute set with a version and src.\n\nrepo (optional, defaults to the value of pname),\n\nowner (optional, defaults to \"coq-community\").\n\ndomain (optional, defaults to \"github.com\"), domains including the strings \"github\" or \"gitlab\" in their names are automatically supported, otherwise, one must change the fetcher argument to support them (cf pkgs/development/coq-modules/heq/default.nix for an example),\n\nreleaseRev (optional, defaults to (v: v)), provides a default mapping from release names to revision hashes/branch names/tags,\n\ndisplayVersion (optional), provides a way to alter the computation of name from pname, by explaining how to display version numbers,\n\nnamePrefix (optional, defaults to [ \"coq\" ]), provides a way to alter the computation of name from pname, by explaining which dependencies must occur in name,\n\nnativeBuildInputs (optional), is a list of executables that are required to build the current derivation, in addition to the default ones (namely which, dune and ocaml depending on whether useDune, useDuneifVersion and mlPlugin are set).\n\nextraNativeBuildInputs (optional, deprecated), an additional list of derivation to add to nativeBuildInputs,\n\noverrideNativeBuildInputs (optional) replaces the default list of derivation to which nativeBuildInputs and extraNativeBuildInputs adds extra elements,\n\nbuildInputs (optional), is a list of libraries and dependencies that are required to build and run the current derivation, in addition to the default one [ coq ],\n\nextraBuildInputs (optional, deprecated), an additional list of derivation to add to buildInputs,\n\noverrideBuildInputs (optional) replaces the default list of derivation to which buildInputs and extraBuildInputs adds extras elements,\n\npropagatedBuildInputs (optional) is passed as is to mkDerivation, we recommend to use this for Coq libraries and Coq plugin dependencies, as this makes sure the paths of the compiled libraries and plugins will always be added to the build environments of subsequent derivation, which is necessary for Coq packages to work correctly,\n\nmlPlugin (optional, defaults to false). Some extensions (plugins) might require OCaml and sometimes other OCaml packages. Standard dependencies can be added by setting the current option to true. For a finer grain control, the coq.ocamlPackages attribute can be used in nativeBuildInputs, buildInputs, and propagatedBuildInputs to depend on the same package set Coq was built against.\n\nuseDuneifVersion (optional, default to (x: false) uses Dune to build the package if the provided predicate evaluates to true on the version, e.g. useDuneifVersion = versions.isGe \"1.1\" will use dune if the version of the package is greater or equal to \"1.1\",\n\nuseDune (optional, defaults to false) uses Dune to build the package if set to true, the presence of this attribute overrides the behavior of the previous one.\n\nopam-name (optional, defaults to concatenating with a dash separator the components of namePrefix and pname), name of the Dune package to build.\n\nenableParallelBuilding (optional, defaults to true), since it is activated by default, we provide a way to disable it.\n\nextraInstallFlags (optional), allows to extend installFlags which initializes the variable COQMF_COQLIB so as to install in the proper subdirectory. Indeed Coq libraries should be installed in $(out)/lib/coq/${coq.coq-version}/user-contrib/. Such directories are automatically added to the $COQPATH environment variable by the hook defined in the Coq derivation.\n\nsetCOQBIN (optional, defaults to true), by default, the environment variable $COQBIN is set to the current Coq’s binary, but one can disable this behavior by setting it to false,\n\nuseMelquiondRemake (optional, default to null) is an attribute set, which, if given, overloads the preConfigurePhases, configureFlags, buildPhase, and installPhase attributes of the derivation for a specific use in libraries using remake as set up by Guillaume Melquiond for flocq, gappalib, interval, and coquelicot (see the corresponding derivation for concrete examples of use of this option). For backward compatibility, the attribute useMelquiondRemake.logpath must be set to the logical root of the library (otherwise, one can pass useMelquiondRemake = {} to activate this without backward compatibility).\n\ndropAttrs, keepAttrs, dropDerivationAttrs are all optional and allow to tune which attribute is added or removed from the final call to mkDerivation.\n\nIt also takes other standard mkDerivation attributes, they are added as such, except for meta which extends an automatically computed meta (where the platform is the same as coq and the homepage is automatically computed).\n\nHere is a simple package example. It is a pure Coq library, thus it depends on Coq. It builds on the Mathematical Components library, thus it also takes some mathcomp derivations as extraBuildInputs.\n\n{ lib, mkCoqDerivation, version ? null\n, coq, mathcomp, mathcomp-finmap, mathcomp-bigenough }:\nwith lib; mkCoqDerivation {\n  /* namePrefix leads to e.g. `name = coq8.11-mathcomp1.11-multinomials-1.5.2` */\n  namePrefix = [ \"coq\" \"mathcomp\" ];\n  pname = \"multinomials\";\n  owner = \"math-comp\";\n  inherit version;\n  defaultVersion =  with versions; switch [ coq.version mathcomp.version ] [\n      { cases = [ (range \"8.7\" \"8.12\")  \"1.11.0\" ];             out = \"1.5.2\"; }\n      { cases = [ (range \"8.7\" \"8.11\")  (range \"1.8\" \"1.10\") ]; out = \"1.5.0\"; }\n      { cases = [ (range \"8.7\" \"8.10\")  (range \"1.8\" \"1.10\") ]; out = \"1.4\"; }\n      { cases = [ \"8.6\"                 (range \"1.6\" \"1.7\") ];  out = \"1.1\"; }\n    ] null;\n  release = {\n    \"1.5.2\".sha256 = \"15aspf3jfykp1xgsxf8knqkxv8aav2p39c2fyirw7pwsfbsv2c4s\";\n    \"1.5.1\".sha256 = \"13nlfm2wqripaq671gakz5mn4r0xwm0646araxv0nh455p9ndjs3\";\n    \"1.5.0\".sha256 = \"064rvc0x5g7y1a0nip6ic91vzmq52alf6in2bc2dmss6dmzv90hw\";\n    \"1.5.0\".rev    = \"1.5\";\n    \"1.4\".sha256   = \"0vnkirs8iqsv8s59yx1fvg1nkwnzydl42z3scya1xp1b48qkgn0p\";\n    \"1.3\".sha256   = \"0l3vi5n094nx3qmy66hsv867fnqm196r8v605kpk24gl0aa57wh4\";\n    \"1.2\".sha256   = \"1mh1w339dslgv4f810xr1b8v2w7rpx6fgk9pz96q0fyq49fw2xcq\";\n    \"1.1\".sha256   = \"1q8alsm89wkc0lhcvxlyn0pd8rbl2nnxg81zyrabpz610qqjqc3s\";\n    \"1.0\".sha256   = \"1qmbxp1h81cy3imh627pznmng0kvv37k4hrwi2faa101s6bcx55m\";\n  };\n\n  propagatedBuildInputs =\n    [ mathcomp.ssreflect mathcomp.algebra mathcomp-finmap mathcomp-bigenough ];\n\n  meta = {\n    description = \"A Coq/SSReflect Library for Monoidal Rings and Multinomials\";\n    license = licenses.cecill-c;\n  };\n}\n\nThree ways of overriding Coq packages \n\nThere are three distinct ways of changing a Coq package by overriding one of its values: .override, overrideCoqDerivation, and .overrideAttrs. This section explains what sort of values can be overridden with each of these methods.\n\n.override\n\n.override lets you change arguments to a Coq derivation. In the case of the multinomials package above, .override would let you override arguments like mkCoqDerivation, version, coq, mathcomp, mathcom-finmap, etc.\n\nFor example, assuming you have a special mathcomp dependency you want to use, here is how you could override the mathcomp dependency:\n\nmultinomials.override {\n  mathcomp = my-special-mathcomp;\n}\n\n\nIn Nixpkgs, all Coq derivations take a version argument. This can be overridden in order to easily use a different version:\n\ncoqPackages.multinomials.override {\n  version = \"1.5.1\";\n}\n\n\nRefer to the section called “Coq packages attribute sets: coqPackages” for all the different formats that you can potentially pass to version, as well as the restrictions.\n\noverrideCoqDerivation\n\nThe overrideCoqDerivation function lets you easily change arguments to mkCoqDerivation. These arguments are described in the section called “Coq packages attribute sets: coqPackages”.\n\nFor example, here is how you could locally add a new release of the multinomials library, and set the defaultVersion to use this release:\n\ncoqPackages.lib.overrideCoqDerivation\n  {\n    defaultVersion = \"2.0\";\n    release.\"2.0\".sha256 = \"1lq8x86vd3vqqh2yq6hvyagpnhfq5wmk5pg2z0xq7b7dbbbhyfkk\";\n  }\n  coqPackages.multinomials\n\n.overrideAttrs\n\n.overrideAttrs lets you override arguments to the underlying stdenv.mkDerivation call. Internally, mkCoqDerivation uses stdenv.mkDerivation to create derivations for Coq libraries. You can override arguments to stdenv.mkDerivation with .overrideAttrs.\n\nFor instance, here is how you could add some code to be performed in the derivation after installation is complete:\n\ncoqPackages.multinomials.overrideAttrs (oldAttrs: {\n  postInstall = oldAttrs.postInstall or \"\" + ''\n    echo \"you can do anything you want here\"\n  '';\n})\n\nCrystal \nBuilding a Crystal package\nBuilding a Crystal package \n\nThis section uses Mint as an example for how to build a Crystal package.\n\nIf the Crystal project has any dependencies, the first step is to get a shards.nix file encoding those. Get a copy of the project and go to its root directory such that its shard.lock file is in the current directory. Executable projects should usually commit the shard.lock file, but sometimes that’s not the case, which means you need to generate it yourself. With an existing shard.lock file, crystal2nix can be run.\n\n$ git clone https://github.com/mint-lang/mint\n$ cd mint\n$ git checkout 0.5.0\n$ if [ ! -f shard.lock ]; then nix-shell -p shards --run \"shards lock\"; fi\n$ nix-shell -p crystal2nix --run crystal2nix\n\n\nThis should have generated a shards.nix file.\n\nNext create a Nix file for your derivation and use pkgs.crystal.buildCrystalPackage as follows:\n\nwith import <nixpkgs> {};\ncrystal.buildCrystalPackage rec {\n  pname = \"mint\";\n  version = \"0.5.0\";\n\n  src = fetchFromGitHub {\n    owner = \"mint-lang\";\n    repo = \"mint\";\n    rev = version;\n    hash = \"sha256-dFN9l5fgrM/TtOPqlQvUYgixE4KPr629aBmkwdDoq28=\";\n  };\n\n  # Insert the path to your shards.nix file here\n  shardsFile = ./shards.nix;\n\n  ...\n}\n\n\nThis won’t build anything yet, because we haven’t told it what files build. We can specify a mapping from binary names to source files with the crystalBinaries attribute. The project’s compilation instructions should show this. For Mint, the binary is called “mint”, which is compiled from the source file src/mint.cr, so we’ll specify this as follows:\n\n  crystalBinaries.mint.src = \"src/mint.cr\";\n\n  # ...\n\n\nAdditionally you can override the default crystal build options (which are currently --release --progress --no-debug --verbose) with\n\n  crystalBinaries.mint.options = [ \"--release\" \"--verbose\" ];\n\n\nDepending on the project, you might need additional steps to get it to compile successfully. In Mint’s case, we need to link against openssl, so in the end the Nix file looks as follows:\n\nwith import <nixpkgs> {};\ncrystal.buildCrystalPackage rec {\n  version = \"0.5.0\";\n  pname = \"mint\";\n  src = fetchFromGitHub {\n    owner = \"mint-lang\";\n    repo = \"mint\";\n    rev = version;\n    hash = \"sha256-dFN9l5fgrM/TtOPqlQvUYgixE4KPr629aBmkwdDoq28=\";\n  };\n\n  shardsFile = ./shards.nix;\n  crystalBinaries.mint.src = \"src/mint.cr\";\n\n  buildInputs = [ openssl ];\n}\n\nCUDA \nAdding a new CUDA release\n\nCUDA-only packages are stored in the cudaPackages packages set. This set includes the cudatoolkit, portions of the toolkit in separate derivations, cudnn, cutensor and nccl.\n\nA package set is available for each CUDA version, so for example cudaPackages_11_6. Within each set is a matching version of the above listed packages. Additionally, other versions of the packages that are packaged and compatible are available as well. For example, there can be a cudaPackages.cudnn_8_3 package.\n\nTo use one or more CUDA packages in an expression, give the expression a cudaPackages parameter, and in case CUDA is optional\n\n{ config\n, cudaSupport ? config.cudaSupport\n, cudaPackages ? { }\n, ...\n}:\n\n\nWhen using callPackage, you can choose to pass in a different variant, e.g. when a different version of the toolkit suffices\n\nmypkg = callPackage { cudaPackages = cudaPackages_11_5; }\n\n\nIf another version of say cudnn or cutensor is needed, you can override the package set to make it the default. This guarantees you get a consistent package set.\n\nmypkg = let\n  cudaPackages = cudaPackages_11_5.overrideScope (final: prev: {\n    cudnn = prev.cudnn_8_3;\n  }});\nin callPackage { inherit cudaPackages; };\n\n\nThe CUDA NVCC compiler requires flags to determine which hardware you want to target for in terms of SASS (real hardware) or PTX (JIT kernels).\n\nNixpkgs tries to target support real architecture defaults based on the CUDA toolkit version with PTX support for future hardware. Experienced users may optimize this configuration for a variety of reasons such as reducing binary size and compile time, supporting legacy hardware, or optimizing for specific hardware.\n\nYou may provide capabilities to add support or reduce binary size through config using cudaCapabilities = [ \"6.0\" \"7.0\" ]; and cudaForwardCompat = true; if you want PTX support for future hardware.\n\nPlease consult GPUs supported for your specific card(s).\n\nLibrary maintainers should consult NVCC Docs and release notes for their software package.\n\nAdding a new CUDA release \n\nWARNING\n\nThis section of the docs is still very much in progress. Feedback is welcome in GitHub Issues tagging @NixOS/cuda-maintainers or on Matrix.\n\nThe CUDA Toolkit is a suite of CUDA libraries and software meant to provide a development environment for CUDA-accelerated applications. Until the release of CUDA 11.4, NVIDIA had only made the CUDA Toolkit available as a multi-gigabyte runfile installer, which we provide through the cudaPackages.cudatoolkit attribute. From CUDA 11.4 and onwards, NVIDIA has also provided CUDA redistributables (“CUDA-redist”): individually packaged CUDA Toolkit components meant to facilitate redistribution and inclusion in downstream projects. These packages are available in the cudaPackages package set.\n\nAll new projects should use the CUDA redistributables available in cudaPackages in place of cudaPackages.cudatoolkit, as they are much easier to maintain and update.\n\nUpdating CUDA redistributables\n\nGo to NVIDIA’s index of CUDA redistributables: https://developer.download.nvidia.com/compute/cuda/redist/\n\nCopy the redistrib_*.json corresponding to the release to pkgs/development/compilers/cudatoolkit/redist/manifests.\n\nGenerate the redistrib_features_*.json file by running:\n\nnix run github:ConnorBaker/cuda-redist-find-features -- <path to manifest>\n\n\nThat command will generate the redistrib_features_*.json file in the same directory as the manifest.\n\nInclude the path to the new manifest in pkgs/development/compilers/cudatoolkit/redist/extension.nix.\n\nUpdating the CUDA Toolkit runfile installer\n\nWARNING\n\nWhile the CUDA Toolkit runfile installer is still available in Nixpkgs as the cudaPackages.cudatoolkit attribute, its use is not recommended and should it be considered deprecated. Please migrate to the CUDA redistributables provided by the cudaPackages package set.\n\nTo ensure packages relying on the CUDA Toolkit runfile installer continue to build, it will continue to be updated until a migration path is available.\n\nGo to NVIDIA’s CUDA Toolkit runfile installer download page: https://developer.nvidia.com/cuda-downloads\n\nSelect the appropriate OS, architecture, distribution, and version, and installer type.\n\nFor example: Linux, x86_64, Ubuntu, 22.04, runfile (local)\n\nNOTE: Typically, we use the Ubuntu runfile. It is unclear if the runfile for other distributions will work.\n\nTake the link provided by the installer instructions on the webpage after selecting the installer type and get its hash by running:\n\nnix store prefetch-file --hash-type sha256 <link>\n\n\nUpdate pkgs/development/compilers/cudatoolkit/versions.toml to include the release.\n\nUpdating the CUDA package set\n\nInclude a new cudaPackages_<major>_<minor> package set in pkgs/top-level/all-packages.nix.\n\nNOTE: Changing the default CUDA package set should occur in a separate PR, allowing time for additional testing.\n\nSuccessfully build the closure of the new package set, updating pkgs/development/compilers/cudatoolkit/redist/overrides.nix as needed. Below are some common failures:\n\nUnable to …\tDuring …\tReason\tSolution\tNote\nFind headers\tconfigurePhase or buildPhase\tMissing dependency on a dev output\tAdd the missing dependency\tThe dev output typically contain the headers\nFind libraries\tconfigurePhase\tMissing dependency on a dev output\tAdd the missing dependency\tThe dev output typically contain CMake configuration files\nFind libraries\tbuildPhase or patchelf\tMissing dependency on a lib or static output\tAdd the missing dependency\tThe lib or static output typically contain the libraries\n\nIn the scenario you are unable to run the resulting binary: this is arguably the most complicated as it could be any combination of the previous reasons. This type of failure typically occurs when a library attempts to load or open a library it depends on that it does not declare in its DT_NEEDED section. As a first step, ensure that dependencies are patched with cudaPackages.autoAddOpenGLRunpath. Failing that, try running the application with nixGL or a similar wrapper tool. If that works, it likely means that the application is attempting to load a library that is not in the RPATH or RUNPATH of the binary.\n\nCue (Cuelang) \nCuelang schema quick start\nwriteCueValidator\n\nCuelang is a language to:\n\ndescribe schemas and validate backward-compatibility\n\ngenerate code and schemas in various formats (e.g. JSON Schema, OpenAPI)\n\ndo configuration akin to Dhall Lang\n\nperform data validation\n\nCuelang schema quick start \n\nCuelang schemas are similar to JSON, here is a quick cheatsheet:\n\nDefault types includes: null, string, bool, bytes, number, int, float, lists as [...T] where T is a type.\n\nAll structures, defined by: myStructName: { <fields> } are open – they accept fields which are not specified.\n\nClosed structures can be built by doing myStructName: close({ <fields> }) – they are strict in what they accept.\n\n#X are definitions, referenced definitions are recursively closed, i.e. all its children structures are closed.\n\n& operator is the unification operator (similar to a type-level merging operator), | is the disjunction operator (similar to a type-level union operator).\n\nValues are types, i.e. myStruct: { a: 3 } is a valid type definition that only allows 3 as value.\n\nRead https://cuelang.org/docs/concepts/logic/ to learn more about the semantics.\n\nRead https://cuelang.org/docs/references/spec/ to learn about the language specification.\n\nwriteCueValidator \n\nNixpkgs provides a pkgs.writeCueValidator helper, which will write a validation script based on the provided Cuelang schema.\n\nHere is an example:\n\npkgs.writeCueValidator\n  (pkgs.writeText \"schema.cue\" ''\n    #Def1: {\n      field1: string\n    }\n  '')\n  { document = \"#Def1\"; }\n\n\nThe first parameter is the Cue schema file.\n\nThe second parameter is an options parameter, currently, only: document can be passed.\n\ndocument : match your input data against this fragment of structure or definition, e.g. you may use the same schema file but different documents based on the data you are validating.\n\nAnother example, given the following validator.nix :\n\n{ pkgs ? import <nixpkgs> {} }:\nlet\n  genericValidator = version:\n  pkgs.writeCueValidator\n    (pkgs.writeText \"schema.cue\" ''\n      #Version1: {\n        field1: string\n      }\n      #Version2: #Version1 & {\n        field1: \"unused\"\n      }''\n    )\n    { document = \"#Version${toString version}\"; };\nin\n{\n  validateV1 = genericValidator 1;\n  validateV2 = genericValidator 2;\n}\n\n\nThe result is a script that will validate the file you pass as the first argument against the schema you provided writeCueValidator.\n\nIt can be any format that cue vet supports, i.e. YAML or JSON for example.\n\nHere is an example, named example.json, given the following JSON:\n\n{ \"field1\": \"abc\" }\n\n\nYou can run the result script (named validate) as the following:\n\n$ nix-build validator.nix\n$ ./result example.json\n$ ./result-2 example.json\nfield1: conflicting values \"unused\" and \"abc\":\n    ./example.json:1:13\n    ../../../../../../nix/store/v64dzx3vr3glpk0cq4hzmh450lrwh6sg-schema.cue:5:11\n$ sed -i 's/\"abc\"/3/' example.json\n$ ./result example.json\nfield1: conflicting values 3 and string (mismatched types int and string):\n    ./example.json:1:13\n    ../../../../../../nix/store/v64dzx3vr3glpk0cq4hzmh450lrwh6sg-schema.cue:5:11\n\n\nKnown limitations\n\nThe script will enforce concrete values and will not accept lossy transformations (strictness). You can add these options if you need them.\n\nDart \nDart applications\nFlutter applications\nDart applications \n\nThe function buildDartApplication builds Dart applications managed with pub.\n\nIt fetches its Dart dependencies automatically through fetchDartDeps, and (through a series of hooks) builds and installs the executables specified in the pubspec file. The hooks can be used in other derivations, if needed. The phases can also be overridden to do something different from installing binaries.\n\nIf you are packaging a Flutter desktop application, use buildFlutterApplication instead.\n\nvendorHash: is the hash of the output of the dependency fetcher derivation. To obtain it, set it to lib.fakeHash (or omit it) and run the build (more details here).\n\nIf the upstream source is missing a pubspec.lock file, you’ll have to vendor one and specify it using pubspecLockFile. If it is needed, one will be generated for you and printed when attempting to build the derivation.\n\nThe depsListFile must always be provided when packaging in Nixpkgs. It will be generated and printed if the derivation is attempted to be built without one. Alternatively, autoDepsList may be set to true only when outside of Nixpkgs, as it relies on import-from-derivation.\n\nThe dart commands run can be overridden through pubGetScript and dartCompileCommand, you can also add flags using dartCompileFlags or dartJitFlags.\n\nDart supports multiple outputs types, you can choose between them using dartOutputType (defaults to exe). If you want to override the binaries path or the source path they come from, you can use dartEntryPoints. Outputs that require a runtime will automatically be wrapped with the relevant runtime (dartaotruntime for aot-snapshot, dart run for jit-snapshot and kernel, node for js), this can be overridden through dartRuntimeCommand.\n\n{ buildDartApplication, fetchFromGitHub }:\n\nbuildDartApplication rec {\n  pname = \"dart-sass\";\n  version = \"1.62.1\";\n\n  src = fetchFromGitHub {\n    owner = \"sass\";\n    repo = pname;\n    rev = version;\n    hash = \"sha256-U6enz8yJcc4Wf8m54eYIAnVg/jsGi247Wy8lp1r1wg4=\";\n  };\n\n  pubspecLockFile = ./pubspec.lock;\n  depsListFile = ./deps.json;\n  vendorHash = \"sha256-Atm7zfnDambN/BmmUf4BG0yUz/y6xWzf0reDw3Ad41s=\";\n}\n\nFlutter applications \n\nThe function buildFlutterApplication builds Flutter applications.\n\nSee the Dart documentation for more details on required files and arguments.\n\n{  flutter, fetchFromGitHub }:\n\nflutter.buildFlutterApplication {\n  pname = \"firmware-updater\";\n  version = \"unstable-2023-04-30\";\n\n  src = fetchFromGitHub {\n    owner = \"canonical\";\n    repo = \"firmware-updater\";\n    rev = \"6e7dbdb64e344633ea62874b54ff3990bd3b8440\";\n    sha256 = \"sha256-s5mwtr5MSPqLMN+k851+pFIFFPa0N1hqz97ys050tFA=\";\n    fetchSubmodules = true;\n  };\n\n  pubspecLockFile = ./pubspec.lock;\n  depsListFile = ./deps.json;\n  vendorHash = \"sha256-cdMO+tr6kYiN5xKXa+uTMAcFf2C75F3wVPrn21G4QPQ=\";\n}\n\nDhall \nRemote imports\nPackaging a Dhall expression from scratch\nContents of a Dhall package\nPackaging functions\ndhall-to-nixpkgs\nOverriding dependency versions\nOverrides\n\nThe Nixpkgs support for Dhall assumes some familiarity with Dhall’s language support for importing Dhall expressions, which is documented here:\n\ndhall-lang.org - Installing packages\n\nRemote imports \n\nNixpkgs bypasses Dhall’s support for remote imports using Dhall’s semantic integrity checks. Specifically, any Dhall import can be protected by an integrity check like:\n\nhttps://prelude.dhall-lang.org/v20.1.0/package.dhall\n  sha256:26b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n\n\n… and if the import is cached then the interpreter will load the import from cache instead of fetching the URL.\n\nNixpkgs uses this trick to add all of a Dhall expression’s dependencies into the cache so that the Dhall interpreter never needs to resolve any remote URLs. In fact, Nixpkgs uses a Dhall interpreter with remote imports disabled when packaging Dhall expressions to enforce that the interpreter never resolves a remote import. This means that Nixpkgs only supports building Dhall expressions if all of their remote imports are protected by semantic integrity checks.\n\nInstead of remote imports, Nixpkgs uses Nix to fetch remote Dhall code. For example, the Prelude Dhall package uses pkgs.fetchFromGitHub to fetch the dhall-lang repository containing the Prelude. Relying exclusively on Nix to fetch Dhall code ensures that Dhall packages built using Nix remain pure and also behave well when built within a sandbox.\n\nPackaging a Dhall expression from scratch \n\nWe can illustrate how Nixpkgs integrates Dhall by beginning from the following trivial Dhall expression with one dependency (the Prelude):\n\n-- ./true.dhall\n\nlet Prelude = https://prelude.dhall-lang.org/v20.1.0/package.dhall\n\nin  Prelude.Bool.not False\n\n\nAs written, this expression cannot be built using Nixpkgs because the expression does not protect the Prelude import with a semantic integrity check, so the first step is to freeze the expression using dhall freeze, like this:\n\n$ dhall freeze --inplace ./true.dhall\n\n\n… which gives us:\n\n-- ./true.dhall\n\nlet Prelude =\n      https://prelude.dhall-lang.org/v20.1.0/package.dhall\n        sha256:26b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n\nin  Prelude.Bool.not False\n\n\nTo package that expression, we create a ./true.nix file containing the following specification for the Dhall package:\n\n# ./true.nix\n\n{ buildDhallPackage, Prelude }:\n\nbuildDhallPackage {\n  name = \"true\";\n  code = ./true.dhall;\n  dependencies = [ Prelude ];\n  source = true;\n}\n\n\n… and we complete the build by incorporating that Dhall package into the pkgs.dhallPackages hierarchy using an overlay, like this:\n\n# ./example.nix\n\nlet\n  nixpkgs = builtins.fetchTarball {\n    url    = \"https://github.com/NixOS/nixpkgs/archive/94b2848559b12a8ed1fe433084686b2a81123c99.tar.gz\";\n    hash = \"sha256-B4Q3c6IvTLg3Q92qYa8y+i4uTaphtFdjp+Ir3QQjdN0=\";\n  };\n\n  dhallOverlay = self: super: {\n    true = self.callPackage ./true.nix { };\n  };\n\n  overlay = self: super: {\n    dhallPackages = super.dhallPackages.override (old: {\n      overrides =\n        self.lib.composeExtensions (old.overrides or (_: _: {})) dhallOverlay;\n    });\n  };\n\n  pkgs = import nixpkgs { config = {}; overlays = [ overlay ]; };\n\nin\n  pkgs\n\n\n… which we can then build using this command:\n\n$ nix build --file ./example.nix dhallPackages.true\n\nContents of a Dhall package \n\nThe above package produces the following directory tree:\n\n$ tree -a ./result\nresult\n├── .cache\n│   └── dhall\n│       └── 122027abdeddfe8503496adeb623466caa47da5f63abd2bc6fa19f6cfcb73ecfed70\n├── binary.dhall\n└── source.dhall\n\n\n… where:\n\nsource.dhall contains the result of interpreting our Dhall package:\n\n$ cat ./result/source.dhall\nTrue\n\n\nThe .cache subdirectory contains one binary cache product encoding the same result as source.dhall:\n\n$ dhall decode < ./result/.cache/dhall/122027abdeddfe8503496adeb623466caa47da5f63abd2bc6fa19f6cfcb73ecfed70\nTrue\n\n\nbinary.dhall contains a Dhall expression which handles fetching and decoding the same cache product:\n\n$ cat ./result/binary.dhall\nmissing sha256:27abdeddfe8503496adeb623466caa47da5f63abd2bc6fa19f6cfcb73ecfed70\n$ cp -r ./result/.cache .cache\n\n$ chmod -R u+w .cache\n\n$ XDG_CACHE_HOME=.cache dhall --file ./result/binary.dhall\nTrue\n\n\nThe source.dhall file is only present for packages that specify source = true;. By default, Dhall packages omit the source.dhall in order to conserve disk space when they are used exclusively as dependencies. For example, if we build the Prelude package it will only contain the binary encoding of the expression:\n\n$ nix build --file ./example.nix dhallPackages.Prelude\n\n$ tree -a result\nresult\n├── .cache\n│   └── dhall\n│       └── 122026b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n└── binary.dhall\n\n2 directories, 2 files\n\n\nTypically, you only specify source = true; for the top-level Dhall expression of interest (such as our example true.nix Dhall package). However, if you wish to specify source = true for all Dhall packages, then you can amend the Dhall overlay like this:\n\n  dhallOverrides = self: super: {\n    # Enable source for all Dhall packages\n    buildDhallPackage =\n      args: super.buildDhallPackage (args // { source = true; });\n\n    true = self.callPackage ./true.nix { };\n  };\n\n\n… and now the Prelude will contain the fully decoded result of interpreting the Prelude:\n\n$ nix build --file ./example.nix dhallPackages.Prelude\n\n$ tree -a result\nresult\n├── .cache\n│   └── dhall\n│       └── 122026b0ef498663d269e4dc6a82b0ee289ec565d683ef4c00d0ebdd25333a5a3c98\n├── binary.dhall\n└── source.dhall\n\n$ cat ./result/source.dhall\n{ Bool =\n  { and =\n      \\(_ : List Bool) ->\n        List/fold Bool _ Bool (\\(_ : Bool) -> \\(_ : Bool) -> _@1 && _) True\n  , build = \\(_ : Type -> _ -> _@1 -> _@2) -> _ Bool True False\n  , even =\n      \\(_ : List Bool) ->\n        List/fold Bool _ Bool (\\(_ : Bool) -> \\(_ : Bool) -> _@1 == _) True\n  , fold =\n      \\(_ : Bool) ->\n…\n\nPackaging functions \n\nWe already saw an example of using buildDhallPackage to create a Dhall package from a single file, but most Dhall packages consist of more than one file and there are two derived utilities that you may find more useful when packaging multiple files:\n\nbuildDhallDirectoryPackage - build a Dhall package from a local directory\n\nbuildDhallGitHubPackage - build a Dhall package from a GitHub repository\n\nThe buildDhallPackage is the lowest-level function and accepts the following arguments:\n\nname: The name of the derivation\n\ndependencies: Dhall dependencies to build and cache ahead of time\n\ncode: The top-level expression to build for this package\n\nNote that the code field accepts an arbitrary Dhall expression. You’re not limited to just a file.\n\nsource: Set to true to include the decoded result as source.dhall in the build product, at the expense of requiring more disk space\n\ndocumentationRoot: Set to the root directory of the package if you want dhall-docs to generate documentation underneath the docs subdirectory of the build product\n\nThe buildDhallDirectoryPackage is a higher-level function implemented in terms of buildDhallPackage that accepts the following arguments:\n\nname: Same as buildDhallPackage\n\ndependencies: Same as buildDhallPackage\n\nsource: Same as buildDhallPackage\n\nsrc: The directory containing Dhall code that you want to turn into a Dhall package\n\nfile: The top-level file (package.dhall by default) that is the entrypoint to the rest of the package\n\ndocument: Set to true to generate documentation for the package\n\nThe buildDhallGitHubPackage is another higher-level function implemented in terms of buildDhallPackage that accepts the following arguments:\n\nname: Same as buildDhallPackage\n\ndependencies: Same as buildDhallPackage\n\nsource: Same as buildDhallPackage\n\nowner: The owner of the repository\n\nrepo: The repository name\n\nrev: The desired revision (or branch, or tag)\n\ndirectory: The subdirectory of the Git repository to package (if a directory other than the root of the repository)\n\nfile: The top-level file (${directory}/package.dhall by default) that is the entrypoint to the rest of the package\n\ndocument: Set to true to generate documentation for the package\n\nAdditionally, buildDhallGitHubPackage accepts the same arguments as fetchFromGitHub, such as hash or fetchSubmodules.\n\ndhall-to-nixpkgs \n\nYou can use the dhall-to-nixpkgs command-line utility to automate packaging Dhall code. For example:\n\n$ nix-shell -p haskellPackages.dhall-nixpkgs nix-prefetch-git\n[nix-shell]$ dhall-to-nixpkgs github https://github.com/Gabriella439/dhall-semver.git\n{ buildDhallGitHubPackage, Prelude }:\n  buildDhallGitHubPackage {\n    name = \"dhall-semver\";\n    githubBase = \"github.com\";\n    owner = \"Gabriella439\";\n    repo = \"dhall-semver\";\n    rev = \"2d44ae605302ce5dc6c657a1216887fbb96392a4\";\n    fetchSubmodules = false;\n    hash = \"sha256-n0nQtswVapWi/x7or0O3MEYmAkt/a1uvlOtnje6GGnk=\";\n    directory = \"\";\n    file = \"package.dhall\";\n    source = false;\n    document = false;\n    dependencies = [ (Prelude.overridePackage { file = \"package.dhall\"; }) ];\n    }\n\nNote\n\nnix-prefetch-git has to be in $PATH for dhall-to-nixpkgs to work.\n\nThe utility takes care of automatically detecting remote imports and converting them to package dependencies. You can also use the utility on local Dhall directories, too:\n\n$ dhall-to-nixpkgs directory ~/proj/dhall-semver\n{ buildDhallDirectoryPackage, Prelude }:\n  buildDhallDirectoryPackage {\n    name = \"proj\";\n    src = ~/proj/dhall-semver;\n    file = \"package.dhall\";\n    source = false;\n    document = false;\n    dependencies = [ (Prelude.overridePackage { file = \"package.dhall\"; }) ];\n    }\n\nRemote imports as fixed-output derivations\n\ndhall-to-nixpkgs has the ability to fetch and build remote imports as fixed-output derivations by using their Dhall integrity check. This is sometimes easier than manually packaging all remote imports.\n\nThis can be used like the following:\n\n$ dhall-to-nixpkgs directory --fixed-output-derivations ~/proj/dhall-semver\n{ buildDhallDirectoryPackage, buildDhallUrl }:\n  buildDhallDirectoryPackage {\n    name = \"proj\";\n    src = ~/proj/dhall-semver;\n    file = \"package.dhall\";\n    source = false;\n    document = false;\n    dependencies = [\n      (buildDhallUrl {\n        url = \"https://prelude.dhall-lang.org/v17.0.0/package.dhall\";\n        hash = \"sha256-ENs8kZwl6QRoM9+Jeo/+JwHcOQ+giT2VjDQwUkvlpD4=\";\n        dhallHash = \"sha256:10db3c919c25e9046833df897a8ffe2701dc390fa0893d958c3430524be5a43e\";\n        })\n      ];\n    }\n\n\nHere, dhall-semver’s Prelude dependency is fetched and built with the buildDhallUrl helper function, instead of being passed in as a function argument.\n\nOverriding dependency versions \n\nSuppose that we change our true.dhall example expression to depend on an older version of the Prelude (19.0.0):\n\n-- ./true.dhall\n\nlet Prelude =\n      https://prelude.dhall-lang.org/v19.0.0/package.dhall\n        sha256:eb693342eb769f782174157eba9b5924cf8ac6793897fc36a31ccbd6f56dafe2\n\nin  Prelude.Bool.not False\n\n\nIf we try to rebuild that expression the build will fail:\n\n$ nix build --file ./example.nix dhallPackages.true\nbuilder for '/nix/store/0f1hla7ff1wiaqyk1r2ky4wnhnw114fi-true.drv' failed with exit code 1; last 10 log lines:\n\n  Dhall was compiled without the 'with-http' flag.\n\n  The requested URL was: https://prelude.dhall-lang.org/v19.0.0/package.dhall\n\n\n  4│       https://prelude.dhall-lang.org/v19.0.0/package.dhall\n  5│         sha256:eb693342eb769f782174157eba9b5924cf8ac6793897fc36a31ccbd6f56dafe2\n\n  /nix/store/rsab4y99h14912h4zplqx2iizr5n4rc2-true.dhall:4:7\n[1 built (1 failed), 0.0 MiB DL]\nerror: build of '/nix/store/0f1hla7ff1wiaqyk1r2ky4wnhnw114fi-true.drv' failed\n\n\n… because the default Prelude selected by Nixpkgs revision 94b2848559b12a8ed1fe433084686b2a81123c99is is version 20.1.0, which doesn’t have the same integrity check as version 19.0.0. This means that version 19.0.0 is not cached and the interpreter is not allowed to fall back to importing the URL.\n\nHowever, we can override the default Prelude version by using dhall-to-nixpkgs to create a Dhall package for our desired Prelude:\n\n$ dhall-to-nixpkgs github https://github.com/dhall-lang/dhall-lang.git \\\n    --name Prelude \\\n    --directory Prelude \\\n    --rev v19.0.0 \\\n    > Prelude.nix\n\n\n… and then referencing that package in our Dhall overlay, by either overriding the Prelude globally for all packages, like this:\n\n  dhallOverrides = self: super: {\n    true = self.callPackage ./true.nix { };\n\n    Prelude = self.callPackage ./Prelude.nix { };\n  };\n\n\n… or selectively overriding the Prelude dependency for just the true package, like this:\n\n  dhallOverrides = self: super: {\n    true = self.callPackage ./true.nix {\n      Prelude = self.callPackage ./Prelude.nix { };\n    };\n  };\n\nOverrides \n\nYou can override any of the arguments to buildDhallGitHubPackage or buildDhallDirectoryPackage using the overridePackage attribute of a package. For example, suppose we wanted to selectively enable source = true just for the Prelude. We can do that like this:\n\n  dhallOverrides = self: super: {\n    Prelude = super.Prelude.overridePackage { source = true; };\n\n    …\n  };\n\nDotnet \nLocal Development Workflow\ndotnet-sdk vs dotnetCorePackages.sdk\ndotnetCorePackages.sdk vs dotnetCorePackages.runtime vs dotnetCorePackages.aspnetcore\nPackaging a Dotnet Application\nDotnet global tools\nLocal Development Workflow \n\nFor local development, it’s recommended to use nix-shell to create a dotnet environment:\n\n# shell.nix\nwith import <nixpkgs> {};\n\nmkShell {\n  name = \"dotnet-env\";\n  packages = [\n    dotnet-sdk\n  ];\n}\n\nUsing many sdks in a workflow\n\nIt’s very likely that more than one sdk will be needed on a given project. Dotnet provides several different frameworks (E.g dotnetcore, aspnetcore, etc.) as well as many versions for a given framework. Normally, dotnet is able to fetch a framework and install it relative to the executable. However, this would mean writing to the nix store in nixpkgs, which is read-only. To support the many-sdk use case, one can compose an environment using dotnetCorePackages.combinePackages:\n\nwith import <nixpkgs> {};\n\nmkShell {\n  name = \"dotnet-env\";\n  packages = [\n    (with dotnetCorePackages; combinePackages [\n      sdk_6_0\n      sdk_7_0\n    ])\n  ];\n}\n\n\nThis will produce a dotnet installation that has the dotnet 6.0 7.0 sdk. The first sdk listed will have it’s cli utility present in the resulting environment. Example info output:\n\n$ dotnet --info\n.NET SDK:\n Version:   7.0.202\n Commit:    6c74320bc3\n\nŚrodowisko uruchomieniowe:\n OS Name:     nixos\n OS Version:  23.05\n OS Platform: Linux\n RID:         linux-x64\n Base Path:   /nix/store/n2pm44xq20hz7ybsasgmd7p3yh31gnh4-dotnet-sdk-7.0.202/sdk/7.0.202/\n\nHost:\n  Version:      7.0.4\n  Architecture: x64\n  Commit:       0a396acafe\n\n.NET SDKs installed:\n  6.0.407 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/sdk]\n  7.0.202 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/sdk]\n\n.NET runtimes installed:\n  Microsoft.AspNetCore.App 6.0.15 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/shared/Microsoft.AspNetCore.App]\n  Microsoft.AspNetCore.App 7.0.4 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/shared/Microsoft.AspNetCore.App]\n  Microsoft.NETCore.App 6.0.15 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/shared/Microsoft.NETCore.App]\n  Microsoft.NETCore.App 7.0.4 [/nix/store/3b19303vwrhv0xxz1hg355c7f2hgxxgd-dotnet-core-combined/shared/Microsoft.NETCore.App]\n\nOther architectures found:\n  None\n\nEnvironment variables:\n  Not set\n\nglobal.json file:\n  Not found\n\nLearn more:\n  https://aka.ms/dotnet/info\n\nDownload .NET:\n  https://aka.ms/dotnet/download\n\ndotnet-sdk vs dotnetCorePackages.sdk \n\nThe dotnetCorePackages.sdk_X_Y is preferred over the old dotnet-sdk as both major and minor version are very important for a dotnet environment. If a given minor version isn’t present (or was changed), then this will likely break your ability to build a project.\n\ndotnetCorePackages.sdk vs dotnetCorePackages.runtime vs dotnetCorePackages.aspnetcore \n\nThe dotnetCorePackages.sdk contains both a runtime and the full sdk of a given version. The runtime and aspnetcore packages are meant to serve as minimal runtimes to deploy alongside already built applications.\n\nPackaging a Dotnet Application \n\nTo package Dotnet applications, you can use buildDotnetModule. This has similar arguments to stdenv.mkDerivation, with the following additions:\n\nprojectFile is used for specifying the dotnet project file, relative to the source root. These have .sln (entire solution) or .csproj (single project) file extensions. This can be a list of multiple projects as well. When omitted, will attempt to find and build the solution (.sln). If running into problems, make sure to set it to a file (or a list of files) with the .csproj extension - building applications as entire solutions is not fully supported by the .NET CLI.\n\nnugetDeps takes either a path to a deps.nix file, or a derivation. The deps.nix file can be generated using the script attached to passthru.fetch-deps. This file can also be generated manually using nuget-to-nix tool, which is available in nixpkgs. If the argument is a derivation, it will be used directly and assume it has the same output as mkNugetDeps.\n\npackNupkg is used to pack project as a nupkg, and installs it to $out/share. If set to true, the derivation can be used as a dependency for another dotnet project by adding it to projectReferences.\n\nprojectReferences can be used to resolve ProjectReference project items. Referenced projects can be packed with buildDotnetModule by setting the packNupkg = true attribute and passing a list of derivations to projectReferences. Since we are sharing referenced projects as NuGets they must be added to csproj/fsproj files as PackageReference as well. For example, your project has a local dependency:\n\n    <ProjectReference Include=\"../foo/bar.fsproj\" />\n\n\nTo enable discovery through projectReferences you would need to add:\n\n    <ProjectReference Include=\"../foo/bar.fsproj\" />\n    <PackageReference Include=\"bar\" Version=\"*\" Condition=\" '$(ContinuousIntegrationBuild)'=='true' \"/>\n\n\nexecutables is used to specify which executables get wrapped to $out/bin, relative to $out/lib/$pname. If this is unset, all executables generated will get installed. If you do not want to install any, set this to []. This gets done in the preFixup phase.\n\nruntimeDeps is used to wrap libraries into LD_LIBRARY_PATH. This is how dotnet usually handles runtime dependencies.\n\nbuildType is used to change the type of build. Possible values are Release, Debug, etc. By default, this is set to Release.\n\nselfContainedBuild allows to enable the self-contained build flag. By default, it is set to false and generated applications have a dependency on the selected dotnet runtime. If enabled, the dotnet runtime is bundled into the executable and the built app has no dependency on .NET.\n\nuseAppHost will enable creation of a binary executable that runs the .NET application using the specified root. More info in Microsoft docs. Enabled by default.\n\nuseDotnetFromEnv will change the binary wrapper so that it uses the .NET from the environment. The runtime specified by dotnet-runtime is given as a fallback in case no .NET is installed in the user’s environment. This is most useful for .NET global tools and LSP servers, which often extend the .NET CLI and their runtime should match the users’ .NET runtime.\n\ndotnet-sdk is useful in cases where you need to change what dotnet SDK is being used. You can also set this to the result of dotnetSdkPackages.combinePackages, if the project uses multiple SDKs to build.\n\ndotnet-runtime is useful in cases where you need to change what dotnet runtime is being used. This can be either a regular dotnet runtime, or an aspnetcore.\n\ndotnet-test-sdk is useful in cases where unit tests expect a different dotnet SDK. By default, this is set to the dotnet-sdk attribute.\n\ntestProjectFile is useful in cases where the regular project file does not contain the unit tests. It gets restored and build, but not installed. You may need to regenerate your nuget lockfile after setting this. Note that if set, only tests from this project are executed.\n\ndisabledTests is used to disable running specific unit tests. This gets passed as: dotnet test --filter \"FullyQualifiedName!={}\", to ensure compatibility with all unit test frameworks.\n\ndotnetRestoreFlags can be used to pass flags to dotnet restore.\n\ndotnetBuildFlags can be used to pass flags to dotnet build.\n\ndotnetTestFlags can be used to pass flags to dotnet test. Used only if doCheck is set to true.\n\ndotnetInstallFlags can be used to pass flags to dotnet install.\n\ndotnetPackFlags can be used to pass flags to dotnet pack. Used only if packNupkg is set to true.\n\ndotnetFlags can be used to pass flags to all of the above phases.\n\nWhen packaging a new application, you need to fetch its dependencies. Create an empty deps.nix, set nugetDeps = ./deps.nix, then run nix-build -A package.fetch-deps to generate a script that will build the lockfile for you.\n\nHere is an example default.nix, using some of the previously discussed arguments:\n\n{ lib, buildDotnetModule, dotnetCorePackages, ffmpeg }:\n\nlet\n  referencedProject = import ../../bar { ... };\nin buildDotnetModule rec {\n  pname = \"someDotnetApplication\";\n  version = \"0.1\";\n\n  src = ./.;\n\n  projectFile = \"src/project.sln\";\n  nugetDeps = ./deps.nix; # File generated with `nix-build -A package.passthru.fetch-deps`.\n\n  projectReferences = [ referencedProject ]; # `referencedProject` must contain `nupkg` in the folder structure.\n\n  dotnet-sdk = dotnetCorePackages.sdk_6.0;\n  dotnet-runtime = dotnetCorePackages.runtime_6_0;\n\n  executables = [ \"foo\" ]; # This wraps \"$out/lib/$pname/foo\" to `$out/bin/foo`.\n  executables = []; # Don't install any executables.\n\n  packNupkg = true; # This packs the project as \"foo-0.1.nupkg\" at `$out/share`.\n\n  runtimeDeps = [ ffmpeg ]; # This will wrap ffmpeg's library path into `LD_LIBRARY_PATH`.\n}\n\nDotnet global tools \n\n.NET Global tools are a mechanism provided by the dotnet CLI to install .NET binaries from Nuget packages.\n\nThey can be installed either as a global tool for the entire system, or as a local tool specific to project.\n\nThe local installation is the easiest and works on NixOS in the same way as on other Linux distributions. See dotnet documentation to learn more.\n\nThe global installation method should also work most of the time. You have to remember to update the PATH value to the location the tools are installed to (the CLI will inform you about it during installation) and also set the DOTNET_ROOT value, so that the tool can find the .NET SDK package. You can find the path to the SDK by running nix eval --raw nixpkgs#dotnet-sdk (substitute the dotnet-sdk package for another if a different SDK version is needed).\n\nThis method is not recommended on NixOS, since it’s not declarative and involves installing binaries not made for NixOS, which will not always work.\n\nThe third, and preferred way, is packaging the tool into a Nix derivation.\n\nPackaging Dotnet global tools\n\nDotnet global tools are standard .NET binaries, just made available through a special NuGet package. Therefore, they can be built and packaged like every .NET application, using buildDotnetModule.\n\nIf however the source is not available or difficult to build, the buildDotnetGlobalTool helper can be used, which will package the tool straight from its NuGet package.\n\nThis helper has the same arguments as buildDotnetModule, with a few differences:\n\npname and version are required, and will be used to find the NuGet package of the tool\n\nnugetName can be used to override the NuGet package name that will be downloaded, if it’s different from pname\n\nnugetSha256 is the hash of the fetched NuGet package. Set this to lib.fakeHash256 for the first build, and it will error out, giving you the proper hash. Also remember to update it during version updates (it will not error out if you just change the version while having a fetched package in /nix/store)\n\ndotnet-runtime is set to dotnet-sdk by default. When changing this, remember that .NET tools fetched from NuGet require an SDK.\n\nHere is an example of packaging pbm, an unfree binary without source available:\n\n{ buildDotnetGlobalTool, lib }:\n\nbuildDotnetGlobalTool {\n  pname = \"pbm\";\n  version = \"1.3.1\";\n\n  nugetSha256 = \"sha256-ZG2HFyKYhVNVYd2kRlkbAjZJq88OADe3yjxmLuxXDUo=\";\n\n  meta = with lib; {\n    homepage = \"https://cmd.petabridge.com/index.html\";\n    changelog = \"https://cmd.petabridge.com/articles/RELEASE_NOTES.html\";\n    license = licenses.unfree;\n    platforms = platforms.linux;\n  };\n}\n\n\nWhen packaging a new .NET application in nixpkgs, you can tag the @NixOS/dotnet team for help and code review.\n\nEmscripten \nExamples\nDebugging\n\nEmscripten: An LLVM-to-JavaScript Compiler\n\nIf you want to work with emcc, emconfigure and emmake as you are used to from Ubuntu and similar distributions,\n\nnix-shell -p emscripten\n\n\nA few things to note:\n\nexport EMCC_DEBUG=2 is nice for debugging\n\nThe build artifact cache in ~/.emscripten sometimes creates issues and needs to be removed from time to time\n\nExamples \n\nLet’s see two different examples from pkgs/top-level/emscripten-packages.nix:\n\npkgs.zlib.override\n\npkgs.buildEmscriptenPackage\n\nA special requirement of the pkgs.buildEmscriptenPackage is the doCheck = true. This means each Emscripten package requires that a checkPhase is implemented.\n\nUse export EMCC_DEBUG=2 from within a phase to get more detailed debug output what is going wrong.\n\nThe cache at ~/.emscripten requires to set HOME=$TMPDIR in individual phases. This makes compilation slower but also more deterministic.\n\nExample 224. Using pkgs.zlib.override {}\n\nThis example uses zlib from Nixpkgs, but instead of compiling C to ELF it compiles C to JavaScript since we were using pkgs.zlib.override and changed stdenv to pkgs.emscriptenStdenv.\n\nA few adaptions and hacks were put in place to make it work. One advantage is that when pkgs.zlib is updated, it will automatically update this package as well.\n\n(pkgs.zlib.override {\n  stdenv = pkgs.emscriptenStdenv;\n}).overrideAttrs\n(old: rec {\n  buildInputs = old.buildInputs ++ [ pkg-config ];\n  # we need to reset this setting!\n  env = (old.env or { }) // { NIX_CFLAGS_COMPILE = \"\"; };\n  configurePhase = ''\n    # FIXME: Some tests require writing at $HOME\n    HOME=$TMPDIR\n    runHook preConfigure\n\n    #export EMCC_DEBUG=2\n    emconfigure ./configure --prefix=$out --shared\n\n    runHook postConfigure\n  '';\n  dontStrip = true;\n  outputs = [ \"out\" ];\n  buildPhase = ''\n    emmake make\n  '';\n  installPhase = ''\n    emmake make install\n  '';\n  checkPhase = ''\n    echo \"================= testing zlib using node =================\"\n\n    echo \"Compiling a custom test\"\n    set -x\n    emcc -O2 -s EMULATE_FUNCTION_POINTER_CASTS=1 test/example.c -DZ_SOLO \\\n    libz.so.${old.version} -I . -o example.js\n\n    echo \"Using node to execute the test\"\n    ${pkgs.nodejs}/bin/node ./example.js\n\n    set +x\n    if [ $? -ne 0 ]; then\n      echo \"test failed for some reason\"\n      exit 1;\n    else\n      echo \"it seems to work! very good.\"\n    fi\n    echo \"================= /testing zlib using node =================\"\n  '';\n\n  postPatch = pkgs.lib.optionalString pkgs.stdenv.isDarwin ''\n    substituteInPlace configure \\\n      --replace '/usr/bin/libtool' 'ar' \\\n      --replace 'AR=\"libtool\"' 'AR=\"ar\"' \\\n      --replace 'ARFLAGS=\"-o\"' 'ARFLAGS=\"-r\"'\n  '';\n})\n\n\nExample 225. Using pkgs.buildEmscriptenPackage {}\n\nThis xmlmirror example features an Emscripten package that is defined completely from this context and no pkgs.zlib.override is used.\n\npkgs.buildEmscriptenPackage rec {\n  name = \"xmlmirror\";\n\n  buildInputs = [ pkg-config autoconf automake libtool gnumake libxml2 nodejs openjdk json_c ];\n  nativeBuildInputs = [ pkg-config zlib ];\n\n  src = pkgs.fetchgit {\n    url = \"https://gitlab.com/odfplugfest/xmlmirror.git\";\n    rev = \"4fd7e86f7c9526b8f4c1733e5c8b45175860a8fd\";\n    hash = \"sha256-i+QgY+5PYVg5pwhzcDnkfXAznBg3e8sWH2jZtixuWsk=\";\n  };\n\n  configurePhase = ''\n    rm -f fastXmlLint.js*\n    # a fix for ERROR:root:For asm.js, TOTAL_MEMORY must be a multiple of 16MB, was 234217728\n    # https://gitlab.com/odfplugfest/xmlmirror/issues/8\n    sed -e \"s/TOTAL_MEMORY=234217728/TOTAL_MEMORY=268435456/g\" -i Makefile.emEnv\n    # https://github.com/kripken/emscripten/issues/6344\n    # https://gitlab.com/odfplugfest/xmlmirror/issues/9\n    sed -e \"s/\\$(JSONC_LDFLAGS) \\$(ZLIB_LDFLAGS) \\$(LIBXML20_LDFLAGS)/\\$(JSONC_LDFLAGS) \\$(LIBXML20_LDFLAGS) \\$(ZLIB_LDFLAGS) /g\" -i Makefile.emEnv\n    # https://gitlab.com/odfplugfest/xmlmirror/issues/11\n    sed -e \"s/-o fastXmlLint.js/-s EXTRA_EXPORTED_RUNTIME_METHODS='[\\\"ccall\\\", \\\"cwrap\\\"]' -o fastXmlLint.js/g\" -i Makefile.emEnv\n  '';\n\n  buildPhase = ''\n    HOME=$TMPDIR\n    make -f Makefile.emEnv\n  '';\n\n  outputs = [ \"out\" \"doc\" ];\n\n  installPhase = ''\n    mkdir -p $out/share\n    mkdir -p $doc/share/${name}\n\n    cp Demo* $out/share\n    cp -R codemirror-5.12 $out/share\n    cp fastXmlLint.js* $out/share\n    cp *.xsd $out/share\n    cp *.js $out/share\n    cp *.xhtml $out/share\n    cp *.html $out/share\n    cp *.json $out/share\n    cp *.rng $out/share\n    cp README.md $doc/share/${name}\n  '';\n  checkPhase = ''\n\n  '';\n}\n\n\n\n\n\nDebugging \n\nUse nix-shell -I nixpkgs=/some/dir/nixpkgs -A emscriptenPackages.libz and from there you can go trough the individual steps. This makes it easy to build a good unit test or list the files of the project.\n\nnix-shell -I nixpkgs=/some/dir/nixpkgs -A emscriptenPackages.libz\n\ncd /tmp/\n\nunpackPhase\n\ncd libz-1.2.3\n\nconfigurePhase\n\nbuildPhase\n\n… happy hacking…\n\nGNOME \nPackaging GNOME applications\nOnto wrapGAppsHook\nUpdating GNOME packages\nFrequently encountered issues\nPackaging GNOME applications \n\nPrograms in the GNOME universe are written in various languages but they all use GObject-based libraries like GLib, GTK or GStreamer. These libraries are often modular, relying on looking into certain directories to find their modules. However, due to Nix’s specific file system organization, this will fail without our intervention. Fortunately, the libraries usually allow overriding the directories through environment variables, either natively or thanks to a patch in nixpkgs. Wrapping the executables to ensure correct paths are available to the application constitutes a significant part of packaging a modern desktop application. In this section, we will describe various modules needed by such applications, environment variables needed to make the modules load, and finally a script that will do the work for us.\n\nSettings\n\nGSettings API is often used for storing settings. GSettings schemas are required, to know the type and other metadata of the stored values. GLib looks for glib-2.0/schemas/gschemas.compiled files inside the directories of XDG_DATA_DIRS.\n\nOn Linux, GSettings API is implemented using dconf backend. You will need to add dconf GIO module to GIO_EXTRA_MODULES variable, otherwise the memory backend will be used and the saved settings will not be persistent.\n\nLast you will need the dconf database D-Bus service itself. You can enable it using programs.dconf.enable.\n\nSome applications will also require gsettings-desktop-schemas for things like reading proxy configuration or user interface customization. This dependency is often not mentioned by upstream, you should grep for org.gnome.desktop and org.gnome.system to see if the schemas are needed.\n\nGIO modules\n\nGLib’s GIO library supports several extension points. Notably, they allow:\n\nimplementing settings backends (already mentioned)\n\nadding TLS support\n\nproxy settings\n\nvirtual file systems\n\nThe modules are typically installed to lib/gio/modules/ directory of a package and you need to add them to GIO_EXTRA_MODULES if you need any of those features.\n\nIn particular, we recommend:\n\nadding dconf.lib for any software on Linux that reads GSettings (even transitively through e.g. GTK’s file manager)\n\nadding glib-networking for any software that accesses network using GIO or libsoup – glib-networking contains a module that implements TLS support and loads system-wide proxy settings\n\nTo allow software to use various virtual file systems, gvfs package can be also added. But that is usually an optional feature so we typically use gvfs from the system (e.g. installed globally using NixOS module).\n\nGdkPixbuf loaders\n\nGTK applications typically use GdkPixbuf to load images. But gdk-pixbuf package only supports basic bitmap formats like JPEG, PNG or TIFF, requiring to use third-party loader modules for other formats. This is especially painful since GTK itself includes SVG icons, which cannot be rendered without a loader provided by librsvg.\n\nUnlike other libraries mentioned in this section, GdkPixbuf only supports a single value in its controlling environment variable GDK_PIXBUF_MODULE_FILE. It is supposed to point to a cache file containing information about the available loaders. Each loader package will contain a lib/gdk-pixbuf-2.0/2.10.0/loaders.cache file describing the default loaders in gdk-pixbuf package plus the loader contained in the package itself. If you want to use multiple third-party loaders, you will need to create your own cache file manually. Fortunately, this is pretty rare as not many loaders exist.\n\ngdk-pixbuf contains a setup hook that sets GDK_PIXBUF_MODULE_FILE from dependencies but as mentioned in further section, it is pretty limited. Loaders should propagate this setup hook.\n\nIcons\n\nWhen an application uses icons, an icon theme should be available in XDG_DATA_DIRS during runtime. The package for the default, icon-less hicolor-icon-theme (should be propagated by every icon theme) contains a setup hook that will pick up icon themes from buildInputs and add their datadirs to XDG_ICON_DIRS environment variable (this is Nixpkgs specific, not actually a XDG standard variable). Unfortunately, relying on that would mean every user has to download the theme included in the package expression no matter their preference. For that reason, we leave the installation of icon theme on the user. If you use one of the desktop environments, you probably already have an icon theme installed.\n\nIn the rare case you need to use icons from dependencies (e.g. when an app forces an icon theme), you can use the following to pick them up:\n\n  buildInputs = [\n    pantheon.elementary-icon-theme\n  ];\n  preFixup = ''\n    gappsWrapperArgs+=(\n      # The icon theme is hardcoded.\n      --prefix XDG_DATA_DIRS : \"$XDG_ICON_DIRS\"\n    )\n  '';\n\n\nTo avoid costly file system access when locating icons, GTK, as well as Qt, can rely on icon-theme.cache files from the themes’ top-level directories. These files are generated using gtk-update-icon-cache, which is expected to be run whenever an icon is added or removed to an icon theme (typically an application icon into hicolor theme) and some programs do indeed run this after icon installation. However, since packages are installed into their own prefix by Nix, this would lead to conflicts. For that reason, gtk3 provides a setup hook that will clean the file from installation. Since most applications only ship their own icon that will be loaded on start-up, it should not affect them too much. On the other hand, icon themes are much larger and more widely used so we need to cache them. Because we recommend installing icon themes globally, we will generate the cache files from all packages in a profile using a NixOS module. You can enable the cache generation using gtk.iconCache.enable option if your desktop environment does not already do that.\n\nPackaging icon themes\n\nIcon themes may inherit from other icon themes. The inheritance is specified using the Inherits key in the index.theme file distributed with the icon theme. According to the icon theme specification, icons not provided by the theme are looked for in its parent icon themes. Therefore the parent themes should be installed as dependencies for a more complete experience regarding the icon sets used.\n\nThe package hicolor-icon-theme provides a setup hook which makes symbolic links for the parent themes into the directory share/icons of the current theme directory in the nix store, making sure they can be found at runtime. For that to work the packages providing parent icon themes should be listed as propagated build dependencies, together with hicolor-icon-theme.\n\nAlso make sure that icon-theme.cache is installed for each theme provided by the package, and set dontDropIconThemeCache to true so that the cache file is not removed by the gtk3 setup hook.\n\nGTK Themes\n\nPreviously, a GTK theme needed to be in XDG_DATA_DIRS. This is no longer necessary for most programs since GTK incorporated Adwaita theme. Some programs (for example, those designed for elementary HIG) might require a special theme like pantheon.elementary-gtk-theme.\n\nGObject introspection typelibs\n\nGObject introspection allows applications to use C libraries in other languages easily. It does this through typelib files searched in GI_TYPELIB_PATH.\n\nVarious plug-ins\n\nIf your application uses GStreamer or Grilo, you should set GST_PLUGIN_SYSTEM_PATH_1_0 and GRL_PLUGIN_PATH, respectively.\n\nOnto wrapGAppsHook \n\nGiven the requirements above, the package expression would become messy quickly:\n\npreFixup = ''\n  for f in $(find $out/bin/ $out/libexec/ -type f -executable); do\n    wrapProgram \"$f\" \\\n      --prefix GIO_EXTRA_MODULES : \"${getLib dconf}/lib/gio/modules\" \\\n      --prefix XDG_DATA_DIRS : \"$out/share\" \\\n      --prefix XDG_DATA_DIRS : \"$out/share/gsettings-schemas/${name}\" \\\n      --prefix XDG_DATA_DIRS : \"${gsettings-desktop-schemas}/share/gsettings-schemas/${gsettings-desktop-schemas.name}\" \\\n      --prefix XDG_DATA_DIRS : \"${hicolor-icon-theme}/share\" \\\n      --prefix GI_TYPELIB_PATH : \"${lib.makeSearchPath \"lib/girepository-1.0\" [ pango json-glib ]}\"\n  done\n'';\n\n\nFortunately, there is wrapGAppsHook. It works in conjunction with other setup hooks that populate environment variables, and it will then wrap all executables in bin and libexec directories using said variables.\n\nFor convenience, it also adds dconf.lib for a GIO module implementing a GSettings backend using dconf, gtk3 for GSettings schemas, and librsvg for GdkPixbuf loader to the closure. There is also wrapGAppsHook4, which replaces GTK 3 with GTK 4. And in case you are packaging a program without a graphical interface, you might want to use wrapGAppsNoGuiHook, which runs the same script as wrapGAppsHook but does not bring gtk3 and librsvg into the closure.\n\nwrapGAppsHook itself will add the package’s share directory to XDG_DATA_DIRS.\n\nglib setup hook will populate GSETTINGS_SCHEMAS_PATH and then wrapGAppsHook will prepend it to XDG_DATA_DIRS.\n\ngdk-pixbuf setup hook will populate GDK_PIXBUF_MODULE_FILE with the path to biggest loaders.cache file from the dependencies containing GdkPixbuf loaders. This works fine when there are only two packages containing loaders (gdk-pixbuf and e.g. librsvg) – it will choose the second one, reasonably expecting that it will be bigger since it describes extra loader in addition to the default ones. But when there are more than two loader packages, this logic will break. One possible solution would be constructing a custom cache file for each package containing a program like services/x11/gdk-pixbuf.nix NixOS module does. wrapGAppsHook copies the GDK_PIXBUF_MODULE_FILE environment variable into the produced wrapper.\n\nOne of gtk3’s setup hooks will remove icon-theme.cache files from package’s icon theme directories to avoid conflicts. Icon theme packages should prevent this with dontDropIconThemeCache = true;.\n\ndconf.lib is a dependency of wrapGAppsHook, which then also adds it to the GIO_EXTRA_MODULES variable.\n\nhicolor-icon-theme’s setup hook will add icon themes to XDG_ICON_DIRS.\n\ngobject-introspection setup hook populates GI_TYPELIB_PATH variable with lib/girepository-1.0 directories of dependencies, which is then added to wrapper by wrapGAppsHook. It also adds share directories of dependencies to XDG_DATA_DIRS, which is intended to promote GIR files but it also pollutes the closures of packages using wrapGAppsHook.\n\nSetup hooks of gst_all_1.gstreamer and grilo will populate the GST_PLUGIN_SYSTEM_PATH_1_0 and GRL_PLUGIN_PATH variables, respectively, which will then be added to the wrapper by wrapGAppsHook.\n\nYou can also pass additional arguments to makeWrapper using gappsWrapperArgs in preFixup hook:\n\npreFixup = ''\n  gappsWrapperArgs+=(\n    # Thumbnailers\n    --prefix XDG_DATA_DIRS : \"${gdk-pixbuf}/share\"\n    --prefix XDG_DATA_DIRS : \"${librsvg}/share\"\n    --prefix XDG_DATA_DIRS : \"${shared-mime-info}/share\"\n  )\n'';\n\nUpdating GNOME packages \n\nMost GNOME package offer updateScript, it is therefore possible to update to latest source tarball by running nix-shell maintainers/scripts/update.nix --argstr package gnome.nautilus or even en masse with nix-shell maintainers/scripts/update.nix --argstr path gnome. Read the package’s NEWS file to see what changed.\n\nFrequently encountered issues \nGLib-GIO-ERROR **: 06:04:50.903: No GSettings schemas are installed on the system\n\nThere are no schemas available in XDG_DATA_DIRS. Temporarily add a random package containing schemas like gsettings-desktop-schemas to buildInputs. glib and wrapGAppsHook setup hooks will take care of making the schemas available to application and you will see the actual missing schemas with the next error. Or you can try looking through the source code for the actual schemas used.\n\nGLib-GIO-ERROR **: 06:04:50.903: Settings schema ‘org.gnome.foo’ is not installed\n\nPackage is missing some GSettings schemas. You can find out the package containing the schema with nix-locate org.gnome.foo.gschema.xml and let the hooks handle the wrapping as above.\n\nWhen using wrapGAppsHook with special derivers you can end up with double wrapped binaries.\n\nThis is because derivers like python.pkgs.buildPythonApplication or qt5.mkDerivation have setup-hooks automatically added that produce wrappers with makeWrapper. The simplest way to workaround that is to disable the wrapGAppsHook automatic wrapping with dontWrapGApps = true; and pass the arguments it intended to pass to makeWrapper to another.\n\nIn the case of a Python application it could look like:\n\npython3.pkgs.buildPythonApplication {\n  pname = \"gnome-music\";\n  version = \"3.32.2\";\n\n  nativeBuildInputs = [\n    wrapGAppsHook\n    gobject-introspection\n    ...\n  ];\n\n  dontWrapGApps = true;\n\n  # Arguments to be passed to `makeWrapper`, only used by buildPython*\n  preFixup = ''\n    makeWrapperArgs+=(\"''${gappsWrapperArgs[@]}\")\n  '';\n}\n\n\nAnd for a QT app like:\n\nmkDerivation {\n  pname = \"calibre\";\n  version = \"3.47.0\";\n\n  nativeBuildInputs = [\n    wrapGAppsHook\n    qmake\n    ...\n  ];\n\n  dontWrapGApps = true;\n\n  # Arguments to be passed to `makeWrapper`, only used by qt5’s mkDerivation\n  preFixup = ''\n    qtWrapperArgs+=(\"''${gappsWrapperArgs[@]}\")\n  '';\n}\n\nI am packaging a project that cannot be wrapped, like a library or GNOME Shell extension.\n\nYou can rely on applications depending on the library setting the necessary environment variables but that is often easy to miss. Instead we recommend to patch the paths in the source code whenever possible. Here are some examples:\n\nReplacing a GI_TYPELIB_PATH in GNOME Shell extension – we are using substituteAll to include the path to a typelib into a patch.\n\nThe following examples are hardcoding GSettings schema paths. To get the schema paths we use the functions\n\nglib.getSchemaPath Takes a nix package attribute as an argument.\n\nglib.makeSchemaPath Takes a package output like $out and a derivation name. You should use this if the schemas you need to hardcode are in the same derivation.\n\nHard-coding GSettings schema path in Vala plug-in (dynamically loaded library) – here, substituteAll cannot be used since the schema comes from the same package preventing us from pass its path to the function, probably due to a Nix bug.\n\nHard-coding GSettings schema path in C library – nothing special other than using Coccinelle patch to generate the patch itself.\n\nI need to wrap a binary outside bin and libexec directories.\n\nYou can manually trigger the wrapping with wrapGApp in preFixup phase. It takes a path to a program as a first argument; the remaining arguments are passed directly to wrapProgram function.\n\nGo \nGo modules\nbuildGoPackage (legacy)\nAttributes used by the builders\nGo modules \n\nThe function buildGoModule builds Go programs managed with Go modules. It builds a Go Modules through a two phase build:\n\nAn intermediate fetcher derivation. This derivation will be used to fetch all of the dependencies of the Go module.\n\nA final derivation will use the output of the intermediate derivation to build the binaries and produce the final output.\n\nExample for buildGoModule\n\nIn the following is an example expression using buildGoModule, the following arguments are of special significance to the function:\n\nvendorHash: is the hash of the output of the intermediate fetcher derivation.\n\nvendorHash can also be set to null. In that case, rather than fetching the dependencies and vendoring them, the dependencies vendored in the source repo will be used.\n\nTo avoid updating this field when dependencies change, run go mod vendor in your source repo and set vendorHash = null;\n\nTo obtain the actual hash, set vendorHash = lib.fakeHash; and run the build (more details here).\n\nproxyVendor: Fetches (go mod download) and proxies the vendor directory. This is useful if your code depends on c code and go mod tidy does not include the needed sources to build or if any dependency has case-insensitive conflicts which will produce platform-dependent vendorHash checksums.\n\nmodPostBuild: Shell commands to run after the build of the goModules executes go mod vendor, and before calculating fixed output derivation’s vendorHash. Note that if you change this attribute, you need to update vendorHash attribute.\n\npet = buildGoModule rec {\n  pname = \"pet\";\n  version = \"0.3.4\";\n\n  src = fetchFromGitHub {\n    owner = \"knqyf263\";\n    repo = \"pet\";\n    rev = \"v${version}\";\n    hash = \"sha256-Gjw1dRrgM8D3G7v6WIM2+50r4HmTXvx0Xxme2fH9TlQ=\";\n  };\n\n  vendorHash = \"sha256-ciBIR+a1oaYH+H1PcC8cD8ncfJczk1IiJ8iYNM+R6aA=\";\n\n  meta = with lib; {\n    description = \"Simple command-line snippet manager, written in Go\";\n    homepage = \"https://github.com/knqyf263/pet\";\n    license = licenses.mit;\n    maintainers = with maintainers; [ kalbasit ];\n  };\n}\n\nbuildGoPackage (legacy) \n\nThe function buildGoPackage builds legacy Go programs, not supporting Go modules.\n\nExample for buildGoPackage\n\nIn the following is an example expression using buildGoPackage, the following arguments are of special significance to the function:\n\ngoPackagePath specifies the package’s canonical Go import path.\n\ngoDeps is where the Go dependencies of a Go program are listed as a list of package source identified by Go import path. It could be imported as a separate deps.nix file for readability. The dependency data structure is described below.\n\ndeis = buildGoPackage rec {\n  pname = \"deis\";\n  version = \"1.13.0\";\n\n  goPackagePath = \"github.com/deis/deis\";\n\n  src = fetchFromGitHub {\n    owner = \"deis\";\n    repo = \"deis\";\n    rev = \"v${version}\";\n    hash = \"sha256-XCPD4LNWtAd8uz7zyCLRfT8rzxycIUmTACjU03GnaeM=\";\n  };\n\n  goDeps = ./deps.nix;\n}\n\n\nThe goDeps attribute can be imported from a separate nix file that defines which Go libraries are needed and should be included in GOPATH for buildPhase:\n\n# deps.nix\n[ # goDeps is a list of Go dependencies.\n  {\n    # goPackagePath specifies Go package import path.\n    goPackagePath = \"gopkg.in/yaml.v2\";\n    fetch = {\n      # `fetch type` that needs to be used to get package source.\n      # If `git` is used there should be `url`, `rev` and `hash` defined next to it.\n      type = \"git\";\n      url = \"https://gopkg.in/yaml.v2\";\n      rev = \"a83829b6f1293c91addabc89d0571c246397bbf4\";\n      hash = \"sha256-EMrdy0M0tNuOcITaTAmT5/dPSKPXwHDKCXFpkGbVjdQ=\";\n    };\n  }\n  {\n    goPackagePath = \"github.com/docopt/docopt-go\";\n    fetch = {\n      type = \"git\";\n      url = \"https://github.com/docopt/docopt-go\";\n      rev = \"784ddc588536785e7299f7272f39101f7faccc3f\";\n      hash = \"sha256-Uo89zjE+v3R7zzOq/gbQOHj3SMYt2W1nDHS7RCUin3M=\";\n    };\n  }\n]\n\n\nTo extract dependency information from a Go package in automated way use go2nix. It can produce complete derivation and goDeps file for Go programs.\n\nYou may use Go packages installed into the active Nix profiles by adding the following to your ~/.bashrc:\n\nfor p in $NIX_PROFILES; do\n    GOPATH=\"$p/share/go:$GOPATH\"\ndone\n\nAttributes used by the builders \n\nMany attributes controlling the build phase are respected by both buildGoModule and buildGoPackage. Note that buildGoModule reads the following attributes also when building the vendor/ goModules fixed output derivation as well:\n\nsourceRoot\n\nprePatch\n\npatches\n\npatchFlags\n\npostPatch\n\npreBuild\n\nIn addition to the above attributes, and the many more variables respected also by stdenv.mkDerivation, both buildGoModule and buildGoPackage respect Go-specific attributes that tweak them to behave slightly differently:\n\nldflags\n\nArguments to pass to the Go linker tool via the -ldflags argument of go build. The most common use case for this argument is to make the resulting executable aware of its own version. For example:\n\n  ldflags = [\n    \"-s\" \"-w\"\n    \"-X main.Version=${version}\"\n    \"-X main.Commit=${version}\"\n  ];\n\ntags\n\nArguments to pass to the Go via the -tags argument of go build. For example:\n\n  tags = [\n    \"production\"\n    \"sqlite\"\n  ];\n\n  tags = [ \"production\" ] ++ lib.optionals withSqlite [ \"sqlite\" ];\n\ndeleteVendor\n\nRemoves the pre-existing vendor directory. This should only be used if the dependencies included in the vendor folder are broken or incomplete.\n\nsubPackages\n\nSpecified as a string or list of strings. Limits the builder from building child packages that have not been listed. If subPackages is not specified, all child packages will be built.\n\nexcludedPackages\n\nSpecified as a string or list of strings. Causes the builder to skip building child packages that match any of the provided values. If excludedPackages is not specified, all child packages will be built.\n\nHaskell \nAvailable packages\nhaskellPackages.mkDerivation\nDevelopment environments\nOverriding Haskell packages\nF.A.Q.\n\nThe Haskell infrastructure in Nixpkgs has two main purposes: The primary purpose is to provide a Haskell compiler and build tools as well as infrastructure for packaging Haskell-based packages.\n\nThe secondary purpose is to provide support for Haskell development environments including prebuilt Haskell libraries. However, in this area sacrifices have been made due to self-imposed restrictions in Nixpkgs, to lessen the maintenance effort and to improve performance. (More details in the subsection Limitations.)\n\nAvailable packages \n\nThe compiler and most build tools are exposed at the top level:\n\nghc is the default version of GHC\n\nLanguage specific tools: cabal-install, stack, hpack, …\n\nMany “normal” user facing packages written in Haskell, like niv or cachix, are also exposed at the top level, and there is nothing Haskell specific to installing and using them.\n\nAll of these packages are originally defined in the haskellPackages package set and are re-exposed with a reduced dependency closure for convenience. (see justStaticExecutables or separateBinOutput below)\n\nThe haskellPackages set includes at least one version of every package from Hackage as well as some manually injected packages. This amounts to a lot of packages, so it is hidden from nix-env -qa by default for performance reasons. You can still list all packages in the set like this:\n\n$ nix-env -f '<nixpkgs>' -qaP -A haskellPackages\nhaskellPackages.a50                                                         a50-0.5\nhaskellPackages.AAI                                                         AAI-0.2.0.1\nhaskellPackages.aasam                                                       aasam-0.2.0.0\nhaskellPackages.abacate                                                     abacate-0.0.0.0\nhaskellPackages.abc-puzzle                                                  abc-puzzle-0.2.1\n…\n\n\nAlso, the haskellPackages set is included on search.nixos.org.\n\nThe attribute names in haskellPackages always correspond with their name on Hackage. Since Hackage allows names that are not valid Nix without escaping, you need to take care when handling attribute names like 3dmodels.\n\nFor packages that are part of Stackage (a curated set of known to be compatible packages), we use the version prescribed by a Stackage snapshot (usually the current LTS one) as the default version. For all other packages we use the latest version from Hackage (the repository of basically all open source Haskell packages). See [below](#haskell-available- versions) for a few more details on this.\n\nRoughly half of the 16K packages contained in haskellPackages don’t actually build and are marked as broken semi-automatically. Most of those packages are deprecated or unmaintained, but sometimes packages that should build, do not build. Very often fixing them is not a lot of work.\n\nhaskellPackages is built with our default compiler, but we also provide other releases of GHC and package sets built with them. You can list all available compilers like this:\n\n$ nix-env -f '<nixpkgs>' -qaP -A haskell.compiler\nhaskell.compiler.ghc810                  ghc-8.10.7\nhaskell.compiler.ghc88                   ghc-8.8.4\nhaskell.compiler.ghc90                   ghc-9.0.2\nhaskell.compiler.ghc924                  ghc-9.2.4\nhaskell.compiler.ghc925                  ghc-9.2.5\nhaskell.compiler.ghc926                  ghc-9.2.6\nhaskell.compiler.ghc92                   ghc-9.2.7\nhaskell.compiler.ghc942                  ghc-9.4.2\nhaskell.compiler.ghc943                  ghc-9.4.3\nhaskell.compiler.ghc94                   ghc-9.4.4\nhaskell.compiler.ghcHEAD                 ghc-9.7.20221224\nhaskell.compiler.ghc8102Binary           ghc-binary-8.10.2\nhaskell.compiler.ghc8102BinaryMinimal    ghc-binary-8.10.2\nhaskell.compiler.ghc8107BinaryMinimal    ghc-binary-8.10.7\nhaskell.compiler.ghc8107Binary           ghc-binary-8.10.7\nhaskell.compiler.ghc865Binary            ghc-binary-8.6.5\nhaskell.compiler.ghc924Binary            ghc-binary-9.2.4\nhaskell.compiler.ghc924BinaryMinimal     ghc-binary-9.2.4\nhaskell.compiler.integer-simple.ghc810   ghc-integer-simple-8.10.7\nhaskell.compiler.integer-simple.ghc8107  ghc-integer-simple-8.10.7\nhaskell.compiler.integer-simple.ghc88    ghc-integer-simple-8.8.4\nhaskell.compiler.integer-simple.ghc884   ghc-integer-simple-8.8.4\nhaskell.compiler.native-bignum.ghc90     ghc-native-bignum-9.0.2\nhaskell.compiler.native-bignum.ghc902    ghc-native-bignum-9.0.2\nhaskell.compiler.native-bignum.ghc924    ghc-native-bignum-9.2.4\nhaskell.compiler.native-bignum.ghc925    ghc-native-bignum-9.2.5\nhaskell.compiler.native-bignum.ghc926    ghc-native-bignum-9.2.6\nhaskell.compiler.native-bignum.ghc92     ghc-native-bignum-9.2.7\nhaskell.compiler.native-bignum.ghc927    ghc-native-bignum-9.2.7\nhaskell.compiler.native-bignum.ghc942    ghc-native-bignum-9.4.2\nhaskell.compiler.native-bignum.ghc943    ghc-native-bignum-9.4.3\nhaskell.compiler.native-bignum.ghc94     ghc-native-bignum-9.4.4\nhaskell.compiler.native-bignum.ghc944    ghc-native-bignum-9.4.4\nhaskell.compiler.native-bignum.ghcHEAD   ghc-native-bignum-9.7.20221224\nhaskell.compiler.ghcjs                   ghcjs-8.10.7\n\n\nEach of those compiler versions has a corresponding attribute set built using it. However, the non-standard package sets are not tested regularly and, as a result, contain fewer working packages. The corresponding package set for GHC 9.4.5 is haskell.packages.ghc945. In fact haskellPackages is just an alias for haskell.packages.ghc927:\n\n$ nix-env -f '<nixpkgs>' -qaP -A haskell.packages.ghc927\nhaskell.packages.ghc927.a50                                                         a50-0.5\nhaskell.packages.ghc927.AAI                                                         AAI-0.2.0.1\nhaskell.packages.ghc927.aasam                                                       aasam-0.2.0.0\nhaskell.packages.ghc927.abacate                                                     abacate-0.0.0.0\nhaskell.packages.ghc927.abc-puzzle                                                  abc-puzzle-0.2.1\n…\n\n\nEvery package set also re-exposes the GHC used to build its packages as haskell.packages.*.ghc.\n\nAvailable package versions\n\nWe aim for a “blessed” package set which only contains one version of each package, like Stackage, which is a curated set of known to be compatible packages. We use the version information from Stackage snapshots and extend it with more packages. Normally in Nixpkgs the number of building Haskell packages is roughly two to three times the size of Stackage. For choosing the version to use for a certain package we use the following rules:\n\nBy default, for haskellPackages.foo is the newest version of the package foo found on Hackage, which is the central registry of all open source Haskell packages. Nixpkgs contains a reference to a pinned Hackage snapshot, thus we use the state of Hackage as of the last time we updated this pin.\n\nIf the Stackage snapshot that we use (usually the newest LTS snapshot) contains a package, we use instead the version in the Stackage snapshot as default version for that package.\n\nFor some packages, which are not on Stackage, we have if necessary manual overrides to set the default version to a version older than the newest on Hackage.\n\nFor all packages, for which the newest Hackage version is not the default version, there will also be a haskellPackages.foo_x_y_z package with the newest version. The x_y_z part encodes the version with dots replaced by underscores. When the newest version changes by a new release to Hackage the old package will disappear under that name and be replaced by a newer one under the name with the new version. The package name including the version will also disappear when the default version e.g. from Stackage catches up with the newest version from Hackage. E.g. if haskellPackages.foo gets updated from 1.0.0 to 1.1.0 the package haskellPackages.foo_1_1_0 becomes obsolete and gets dropped.\n\nFor some packages, we also manually add other haskellPackages.foo_x_y_z versions, if they are required for a certain build.\n\nRelying on haskellPackages.foo_x_y_z attributes in derivations outside nixpkgs is discouraged because they may change or disappear with every package set update.\n\nAll haskell.packages.* package sets use the same package descriptions and the same sets of versions by default. There are however GHC version specific override .nix files to loosen this a bit.\n\nDependency resolution\n\nNormally when you build Haskell packages with cabal-install, cabal-install does dependency resolution. It will look at all Haskell package versions known on Hackage and tries to pick for every (transitive) dependency of your build exactly one version. Those versions need to satisfy all the version constraints given in the .cabal file of your package and all its dependencies.\n\nThe Haskell builder in nixpkgs does no such thing. It will take as input packages with names off the desired dependencies and just check whether they fulfill the version bounds and fail if they don’t (by default, see jailbreak to circumvent this).\n\nThe haskellPackages.callPackage function does the package resolution. It will, e.g., use haskellPackages.aesonwhich has the default version as described above for a package input of name aeson. (More general: <packages>.callPackage f will call f with named inputs provided from the package set <packages>.) While this is the default behavior, it is possible to override the dependencies for a specific package, see override and overrideScope.\n\nLimitations\n\nOur main objective with haskellPackages is to package Haskell software in nixpkgs. This entails some limitations, partially due to self-imposed restrictions of nixpkgs, partially in the name of maintainability:\n\nOnly the packages built with the default compiler see extensive testing of the whole package set. For other GHC versions only a few essential packages are tested and cached.\n\nAs described above we only build one version of most packages.\n\nThe experience using an older or newer packaged compiler or using different versions may be worse, because builds will not be cached on cache.nixos.org or may fail.\n\nThus, to get the best experience, make sure that your project can be compiled using the default compiler of nixpkgs and recent versions of its dependencies.\n\nA result of this setup is, that getting a valid build plan for a given package can sometimes be quite painful, and in fact this is where most of the maintenance work for haskellPackages is required. Besides that, it is not possible to get the dependencies of a legacy project from nixpkgs or to use a specific stack solver for compiling a project.\n\nEven though we couldn’t use them directly in nixpkgs, it would be desirable to have tooling to generate working Nix package sets from build plans generated by cabal-install or a specific Stackage snapshot via import-from-derivation. Sadly we currently don’t have tooling for this. For this you might be interested in the alternative haskell.nix framework, which, be warned, is completely incompatible with packages from haskellPackages.\n\nhaskellPackages.mkDerivation \n\nEvery haskell package set has its own haskell-aware mkDerivation which is used to build its packages. Generally you won’t have to interact with this builder since cabal2nix can generate packages using it for an arbitrary cabal package definition. Still it is useful to know the parameters it takes when you need to override a generated Nix expression.\n\nhaskellPackages.mkDerivation is a wrapper around stdenv.mkDerivation which re-defines the default phases to be haskell aware and handles dependency specification, test suites, benchmarks etc. by compiling and invoking the package’s Setup.hs. It does not use or invoke the cabal-install binary, but uses the underlying Cabal library instead.\n\nGeneral arguments\npname\n\nPackage name, assumed to be the same as on Hackage (if applicable)\n\nversion\n\nPackaged version, assumed to be the same as on Hackage (if applicable)\n\nsrc\n\nSource of the package. If omitted, fetch package corresponding to pname and version from Hackage.\n\nsha256\n\nHash to use for the default case of src.\n\nrevision\n\nRevision number of the updated cabal file to fetch from Hackage. If null (which is the default value), the one included in src is used.\n\neditedCabalFile\n\nsha256 hash of the cabal file identified by revision or null.\n\nconfigureFlags\n\nExtra flags passed when executing the configure command of Setup.hs.\n\nbuildFlags\n\nExtra flags passed when executing the build command of Setup.hs.\n\nhaddockFlags\n\nExtra flags passed to Setup.hs haddock when building the documentation.\n\ndoCheck\n\nWhether to execute the package’s test suite if it has one. Defaults to true unless cross-compiling.\n\ndoBenchmark\n\nWhether to execute the package’s benchmark if it has one. Defaults to false.\n\ndoHoogle\n\nWhether to generate an index file for hoogle as part of haddockPhase by passing the --hoogle option. Defaults to true.\n\ndoHaddockQuickjump\n\nWhether to generate an index for interactive navigation of the HTML documentation. Defaults to true if supported.\n\ndoInstallIntermediates\n\nWhether to install intermediate build products (files written to dist/build by GHC during the build process). With enableSeparateIntermediatesOutput, these files are instead installed to a separate intermediates output. The output can then be passed into a future build of the same package with the previousIntermediates argument to support incremental builds. See “Incremental builds” for more information. Defaults to false.\n\nenableLibraryProfiling\n\nWhether to enable profiling for libraries contained in the package. Enabled by default if supported.\n\nenableExecutableProfiling\n\nWhether to enable profiling for executables contained in the package. Disabled by default.\n\nprofilingDetail\n\nProfiling detail level to set. Defaults to exported-functions.\n\nenableSharedExecutables\n\nWhether to link executables dynamically. By default, executables are linked statically.\n\nenableSharedLibraries\n\nWhether to build shared Haskell libraries. This is enabled by default unless we are using pkgsStatic or shared libraries have been disabled in GHC.\n\nenableStaticLibraries\n\nWhether to build static libraries. Enabled by default if supported.\n\nenableDeadCodeElimination\n\nWhether to enable linker based dead code elimination in GHC. Enabled by default if supported.\n\nenableHsc2hsViaAsm\n\nWhether to pass --via-asm to hsc2hs. Enabled by default only on Windows.\n\nhyperlinkSource\n\nWhether to render the source as well as part of the haddock documentation by passing the --hyperlinked-source flag. Defaults to true.\n\nisExecutable\n\nWhether the package contains an executable.\n\nisLibrary\n\nWhether the package contains a library.\n\njailbreak\n\nWhether to execute jailbreak-cabal before configurePhase to lift any version constraints in the cabal file. Note that this can’t lift version bounds if they are conditional, i.e. if a dependency is hidden behind a flag.\n\nenableParallelBuilding\n\nWhether to use the -j flag to make GHC/Cabal start multiple jobs in parallel.\n\nmaxBuildCores\n\nUpper limit of jobs to use in parallel for compilation regardless of $NIX_BUILD_CORES. Defaults to 16 as Haskell compilation with GHC currently sees a performance regression if too many parallel jobs are used.\n\ndoCoverage\n\nWhether to generate and install files needed for HPC. Defaults to false.\n\ndoHaddock\n\nWhether to build (HTML) documentation using haddock. Defaults to true if supported.\n\ntestTarget\n\nName of the test suite to build and run. If unset, all test suites will be executed.\n\npreCompileBuildDriver\n\nShell code to run before compiling Setup.hs.\n\npostCompileBuildDriver\n\nShell code to run after compiling Setup.hs.\n\npreHaddock\n\nShell code to run before building documentation using haddock.\n\npostHaddock\n\nShell code to run after building documentation using haddock.\n\ncoreSetup\n\nWhether to only allow core libraries to be used while building Setup.hs. Defaults to false.\n\nuseCpphs\n\nWhether to enable the cpphs preprocessor. Defaults to false.\n\nenableSeparateBinOutput\n\nWhether to install executables to a separate bin output. Defaults to false.\n\nenableSeparateDataOutput\n\nWhether to install data files shipped with the package to a separate data output. Defaults to false.\n\nenableSeparateDocOutput\n\nWhether to install documentation to a separate doc output. Is automatically enabled if doHaddock is true.\n\nenableSeparateIntermediatesOutput\n\nWhen doInstallIntermediates is true, whether to install intermediate build products to a separate intermediates output. See “Incremental builds” for more information. Defaults to false.\n\nallowInconsistentDependencies\n\nIf enabled, allow multiple versions of the same Haskell package in the dependency tree at configure time. Often in such a situation compilation would later fail because of type mismatches. Defaults to false.\n\nenableLibraryForGhci\n\nBuild and install a special object file for GHCi. This improves performance when loading the library in the REPL, but requires extra build time and disk space. Defaults to false.\n\npreviousIntermediates\n\nIf non-null, intermediate build artifacts are copied from this input to dist/build before performing compiling. See “Incremental builds” for more information. Defaults to null.\n\nbuildTarget\n\nName of the executable or library to build and install. If unset, all available targets are built and installed.\n\nSpecifying dependencies\n\nSince haskellPackages.mkDerivation is intended to be generated from cabal files, it reflects cabal’s way of specifying dependencies. For one, dependencies are grouped by what part of the package they belong to. This helps to reduce the dependency closure of a derivation, for example benchmark dependencies are not included if doBenchmark == false.\n\nsetup*Depends\n\ndependencies necessary to compile Setup.hs\n\nlibrary*Depends\n\ndependencies of a library contained in the package\n\nexecutable*Depends\n\ndependencies of an executable contained in the package\n\ntest*Depends\n\ndependencies of a test suite contained in the package\n\nbenchmark*Depends\n\ndependencies of a benchmark contained in the package\n\nThe other categorization relates to the way the package depends on the dependency:\n\n*ToolDepends\n\nTools we need to run as part of the build process. They are added to the derivation’s nativeBuildInputs.\n\n*HaskellDepends\n\nHaskell libraries the package depends on. They are added to propagatedBuildInputs.\n\n*SystemDepends\n\nNon-Haskell libraries the package depends on. They are added to buildInputs\n\n*PkgconfigDepends\n\n*SystemDepends which are discovered using pkg-config. They are added to buildInputs and it is additionally ensured that pkg-config is available at build time.\n\n*FrameworkDepends\n\nApple SDK Framework which the package depends on when compiling it on Darwin.\n\nUsing these two distinctions, you should be able to categorize most of the dependency specifications that are available: benchmarkFrameworkDepends, benchmarkHaskellDepends, benchmarkPkgconfigDepends, benchmarkSystemDepends, benchmarkToolDepends, executableFrameworkDepends, executableHaskellDepends, executablePkgconfigDepends, executableSystemDepends, executableToolDepends, libraryFrameworkDepends, libraryHaskellDepends, libraryPkgconfigDepends, librarySystemDepends, libraryToolDepends, setupHaskellDepends, testFrameworkDepends, testHaskellDepends, testPkgconfigDepends, testSystemDepends and testToolDepends.\n\nThat only leaves the following extra ways for specifying dependencies:\n\nbuildDepends\n\nAllows specifying Haskell dependencies which are added to propagatedBuildInputs unconditionally.\n\nbuildTools\n\nLike *ToolDepends, but are added to nativeBuildInputs unconditionally.\n\nextraLibraries\n\nLike *SystemDepends, but are added to buildInputs unconditionally.\n\npkg-configDepends\n\nLike *PkgconfigDepends, but are added to buildInputs unconditionally.\n\ntestDepends\n\nDeprecated, use either testHaskellDepends or testSystemDepends.\n\nbenchmarkDepends\n\nDeprecated, use either benchmarkHaskellDepends or benchmarkSystemDepends.\n\nThe dependency specification methods in this list which are unconditional are especially useful when writing overrides when you want to make sure that they are definitely included. However, it is recommended to use the more accurate ones listed above when possible.\n\nMeta attributes\n\nhaskellPackages.mkDerivation accepts the following attributes as direct arguments which are transparently set in meta of the resulting derivation. See the Meta-attributes section for their documentation.\n\nThese attributes are populated with a default value if omitted:\n\nhomepage: defaults to the Hackage page for pname.\n\nplatforms: defaults to lib.platforms.all (since GHC can cross-compile)\n\nThese attributes are only set if given:\n\ndescription\n\nlicense\n\nchangelog\n\nmaintainers\n\nbroken\n\nhydraPlatforms\n\nIncremental builds\n\nhaskellPackages.mkDerivation supports incremental builds for GHC 9.4 and newer with the doInstallIntermediates, enableSeparateIntermediatesOutput, and previousIntermediates arguments.\n\nThe basic idea is to first perform a full build of the package in question, save its intermediate build products for later, and then copy those build products into the build directory of an incremental build performed later. Then, GHC will use those build artifacts to avoid recompiling unchanged modules.\n\nFor more detail on how to store and use incremental build products, see Gabriella Gonzalez’ blog post “Nixpkgs support for incremental Haskell builds”. motivation behind this feature.\n\nAn incremental build for the turtle package can be performed like so:\n\nlet\n  pkgs = import <nixpkgs> {};\n  inherit (pkgs) haskell;\n  inherit (haskell.lib.compose) overrideCabal;\n\n  # Incremental builds work with GHC >=9.4.\n  turtle = haskell.packages.ghc944.turtle;\n\n  # This will do a full build of `turtle`, while writing the intermediate build products\n  # (compiled modules, etc.) to the `intermediates` output.\n  turtle-full-build-with-incremental-output = overrideCabal (drv: {\n    doInstallIntermediates = true;\n    enableSeparateIntermediatesOutput = true;\n  }) turtle;\n\n  # This will do an incremental build of `turtle` by copying the previously\n  # compiled modules and intermediate build products into the source tree\n  # before running the build.\n  #\n  # GHC will then naturally pick up and reuse these products, making this build\n  # complete much more quickly than the previous one.\n  turtle-incremental-build = overrideCabal (drv: {\n    previousIntermediates = turtle-full-build-with-incremental-output.intermediates;\n  }) turtle;\nin\n  turtle-incremental-build\n\nDevelopment environments \n\nIn addition to building and installing Haskell software, nixpkgs can also provide development environments for Haskell projects. This has the obvious advantage that you benefit from cache.nixos.org and no longer need to compile all project dependencies yourself. While it is often very useful, this is not the primary use case of our package set. Have a look at the section available package versions to learn which versions of packages we provide and the section limitations, to judge whether a haskellPackages based development environment for your project is feasible.\n\nBy default, every derivation built using haskellPackages.mkDerivation exposes an environment suitable for building it interactively as the env attribute. For example, if you have a local checkout of random, you can enter a development environment for it like this (if the dependencies in the development and packaged version match):\n\n$ cd ~/src/random\n$ nix-shell -A haskellPackages.random.env '<nixpkgs>'\n[nix-shell:~/src/random]$ ghc-pkg list\n/nix/store/a8hhl54xlzfizrhcf03c1l3f6l9l8qwv-ghc-9.2.4-with-packages/lib/ghc-9.2.4/package.conf.d\n    Cabal-3.6.3.0\n    array-0.5.4.0\n    base-4.16.3.0\n    binary-0.8.9.0\n    …\n    ghc-9.2.4\n    …\n\n\nAs you can see, the environment contains a GHC which is set up so it finds all dependencies of random. Note that this environment does not mirror the environment used to build the package, but is intended as a convenient tool for development and simple debugging. env relies on the ghcWithPackages wrapper which automatically injects a pre-populated package-db into every GHC invocation. In contrast, using nix-shell -A haskellPackages.random will not result in an environment in which the dependencies are in GHCs package database. Instead, the Haskell builder will pass in all dependencies explicitly via configure flags.\n\nenv mirrors the normal derivation environment in one aspect: It does not include familiar development tools like cabal-install, since we rely on plain Setup.hs to build all packages. However, cabal-install will work as expected if in PATH (e.g. when installed globally and using a nix-shell without --pure). A declarative and pure way of adding arbitrary development tools is provided via shellFor.\n\nWhen using cabal-install for dependency resolution you need to be a bit careful to achieve build purity. cabal-install will find and use all dependencies installed from the packages env via Nix, but it will also consult Hackage to potentially download and compile dependencies if it can’t find a valid build plan locally. To prevent this you can either never run cabal update, remove the cabal database from your ~/.cabal folder or run cabal with --offline. Note though, that for some usecases cabal2nix needs the local Hackage db.\n\nOften you won’t work on a package that is already part of haskellPackages or Hackage, so we first need to write a Nix expression to obtain the development environment from. Luckily, we can generate one very easily from an already existing cabal file using cabal2nix:\n\n$ ls\nmy-project.cabal src …\n$ cabal2nix ./. > my-project.nix\n\n\nThe generated Nix expression evaluates to a function ready to be callPackage-ed. For now, we can add a minimal default.nix which does just that:\n\n# Retrieve nixpkgs impurely from NIX_PATH for now, you can pin it instead, of course.\n{ pkgs ? import <nixpkgs> {} }:\n\n# use the nixpkgs default haskell package set\npkgs.haskellPackages.callPackage ./my-project.nix { }\n\n\nUsing nix-build default.nix we can now build our project, but we can also enter a shell with all the package’s dependencies available using nix-shell -A env default.nix. If you have cabal-install installed globally, it’ll work inside the shell as expected.\n\nshellFor\n\nHaving to install tools globally is obviously not great, especially if you want to provide a batteries-included shell.nix with your project. Luckily there’s a proper tool for making development environments out of packages’ build environments: shellFor, a function exposed by every haskell package set. It takes the following arguments and returns a derivation which is suitable as a development environment inside nix-shell:\n\npackages\n\nThis argument is used to select the packages for which to build the development environment. This should be a function which takes a haskell package set and returns a list of packages. shellFor will pass the used package set to this function and include all dependencies of the returned package in the build environment. This means you can reuse Nix expressions of packages included in nixpkgs, but also use local Nix expressions like this: hpkgs: [ (hpkgs.callPackage ./my-project.nix { }) ].\n\nnativeBuildInputs\n\nExpects a list of derivations to add as build tools to the build environment. This is the place to add packages like cabal-install, doctest or hlint. Defaults to [].\n\nbuildInputs\n\nExpects a list of derivations to add as library dependencies, like openssl. This is rarely necessary as the haskell package expressions usually track system dependencies as well. Defaults to []. (see also derivation dependencies)\n\nwithHoogle\n\nIf this is true, hoogle will be added to nativeBuildInputs. Additionally, its database will be populated with all included dependencies, so you’ll be able search through the documentation of your dependencies. Defaults to false.\n\ngenericBuilderArgsModifier\n\nThis argument accepts a function allowing you to modify the arguments passed to mkDerivation in order to create the development environment. For example, args: { doCheck = false; } would cause the environment to not include any test dependencies. Defaults to lib.id.\n\ndoBenchmark\n\nThis is a shortcut for enabling doBenchmark via genericBuilderArgsModifier. Setting it to true will cause the development environment to include all benchmark dependencies which would be excluded by default. Defaults to false.\n\nOne neat property of shellFor is that it allows you to work on multiple packages using the same environment in conjunction with cabal.project files. Say our example above depends on distribution-nixpkgs and we have a project file set up for both, we can add the following shell.nix expression:\n\n{ pkgs ? import <nixpkgs> {} }:\n\npkgs.haskellPackages.shellFor {\n  packages = hpkgs: [\n    # reuse the nixpkgs for this package\n    hpkgs.distribution-nixpkgs\n    # call our generated Nix expression manually\n    (hpkgs.callPackage ./my-project/my-project.nix { })\n  ];\n\n  # development tools we use\n  nativeBuildInputs = [\n    pkgs.cabal-install\n    pkgs.haskellPackages.doctest\n    pkgs.cabal2nix\n  ];\n\n  # Extra arguments are added to mkDerivation's arguments as-is.\n  # Since it adds all passed arguments to the shell environment,\n  # we can use this to set the environment variable the `Paths_`\n  # module of distribution-nixpkgs uses to search for bundled\n  # files.\n  # See also: https://cabal.readthedocs.io/en/latest/cabal-package.html#accessing-data-files-from-package-code\n  distribution_nixpkgs_datadir = toString ./distribution-nixpkgs;\n}\n\nhaskell-language-server\n\nTo use HLS in short: Install pkgs.haskell-language-server e.g. in nativeBuildInputs in shellFor and use the haskell-language-server-wrapper command to run it. See the HLS user guide on how to configure your text editor to use HLS and how to test your setup.\n\nHLS needs to be compiled with the GHC version of the project you use it on.\n\npkgs.haskell-language-server provides haskell-language-server-wrapper, haskell-language-server and haskell-language-server-x.x.x binaries, where x.x.x is the GHC version for which it is compiled. By default, it only includes binaries for the current GHC version, to reduce closure size. The closure size is large, because HLS needs to be dynamically linked to work reliably. You can override the list of supported GHC versions with e.g.\n\npkgs.haskell-language-server.override { supportedGhcVersions = [ \"90\" \"94\" ]; }\n\n\nWhere all strings version are allowed such that haskell.packages.ghc${version} is an existing package set.\n\nWhen you run haskell-language-server-wrapper it will detect the GHC version used by the project you are working on (by asking e.g. cabal or stack) and pick the appropriate versioned binary from your path.\n\nBe careful when installing HLS globally and using a pinned nixpkgs for a Haskell project in a nix-shell. If the nixpkgs versions deviate to much (e.g., use different glibc versions) the haskell-language-server-?.?.? executable will try to detect these situations and refuse to start. It is recommended to obtain HLS via nix-shell from the nixpkgs version pinned in there instead.\n\nThe top level pkgs.haskell-language-server attribute is just a convenience wrapper to make it possible to install HLS for multiple GHC versions at the same time. If you know, that you only use one GHC version, e.g., in a project specific nix-shell you can use pkgs.haskellPackages.haskell-language-server or pkgs.haskell.packages.*.haskell-language-server from the package set you use.\n\nIf you use nix-shell for your development environments remember to start your editor in that environment. You may want to use something like direnv and/or an editor plugin to achieve this.\n\nOverriding Haskell packages \nOverriding a single package\n\nLike many language specific subsystems in nixpkgs, the Haskell infrastructure also has its own quirks when it comes to overriding. Overriding of the inputs to a package at least follows the standard procedure. For example, imagine you need to build nix-tree with a more recent version of brick than the default one provided by haskellPackages:\n\nhaskellPackages.nix-tree.override {\n  brick = haskellPackages.brick_0_67;\n}\n\n\nThe custom interface comes into play when you want to override the arguments passed to haskellPackages.mkDerivation. For this, the function overrideCabal from haskell.lib.compose is used. E.g., if you want to install a man page that is distributed with the package, you can do something like this:\n\nhaskell.lib.compose.overrideCabal (drv: {\n  postInstall = ''\n    ${drv.postInstall or \"\"}\n    install -Dm644 man/pnbackup.1 -t $out/share/man/man1\n  '';\n}) haskellPackages.pnbackup\n\n\noverrideCabal takes two arguments:\n\nA function which receives all arguments passed to haskellPackages.mkDerivation before and returns a set of arguments to replace (or add) with a new value.\n\nThe Haskell derivation to override.\n\nThe arguments are ordered so that you can easily create helper functions by making use of currying:\n\nlet\n  installManPage = haskell.lib.compose.overrideCabal (drv: {\n    postInstall = ''\n      ${drv.postInstall or \"\"}\n      install -Dm644 man/${drv.pname}.1 -t \"$out/share/man/man1\"\n    '';\n  });\nin\n\ninstallManPage haskellPackages.pnbackup\n\n\nIn fact, haskell.lib.compose already provides lots of useful helpers for common tasks, detailed in the next section. They are also structured in such a way that they can be combined using lib.pipe:\n\nlib.pipe my-haskell-package [\n  # lift version bounds on dependencies\n  haskell.lib.compose.doJailbreak\n  # disable building the haddock documentation\n  haskell.lib.compose.dontHaddock\n  # pass extra package flag to Cabal's configure step\n  (haskell.lib.compose.enableCabalFlag \"myflag\")\n]\n\nhaskell.lib.compose\n\nThe base interface for all overriding is the following function:\n\noverrideCabal f drv\n\nTakes the arguments passed to obtain drv to f and uses the resulting attribute set to update the argument set. Then a recomputed version of drv using the new argument set is returned.\n\nAll other helper functions are implemented in terms of overrideCabal and make common overrides shorter and more complicate ones trivial. The simple overrides which only change a single argument are only described very briefly in the following overview. Refer to the documentation of haskellPackages.mkDerivation for a more detailed description of the effects of the respective arguments.\n\nPackaging Helpers\noverrideSrc { src, version } drv\n\nReplace the source used for building drv with the path or derivation given as src. The version attribute is optional. Prefer this function over overriding src via overrideCabal, since it also automatically takes care of removing any Hackage revisions.\n\njustStaticExecutables drv\n\nOnly build and install the executables produced by drv, removing everything that may refer to other Haskell packages’ store paths (like libraries and documentation). This dramatically reduces the closure size of the resulting derivation. Note that the executables are only statically linked against their Haskell dependencies, but will still link dynamically against libc, GMP and other system library dependencies. If dependencies use their Cabal-generated Paths_* module, this may not work as well if GHC’s dead code elimination is unable to remove the references to the dependency’s store path that module contains.\n\nenableSeparateBinOutput drv\n\nInstall executables produced by drv to a separate bin output. This has a similar effect as justStaticExecutables, but preserves the libraries and documentation in the out output alongside the bin output with a much smaller closure size.\n\nmarkBroken drv\n\nSets the broken flag to true for drv.\n\nmarkUnbroken drv, unmarkBroken drv\n\nSet the broken flag to false for drv.\n\ndoDistribute drv\n\nUpdates hydraPlatforms so that Hydra will build drv. This is sometimes necessary when working with versioned packages in haskellPackages which are not built by default.\n\ndontDistribute drv\n\nSets hydraPlatforms to [], causing Hydra to skip this package altogether. Useful if it fails to evaluate cleanly and is causing noise in the evaluation errors tab on Hydra.\n\nDevelopment Helpers\nsdistTarball drv\n\nCreate a source distribution tarball like those found on Hackage instead of building the package drv.\n\ndocumentationTarball drv\n\nCreate a documentation tarball suitable for uploading to Hackage instead of building the package drv.\n\nbuildFromSdist drv\n\nUses sdistTarball drv as the source to compile drv. This helps to catch packaging bugs when building from a local directory, e.g. when required files are missing from extra-source-files.\n\nfailOnAllWarnings drv\n\nEnables all warnings GHC supports and makes it fail the build if any of them are emitted.\n\nenableDWARFDebugging drv\n\nCompiles the package with additional debug symbols enabled, useful for debugging with e.g. gdb.\n\ndoStrip drv\n\nSets doStrip to true for drv.\n\ndontStrip drv\n\nSets doStrip to false for drv.\n\nTrivial Helpers\ndoJailbreak drv\n\nSets the jailbreak argument to true for drv.\n\ndontJailbreak drv\n\nSets the jailbreak argument to false for drv.\n\ndoHaddock drv\n\nSets doHaddock to true for drv.\n\ndontHaddock drv\n\nSets doHaddock to false for drv. Useful if the build of a package is failing because of e.g. a syntax error in the Haddock documentation.\n\ndoHyperlinkSource drv\n\nSets hyperlinkSource to true for drv.\n\ndontHyperlinkSource drv\n\nSets hyperlinkSource to false for drv.\n\ndoCheck drv\n\nSets doCheck to true for drv.\n\ndontCheck drv\n\nSets doCheck to false for drv. Useful if a package has a broken, flaky or otherwise problematic test suite breaking the build.\n\nappendConfigureFlags list drv\n\nAdds the strings in list to the configureFlags argument for drv.\n\nenableCabalFlag flag drv\n\nMakes sure that the Cabal flag flag is enabled in Cabal’s configure step.\n\ndisableCabalFlag flag drv\n\nMakes sure that the Cabal flag flag is disabled in Cabal’s configure step.\n\nappendBuildFlags list drv\n\nAdds the strings in list to the buildFlags argument for drv.\n\nappendPatches list drv\n\nAdds the list of derivations or paths to the patches argument for drv.\n\naddBuildTools list drv\n\nAdds the list of derivations to the buildTools argument for drv.\n\naddExtraLibraries list drv\n\nAdds the list of derivations to the extraLibraries argument for drv.\n\naddBuildDepends list drv\n\nAdds the list of derivations to the buildDepends argument for drv.\n\naddTestToolDepends list drv\n\nAdds the list of derivations to the testToolDepends argument for drv.\n\naddPkgconfigDepends list drv\n\nAdds the list of derivations to the pkg-configDepends argument for drv.\n\naddSetupDepends list drv\n\nAdds the list of derivations to the setupHaskellDepends argument for drv.\n\ndoBenchmark drv\n\nSet doBenchmark to true for drv. Useful if your development environment is missing the dependencies necessary for compiling the benchmark component.\n\ndontBenchmark drv\n\nSet doBenchmark to false for drv.\n\nsetBuildTargets drv list\n\nSets the buildTarget argument for drv so that the targets specified in list are built.\n\ndoCoverage drv\n\nSets the doCoverage argument to true for drv.\n\ndontCoverage drv\n\nSets the doCoverage argument to false for drv.\n\nenableExecutableProfiling drv\n\nSets the enableExecutableProfiling argument to true for drv.\n\ndisableExecutableProfiling drv\n\nSets the enableExecutableProfiling argument to false for drv.\n\nenableLibraryProfiling drv\n\nSets the enableLibraryProfiling argument to true for drv.\n\ndisableLibraryProfiling drv\n\nSets the enableLibraryProfiling argument to false for drv.\n\nLibrary functions in the Haskell package sets\n\nSome library functions depend on packages from the Haskell package sets. Thus they are exposed from those instead of from haskell.lib.compose which can only access what is passed directly to it. When using the functions below, make sure that you are obtaining them from the same package set (haskellPackages, haskell.packages.ghc944 etc.) as the packages you are working with or – even better – from the self/final fix point of your overlay to haskellPackages.\n\nNote: Some functions like shellFor that are not intended for overriding per se, are omitted in this section.\n\ncabalSdist { src, name ? ... }\n\nGenerates the Cabal sdist tarball for src, suitable for uploading to Hackage. Contrary to haskell.lib.compose.sdistTarball, it uses cabal-install over Setup.hs, so it is usually faster: No build dependencies need to be downloaded, and we can skip compiling Setup.hs.\n\nbuildFromCabalSdist drv\n\nBuild drv, but run its src attribute through cabalSdist first. Useful for catching files necessary for compilation that are missing from the sdist.\n\ngenerateOptparseApplicativeCompletions list drv\n\nGenerate and install shell completion files for the installed executables whose names are given via list. The executables need to be using optparse-applicative for this to work. Note that this feature is automatically disabled when cross-compiling, since it requires executing the binaries in question.\n\nF.A.Q. \nWhy is topic X not covered in this section? Why is section Y missing?\n\nWe have been working on moving the nixpkgs Haskell documentation back into the nixpkgs manual. Since this process has not been completed yet, you may find some topics missing here covered in the old haskell4nix docs.\n\nIf you feel any important topic is not documented at all, feel free to comment on the issue linked above.\n\nHow to enable or disable profiling builds globally?\n\nBy default, Nixpkgs builds a profiling version of each Haskell library. The exception to this rule are some platforms where it is disabled due to concerns over output size. You may want to…\n\n…enable profiling globally so that you can build a project you are working on with profiling ability giving you insight in the time spent across your code and code you depend on using GHC’s profiling feature.\n\n…disable profiling (globally) to reduce the time spent building the profiling versions of libraries which a significant amount of build time is spent on (although they are not as expensive as the “normal” build of a Haskell library).\n\nNote\n\nThe method described below affects the build of all libraries in the respective Haskell package set as well as GHC. If your choices differ from Nixpkgs’ default for your (host) platform, you will lose the ability to substitute from the official binary cache.\n\nIf you are concerned about build times and thus want to disable profiling, it probably makes sense to use haskell.lib.compose.disableLibraryProfiling (see the section called “Trivial Helpers”) on the packages you are building locally while continuing to substitute their dependencies and GHC.\n\nSince we need to change the profiling settings for the desired Haskell package set and GHC (as the core libraries like base, filepath etc. are bundled with GHC), it is recommended to use overlays for Nixpkgs to change them. Since the interrelated parts, i.e. the package set and GHC, are connected via the Nixpkgs fixpoint, we need to modify them both in a way that preserves their connection (or else we’d have to wire it up again manually). This is achieved by changing GHC and the package set in separate overlays to prevent the package set from pulling in GHC from prev.\n\nThe result is two overlays like the ones shown below. Adjustable parts are annotated with comments, as are any optional or alternative ways to achieve the desired profiling settings without causing too many rebuilds.\n\nlet\n  # Name of the compiler and package set you want to change. If you are using\n  # the default package set `haskellPackages`, you need to look up what version\n  # of GHC it currently uses (note that this is subject to change).\n  ghcName = \"ghc92\";\n  # Desired new setting\n  enableProfiling = true;\nin\n\n[\n  # The first overlay modifies the GHC derivation so that it does or does not\n  # build profiling versions of the core libraries bundled with it. It is\n  # recommended to only use such an overlay if you are enabling profiling on a\n  # platform that doesn't by default, because compiling GHC from scratch is\n  # quite expensive.\n  (final: prev:\n  let\n    inherit (final) lib;\n  in\n\n  {\n    haskell = lib.recursiveUpdate prev.haskell {\n      compiler.${ghcName} = prev.haskell.compiler.${ghcName}.override {\n        # Unfortunately, the GHC setting is named differently for historical reasons\n        enableProfiledLibs = enableProfiling;\n      };\n    };\n  })\n\n  (final: prev:\n  let\n    inherit (final) lib;\n    haskellLib = final.haskell.lib.compose;\n  in\n\n  {\n    haskell = lib.recursiveUpdate prev.haskell {\n      packages.${ghcName} = prev.haskell.packages.${ghcName}.override {\n        overrides = hfinal: hprev: {\n          mkDerivation = args: hprev.mkDerivation (args // {\n            # Since we are forcing our ideas upon mkDerivation, this change will\n            # affect every package in the package set.\n            enableLibraryProfiling = enableProfiling;\n\n            # To actually use profiling on an executable, executable profiling\n            # needs to be enabled for the executable you want to profile. You\n            # can either do this globally or…\n            enableExecutableProfiling = enableProfiling;\n          });\n\n          # …only for the package that contains an executable you want to profile.\n          # That saves on unnecessary rebuilds for packages that you only depend\n          # on for their library, but also contain executables (e.g. pandoc).\n          my-executable = haskellLib.enableExecutableProfiling hprev.my-executable;\n\n          # If you are disabling profiling to save on build time, but want to\n          # retain the ability to substitute from the binary cache. Drop the\n          # override for mkDerivation above and instead have an override like\n          # this for the specific packages you are building locally and want\n          # to make cheaper to build.\n          my-library = haskellLib.disableLibraryProfiling hprev.my-library;\n        };\n      };\n    };\n  })\n]\n\nHy \nInstallation\nInstallation \nInstallation without packages\n\nYou can install hy via nix-env or by adding it to configuration.nix by referring to it as a hy attribute. This kind of installation adds hy to your environment and it successfully works with python3.\n\nCaution\n\nPackages that are installed with your python derivation, are not accessible by hy this way.\n\nInstallation with packages\n\nCreating hy derivation with custom python packages is really simple and similar to the way that python does it. Attribute hy provides function withPackages that creates custom hy derivation with specified packages.\n\nFor example if you want to create shell with matplotlib and numpy, you can do it like so:\n\n$ nix-shell -p \"hy.withPackages (ps: with ps; [ numpy matplotlib ])\"\n\n\nOr if you want to extend your configuration.nix:\n\n{ # ...\n\n  environment.systemPackages = with pkgs; [\n    (hy.withPackages (py-packages: with py-packages; [ numpy matplotlib ]))\n  ];\n}\n\nIdris \nInstalling Idris\nStarting Idris with library support\nBuilding an Idris project with Nix\nPassing options to idris commands\nInstalling Idris \n\nThe easiest way to get a working idris version is to install the idris attribute:\n\n$ nix-env -f \"<nixpkgs>\" -iA idris\n\n\nThis however only provides the prelude and base libraries. To install idris with additional libraries, you can use the idrisPackages.with-packages function, e.g. in an overlay in ~/.config/nixpkgs/overlays/my-idris.nix:\n\nself: super: {\n  myIdris = with self.idrisPackages; with-packages [ contrib pruviloj ];\n}\n\n\nAnd then:\n\n$ # On NixOS\n$ nix-env -iA nixos.myIdris\n$ # On non-NixOS\n$ nix-env -iA nixpkgs.myIdris\n\n\nTo see all available Idris packages:\n\n$ # On NixOS\n$ nix-env -qaPA nixos.idrisPackages\n$ # On non-NixOS\n$ nix-env -qaPA nixpkgs.idrisPackages\n\n\nSimilarly, entering a nix-shell:\n\n$ nix-shell -p 'idrisPackages.with-packages (with idrisPackages; [ contrib pruviloj ])'\n\nStarting Idris with library support \n\nTo have access to these libraries in idris, call it with an argument -p <library name> for each library:\n\n$ nix-shell -p 'idrisPackages.with-packages (with idrisPackages; [ contrib pruviloj ])'\n[nix-shell:~]$ idris -p contrib -p pruviloj\n\n\nA listing of all available packages the Idris binary has access to is available via --listlibs:\n\n$ idris --listlibs\n00prelude-idx.ibc\npruviloj\nbase\ncontrib\nprelude\n00pruviloj-idx.ibc\n00base-idx.ibc\n00contrib-idx.ibc\n\nBuilding an Idris project with Nix \n\nAs an example of how a Nix expression for an Idris package can be created, here is the one for idrisPackages.yaml:\n\n{ lib\n, build-idris-package\n, fetchFromGitHub\n, contrib\n, lightyear\n}:\nbuild-idris-package  {\n  name = \"yaml\";\n  version = \"2018-01-25\";\n\n  # This is the .ipkg file that should be built, defaults to the package name\n  # In this case it should build `Yaml.ipkg` instead of `yaml.ipkg`\n  # This is only necessary because the yaml packages ipkg file is\n  # different from its package name here.\n  ipkgName = \"Yaml\";\n  # Idris dependencies to provide for the build\n  idrisDeps = [ contrib lightyear ];\n\n  src = fetchFromGitHub {\n    owner = \"Heather\";\n    repo = \"Idris.Yaml\";\n    rev = \"5afa51ffc839844862b8316faba3bafa15656db4\";\n    hash = \"sha256-h28F9EEPuvab6zrfeE+0k1XGQJGwINnsJEG8yjWIl7w=\";\n  };\n\n  meta = with lib; {\n    description = \"Idris YAML lib\";\n    homepage = \"https://github.com/Heather/Idris.Yaml\";\n    license = licenses.mit;\n    maintainers = [ maintainers.brainrape ];\n  };\n}\n\n\nAssuming this file is saved as yaml.nix, it’s buildable using\n\n$ nix-build -E '(import <nixpkgs> {}).idrisPackages.callPackage ./yaml.nix {}'\n\n\nOr it’s possible to use\n\nwith import <nixpkgs> {};\n\n{\n  yaml = idrisPackages.callPackage ./yaml.nix {};\n}\n\n\nin another file (say default.nix) to be able to build it with\n\n$ nix-build -A yaml\n\nPassing options to idris commands \n\nThe build-idris-package function provides also optional input values to set additional options for the used idris commands.\n\nSpecifically, you can set idrisBuildOptions, idrisTestOptions, idrisInstallOptions and idrisDocOptions to provide additional options to the idris command respectively when building, testing, installing and generating docs for your package.\n\nFor example you could set\n\nbuild-idris-package {\n  idrisBuildOptions = [ \"--log\" \"1\" \"--verbose\" ]\n\n  ...\n}\n\n\nto require verbose output during idris build phase.\n\niOS \nDeploying a proxy component wrapper exposing Xcode\nBuilding an iOS application\nSpawning simulator instances\nTroubleshooting\n\nThis component is basically a wrapper/workaround that makes it possible to expose an Xcode installation as a Nix package by means of symlinking to the relevant executables on the host system.\n\nSince Xcode can’t be packaged with Nix, nor we can publish it as a Nix package (because of its license) this is basically the only integration strategy making it possible to do iOS application builds that integrate with other components of the Nix ecosystem\n\nThe primary objective of this project is to use the Nix expression language to specify how iOS apps can be built from source code, and to automatically spawn iOS simulator instances for testing.\n\nThis component also makes it possible to use Hydra, the Nix-based continuous integration server to regularly build iOS apps and to do wireless ad-hoc installations of enterprise IPAs on iOS devices through Hydra.\n\nThe Xcode build environment implements a number of features.\n\nDeploying a proxy component wrapper exposing Xcode \n\nThe first use case is deploying a Nix package that provides symlinks to the Xcode installation on the host system. This package can be used as a build input to any build function implemented in the Nix expression language that requires Xcode.\n\nlet\n  pkgs = import <nixpkgs> {};\n\n  xcodeenv = import ./xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcodeenv.composeXcodeWrapper {\n  version = \"9.2\";\n  xcodeBaseDir = \"/Applications/Xcode.app\";\n}\n\n\nBy deploying the above expression with nix-build and inspecting its content you will notice that several Xcode-related executables are exposed as a Nix package:\n\n$ ls result/bin\nlrwxr-xr-x  1 sander  staff  94  1 jan  1970 Simulator -> /Applications/Xcode.app/Contents/Developer/Applications/Simulator.app/Contents/MacOS/Simulator\nlrwxr-xr-x  1 sander  staff  17  1 jan  1970 codesign -> /usr/bin/codesign\nlrwxr-xr-x  1 sander  staff  17  1 jan  1970 security -> /usr/bin/security\nlrwxr-xr-x  1 sander  staff  21  1 jan  1970 xcode-select -> /usr/bin/xcode-select\nlrwxr-xr-x  1 sander  staff  61  1 jan  1970 xcodebuild -> /Applications/Xcode.app/Contents/Developer/usr/bin/xcodebuild\nlrwxr-xr-x  1 sander  staff  14  1 jan  1970 xcrun -> /usr/bin/xcrun\n\nBuilding an iOS application \n\nWe can build an iOS app executable for the simulator, or an IPA/xcarchive file for release purposes, e.g. ad-hoc, enterprise or store installations, by executing the xcodeenv.buildApp {} function:\n\nlet\n  pkgs = import <nixpkgs> {};\n\n  xcodeenv = import ./xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcodeenv.buildApp {\n  name = \"MyApp\";\n  src = ./myappsources;\n  sdkVersion = \"11.2\";\n\n  target = null; # Corresponds to the name of the app by default\n  configuration = null; # Release for release builds, Debug for debug builds\n  scheme = null; # -scheme will correspond to the app name by default\n  sdk = null; # null will set it to 'iphonesimulator` for simulator builds or `iphoneos` to real builds\n  xcodeFlags = \"\";\n\n  release = true;\n  certificateFile = ./mycertificate.p12;\n  certificatePassword = \"secret\";\n  provisioningProfile = ./myprovisioning.profile;\n  signMethod = \"ad-hoc\"; # 'enterprise' or 'store'\n  generateIPA = true;\n  generateXCArchive = false;\n\n  enableWirelessDistribution = true;\n  installURL = \"/installipa.php\";\n  bundleId = \"mycompany.myapp\";\n  appVersion = \"1.0\";\n\n  # Supports all xcodewrapper parameters as well\n  xcodeBaseDir = \"/Applications/Xcode.app\";\n}\n\n\nThe above function takes a variety of parameters:\n\nThe name and src parameters are mandatory and specify the name of the app and the location where the source code resides\n\nsdkVersion specifies which version of the iOS SDK to use.\n\nIt also possible to adjust the xcodebuild parameters. This is only needed in rare circumstances. In most cases the default values should suffice:\n\nSpecifies which xcodebuild target to build. By default it takes the target that has the same name as the app.\n\nThe configuration parameter can be overridden if desired. By default, it will do a debug build for the simulator and a release build for real devices.\n\nThe scheme parameter specifies which -scheme parameter to propagate to xcodebuild. By default, it corresponds to the app name.\n\nThe sdk parameter specifies which SDK to use. By default, it picks iphonesimulator for simulator builds and iphoneos for release builds.\n\nThe xcodeFlags parameter specifies arbitrary command line parameters that should be propagated to xcodebuild.\n\nBy default, builds are carried out for the iOS simulator. To do release builds (builds for real iOS devices), you must set the release parameter to true. In addition, you need to set the following parameters:\n\ncertificateFile refers to a P12 certificate file.\n\ncertificatePassword specifies the password of the P12 certificate.\n\nprovisioningProfile refers to the provision profile needed to sign the app\n\nsignMethod should refer to ad-hoc for signing the app with an ad-hoc certificate, enterprise for enterprise certificates and app-store for App store certificates.\n\ngenerateIPA specifies that we want to produce an IPA file (this is probably what you want)\n\ngenerateXCArchive specifies that we want to produce an xcarchive file.\n\nWhen building IPA files on Hydra and when it is desired to allow iOS devices to install IPAs by browsing to the Hydra build products page, you can enable the enableWirelessDistribution parameter.\n\nWhen enabled, you need to configure the following options:\n\nThe installURL parameter refers to the URL of a PHP script that composes the itms-services:// URL allowing iOS devices to install the IPA file.\n\nbundleId refers to the bundle ID value of the app\n\nappVersion refers to the app’s version number\n\nTo use wireless adhoc distributions, you must also install the corresponding PHP script on a web server (see section: ‘Installing the PHP script for wireless ad hoc installations from Hydra’ for more information).\n\nIn addition to the build parameters, you can also specify any parameters that the xcodeenv.composeXcodeWrapper {} function takes. For example, the xcodeBaseDir parameter can be overridden to refer to a different Xcode version.\n\nSpawning simulator instances \n\nIn addition to building iOS apps, we can also automatically spawn simulator instances:\n\nlet\n  pkgs = import <nixpkgs> {};\n\n  xcodeenv = import ./xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcode.simulateApp {\n  name = \"simulate\";\n\n  # Supports all xcodewrapper parameters as well\n  xcodeBaseDir = \"/Applications/Xcode.app\";\n}\n\n\nThe above expression produces a script that starts the simulator from the provided Xcode installation. The script can be started as follows:\n\n./result/bin/run-test-simulator\n\n\nBy default, the script will show an overview of UDID for all available simulator instances and asks you to pick one. You can also provide a UDID as a command-line parameter to launch an instance automatically:\n\n./result/bin/run-test-simulator 5C93129D-CF39-4B1A-955F-15180C3BD4B8\n\n\nYou can also extend the simulator script to automatically deploy and launch an app in the requested simulator instance:\n\nlet\n  pkgs = import <nixpkgs> {};\n\n  xcodeenv = import ./xcodeenv {\n    inherit (pkgs) stdenv;\n  };\nin\nxcode.simulateApp {\n  name = \"simulate\";\n  bundleId = \"mycompany.myapp\";\n  app = xcode.buildApp {\n    # ...\n  };\n\n  # Supports all xcodewrapper parameters as well\n  xcodeBaseDir = \"/Applications/Xcode.app\";\n}\n\n\nBy providing the result of an xcode.buildApp {} function and configuring the app bundle id, the app gets deployed automatically and started.\n\nTroubleshooting \n\nIn some rare cases, it may happen that after a failure, changes are not picked up. Most likely, this is caused by a derived data cache that Xcode maintains. To wipe it you can run:\n\n$ rm -rf ~/Library/Developer/Xcode/DerivedData\n\nJava \n\nAnt-based Java packages are typically built from source as follows:\n\nstdenv.mkDerivation {\n  name = \"...\";\n  src = fetchurl { ... };\n\n  nativeBuildInputs = [ jdk ant ];\n\n  buildPhase = \"ant\";\n}\n\n\nNote that jdk is an alias for the OpenJDK (self-built where available, or pre-built via Zulu). Platforms with OpenJDK not (yet) in Nixpkgs (Aarch32, Aarch64) point to the (unfree) oraclejdk.\n\nJAR files that are intended to be used by other packages should be installed in $out/share/java. JDKs have a stdenv setup hook that add any JARs in the share/java directories of the build inputs to the CLASSPATH environment variable. For instance, if the package libfoo installs a JAR named foo.jar in its share/java directory, and another package declares the attribute\n\nbuildInputs = [ libfoo ];\nnativeBuildInputs = [ jdk ];\n\n\nthen CLASSPATH will be set to /nix/store/...-libfoo/share/java/foo.jar.\n\nPrivate JARs should be installed in a location like $out/share/package-name.\n\nIf your Java package provides a program, you need to generate a wrapper script to run it using a JRE. You can use makeWrapper for this:\n\nnativeBuildInputs = [ makeWrapper ];\n\ninstallPhase = ''\n  mkdir -p $out/bin\n  makeWrapper ${jre}/bin/java $out/bin/foo \\\n    --add-flags \"-cp $out/share/java/foo.jar org.foo.Main\"\n'';\n\n\nSince the introduction of the Java Platform Module System in Java 9, Java distributions typically no longer ship with a general-purpose JRE: instead, they allow generating a JRE with only the modules required for your application(s). Because we can’t predict what modules will be needed on a general-purpose system, the default jre package is the full JDK. When building a minimal system/image, you can override the modules parameter on jre_minimal to build a JRE with only the modules relevant for you:\n\nlet\n  my_jre = pkgs.jre_minimal.override {\n    modules = [\n      # The modules used by 'something' and 'other' combined:\n      \"java.base\"\n      \"java.logging\"\n    ];\n  };\n  something = (pkgs.something.override { jre = my_jre; });\n  other = (pkgs.other.override { jre = my_jre; });\nin\n  ...\n\n\nYou can also specify what JDK your JRE should be based on, for example selecting a ‘headless’ build to avoid including a link to GTK+:\n\nmy_jre = pkgs.jre_minimal.override {\n  jdk = jdk11_headless;\n};\n\n\nNote all JDKs passthru home, so if your application requires environment variables like JAVA_HOME being set, that can be done in a generic fashion with the --set argument of makeWrapper:\n\n--set JAVA_HOME ${jdk.home}\n\n\nIt is possible to use a different Java compiler than javac from the OpenJDK. For instance, to use the GNU Java Compiler:\n\nnativeBuildInputs = [ gcj ant ];\n\n\nHere, Ant will automatically use gij (the GNU Java Runtime) instead of the OpenJRE.\n\nJavascript \nIntroduction\nGetting unstuck / finding code examples\nTools overview\nGeneral principles\nJavascript packages inside nixpkgs\nTool specific instructions\nOutside Nixpkgs\nIntroduction \n\nThis contains instructions on how to package javascript applications.\n\nThe various tools available will be listed in the tools-overview. Some general principles for packaging will follow. Finally some tool specific instructions will be given.\n\nGetting unstuck / finding code examples \n\nIf you find you are lacking inspiration for packing javascript applications, the links below might prove useful. Searching online for prior art can be helpful if you are running into solved problems.\n\nGithub\n\nSearching Nix files for mkYarnPackage: https://github.com/search?q=mkYarnPackage+language%3ANix&type=code\n\nSearching just flake.nix files for mkYarnPackage: https://github.com/search?q=mkYarnPackage+path%3A**%2Fflake.nix&type=code\n\nGitlab\n\nSearching Nix files for mkYarnPackage: https://gitlab.com/search?scope=blobs&search=mkYarnPackage+extension%3Anix\n\nSearching just flake.nix files for mkYarnPackage: https://gitlab.com/search?scope=blobs&search=mkYarnPackage+filename%3Aflake.nix\n\nTools overview \nGeneral principles \n\nThe following principles are given in order of importance with potential exceptions.\n\nTry to use the same node version used upstream\n\nIt is often not documented which node version is used upstream, but if it is, try to use the same version when packaging.\n\nThis can be a problem if upstream is using the latest and greatest and you are trying to use an earlier version of node. Some cryptic errors regarding V8 may appear.\n\nTry to respect the package manager originally used by upstream (and use the upstream lock file)\n\nA lock file (package-lock.json, yarn.lock…) is supposed to make reproducible installations of node_modules for each tool.\n\nGuidelines of package managers, recommend to commit those lock files to the repos. If a particular lock file is present, it is a strong indication of which package manager is used upstream.\n\nIt’s better to try to use a Nix tool that understand the lock file. Using a different tool might give you hard to understand error because different packages have been installed. An example of problems that could arise can be found here. Upstream use NPM, but this is an attempt to package it with yarn2nix (that uses yarn.lock).\n\nUsing a different tool forces to commit a lock file to the repository. Those files are fairly large, so when packaging for nixpkgs, this approach does not scale well.\n\nExceptions to this rule are:\n\nWhen you encounter one of the bugs from a Nix tool. In each of the tool specific instructions, known problems will be detailed. If you have a problem with a particular tool, then it’s best to try another tool, even if this means you will have to recreate a lock file and commit it to nixpkgs. In general yarn2nix has less known problems and so a simple search in nixpkgs will reveal many yarn.lock files committed.\n\nSome lock files contain particular version of a package that has been pulled off NPM for some reason. In that case, you can recreate upstream lock (by removing the original and npm install, yarn, …) and commit this to nixpkgs.\n\nThe only tool that supports workspaces (a feature of NPM that helps manage sub-directories with different package.json from a single top level package.json) is yarn2nix. If upstream has workspaces you should try yarn2nix.\n\nTry to use upstream package.json\n\nExceptions to this rule are:\n\nSometimes the upstream repo assumes some dependencies be installed globally. In that case you can add them manually to the upstream package.json (yarn add xxx or npm install xxx, …). Dependencies that are installed locally can be executed with npx for CLI tools. (e.g. npx postcss ..., this is how you can call those dependencies in the phases).\n\nSometimes there is a version conflict between some dependency requirements. In that case you can fix a version by removing the ^.\n\nSometimes the script defined in the package.json does not work as is. Some scripts for example use CLI tools that might not be available, or cd in directory with a different package.json (for workspaces notably). In that case, it’s perfectly fine to look at what the particular script is doing and break this down in the phases. In the build script you can see build:* calling in turns several other build scripts like build:ui or build:server. If one of those fails, you can try to separate those into,\n\nyarn build:ui\nyarn build:server\n# OR\nnpm run build:ui\nnpm run build:server\n\n\nwhen you need to override a package.json. It’s nice to use the one from the upstream source and do some explicit override. Here is an example:\n\npatchedPackageJSON = final.runCommand \"package.json\" { } ''\n  ${jq}/bin/jq '.version = \"0.4.0\" |\n    .devDependencies.\"@jsdoc/cli\" = \"^0.2.5\"\n    ${sonar-src}/package.json > $out\n'';\n\n\nYou will still need to commit the modified version of the lock files, but at least the overrides are explicit for everyone to see.\n\nUsing node_modules directly\n\nEach tool has an abstraction to just build the node_modules (dependencies) directory. You can always use the stdenv.mkDerivation with the node_modules to build the package (symlink the node_modules directory and then use the package build command). The node_modules abstraction can be also used to build some web framework frontends. For an example of this see how plausible is built. mkYarnModules to make the derivation containing node_modules. Then when building the frontend you can just symlink the node_modules directory.\n\nJavascript packages inside nixpkgs \n\nThe pkgs/development/node-packages folder contains a generated collection of NPM packages that can be installed with the Nix package manager.\n\nAs a rule of thumb, the package set should only provide end user software packages, such as command-line utilities. Libraries should only be added to the package set if there is a non-NPM package that requires it.\n\nWhen it is desired to use NPM libraries in a development project, use the node2nix generator directly on the package.json configuration file of the project.\n\nThe package set provides support for the official stable Node.js versions. The latest stable LTS release in nodePackages, as well as the latest stable current release in nodePackages_latest.\n\nIf your package uses native addons, you need to examine what kind of native build system it uses. Here are some examples:\n\nnode-gyp\n\nnode-gyp-builder\n\nnode-pre-gyp\n\nAfter you have identified the correct system, you need to override your package expression while adding in build system as a build input. For example, dat requires node-gyp-build, so we override its expression in pkgs/development/node-packages/overrides.nix:\n\n    dat = prev.dat.override (oldAttrs: {\n      buildInputs = [ final.node-gyp-build pkgs.libtool pkgs.autoconf pkgs.automake ];\n      meta = oldAttrs.meta // { broken = since \"12\"; };\n    });\n\nAdding and Updating Javascript packages in nixpkgs\n\nTo add a package from NPM to nixpkgs:\n\nModify pkgs/development/node-packages/node-packages.json to add, update or remove package entries to have it included in nodePackages and nodePackages_latest.\n\nRun the script:\n\n./pkgs/development/node-packages/generate.sh\n\n\nBuild your new package to test your changes:\n\nnix-build -A nodePackages.<new-or-updated-package>\n\n\nTo build against the latest stable Current Node.js version (e.g. 18.x):\n\nnix-build -A nodePackages_latest.<new-or-updated-package>\n\n\nIf the package doesn’t build, you may need to add an override as explained above.\n\nIf the package’s name doesn’t match any of the executables it provides, add an entry in pkgs/development/node-packages/main-programs.nix. This will be the case for all scoped packages, e.g., @angular/cli.\n\nAdd and commit all modified and generated files.\n\nFor more information about the generation process, consult the README.md file of the node2nix tool.\n\nTo update NPM packages in nixpkgs, run the same generate.sh script:\n\n./pkgs/development/node-packages/generate.sh\n\nGit protocol error\n\nSome packages may have Git dependencies from GitHub specified with git://. GitHub has disabled unencrypted Git connections, so you may see the following error when running the generate script:\n\nThe unauthenticated git protocol on port 9418 is no longer supported\n\n\nUse the following Git configuration to resolve the issue:\n\ngit config --global url.\"https://github.com/\".insteadOf git://github.com/\n\nTool specific instructions \nbuildNpmPackage\n\nbuildNpmPackage allows you to package npm-based projects in Nixpkgs without the use of an auto-generated dependencies file (as used in node2nix). It works by utilizing npm’s cache functionality – creating a reproducible cache that contains the dependencies of a project, and pointing npm to it.\n\nHere’s an example:\n\n{ lib, buildNpmPackage, fetchFromGitHub }:\n\nbuildNpmPackage rec {\n  pname = \"flood\";\n  version = \"4.7.0\";\n\n  src = fetchFromGitHub {\n    owner = \"jesec\";\n    repo = pname;\n    rev = \"v${version}\";\n    hash = \"sha256-BR+ZGkBBfd0dSQqAvujsbgsEPFYw/ThrylxUbOksYxM=\";\n  };\n\n  npmDepsHash = \"sha256-tuEfyePwlOy2/mOPdXbqJskO6IowvAP4DWg8xSZwbJw=\";\n\n  # The prepack script runs the build script, which we'd rather do in the build phase.\n  npmPackFlags = [ \"--ignore-scripts\" ];\n\n  NODE_OPTIONS = \"--openssl-legacy-provider\";\n\n  meta = with lib; {\n    description = \"A modern web UI for various torrent clients with a Node.js backend and React frontend\";\n    homepage = \"https://flood.js.org\";\n    license = licenses.gpl3Only;\n    maintainers = with maintainers; [ winter ];\n  };\n}\n\n\nIn the default installPhase set by buildNpmPackage, it uses npm pack --json --dry-run to decide what files to install in $out/lib/node_modules/$name/, where $name is the name string defined in the package’s package.json. Additionally, the bin and man keys in the source’s package.json are used to decide what binaries and manpages are supposed to be installed. If these are not defined, npm pack may miss some files, and no binaries will be produced.\n\nArguments\n\nnpmDepsHash: The output hash of the dependencies for this project. Can be calculated in advance with prefetch-npm-deps.\n\nmakeCacheWritable: Whether to make the cache writable prior to installing dependencies. Don’t set this unless npm tries to write to the cache directory, as it can slow down the build.\n\nnpmBuildScript: The script to run to build the project. Defaults to \"build\".\n\nnpmWorkspace: The workspace directory within the project to build and install.\n\ndontNpmBuild: Option to disable running the build script. Set to true if the package does not have a build script. Defaults to false. Alternatively, setting buildPhase explicitly also disables this.\n\ndontNpmInstall: Option to disable running npm install. Defaults to false. Alternatively, setting installPhase explicitly also disables this.\n\nnpmFlags: Flags to pass to all npm commands.\n\nnpmInstallFlags: Flags to pass to npm ci.\n\nnpmBuildFlags: Flags to pass to npm run ${npmBuildScript}.\n\nnpmPackFlags: Flags to pass to npm pack.\n\nnpmPruneFlags: Flags to pass to npm prune. Defaults to the value of npmInstallFlags.\n\nmakeWrapperArgs: Flags to pass to makeWrapper, added to executable calling the generated .js with node as an interpreter. These scripts are defined in package.json.\n\nnodejs: The nodejs package to build against, using the corresponding npm shipped with that version of node. Defaults to pkgs.nodejs.\n\nprefetch-npm-deps\n\nprefetch-npm-deps is a Nixpkgs package that calculates the hash of the dependencies of an npm project ahead of time.\n\n$ ls\npackage.json package-lock.json index.js\n$ prefetch-npm-deps package-lock.json\n...\nsha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\n\nfetchNpmDeps\n\nfetchNpmDeps is a Nix function that requires the following mandatory arguments:\n\nsrc: A directory / tarball with package-lock.json file\n\nhash: The output hash of the node dependencies defined in package-lock.json.\n\nIt returns a derivation with all package-lock.json dependencies downloaded into $out/, usable as an npm cache.\n\ncorepack\n\nThis package puts the corepack wrappers for pnpm and yarn in your PATH, and they will honor the packageManager setting in the package.json.\n\nnode2nix\nPreparation\n\nYou will need to generate a Nix expression for the dependencies. Don’t forget the -l package-lock.json if there is a lock file. Most probably you will need the --development to include the devDependencies\n\nSo the command will most likely be:\n\nnode2nix --development -l package-lock.json\n\n\nSee node2nix docs for more info.\n\nPitfalls\n\nIf upstream package.json does not have a “version” attribute, node2nix will crash. You will need to add it like shown in the package.json section.\n\nnode2nix has some bugs related to working with lock files from NPM distributed with nodejs_16.\n\nnode2nix does not like missing packages from NPM. If you see something like Cannot resolve version: vue-loader-v16@undefined then you might want to try another tool. The package might have been pulled off of NPM.\n\nyarn2nix\nPreparation\n\nYou will need at least a yarn.lock file. If upstream does not have one you need to generate it and reference it in your package definition.\n\nIf the downloaded files contain the package.json and yarn.lock files they can be used like this:\n\nofflineCache = fetchYarnDeps {\n  yarnLock = src + \"/yarn.lock\";\n  hash = \"....\";\n};\n\nmkYarnPackage\n\nmkYarnPackage will by default try to generate a binary. For package only generating static assets (Svelte, Vue, React, WebPack, …), you will need to explicitly override the build step with your instructions.\n\nIt’s important to use the --offline flag. For example if you script is \"build\": \"something\" in package.json use:\n\nbuildPhase = ''\n  export HOME=$(mktemp -d)\n  yarn --offline build\n'';\n\n\nThe dist phase is also trying to build a binary, the only way to override it is with:\n\ndistPhase = \"true\";\n\n\nThe configure phase can sometimes fail because it makes many assumptions which may not always apply. One common override is:\n\nconfigurePhase = ''\n  ln -s $node_modules node_modules\n'';\n\n\nor if you need a writeable node_modules directory:\n\nconfigurePhase = ''\n  cp -r $node_modules node_modules\n  chmod +w node_modules\n'';\n\nmkYarnModules\n\nThis will generate a derivation including the node_modules directory. If you have to build a derivation for an integrated web framework (rails, phoenix…), this is probably the easiest way.\n\nOverriding dependency behavior\n\nIn the mkYarnPackage record the property pkgConfig can be used to override packages when you encounter problems building.\n\nFor instance, say your package is throwing errors when trying to invoke node-sass:\n\nENOENT: no such file or directory, scandir '/build/source/node_modules/node-sass/vendor'\n\n\nTo fix this we will specify different versions of build inputs to use, as well as some post install steps to get the software built the way we want:\n\nmkYarnPackage rec {\n  pkgConfig = {\n    node-sass = {\n      buildInputs = with final;[ python libsass pkg-config ];\n      postInstall = ''\n        LIBSASS_EXT=auto yarn --offline run build\n        rm build/config.gypi\n      '';\n    };\n  };\n}\n\nPitfalls\n\nIf version is missing from upstream package.json, yarn will silently install nothing. In that case, you will need to override package.json as shown in the package.json section\n\nHaving trouble with node-gyp? Try adding these lines to the yarnPreBuild steps:\n\nyarnPreBuild = ''\n  mkdir -p $HOME/.node-gyp/${nodejs.version}\n  echo 9 > $HOME/.node-gyp/${nodejs.version}/installVersion\n  ln -sfv ${nodejs}/include $HOME/.node-gyp/${nodejs.version}\n  export npm_config_nodedir=${nodejs}\n'';\n\n\nThe echo 9 steps comes from this answer: https://stackoverflow.com/a/49139496\n\nExporting the headers in npm_config_nodedir comes from this issue: https://github.com/nodejs/node-gyp/issues/1191#issuecomment-301243919\n\nOutside Nixpkgs \n\nThere are some other tools available, which are written in the Nix language. These that can’t be used inside Nixpkgs because they require Import From Derivation, which is not allowed in Nixpkgs.\n\nIf you are packaging something outside Nixpkgs, consider the following:\n\nnpmlock2nix\n\nnpmlock2nix aims at building node_modules without code generation. It hasn’t reached v1 yet, the API might be subject to change.\n\nPitfalls\n\nThere are some problems with npm v7.\n\nnix-npm-buildpackage\n\nnix-npm-buildpackage aims at building node_modules without code generation. It hasn’t reached v1 yet, the API might change. It supports both package-lock.json and yarn.lock.\n\nPitfalls\n\nThere are some problems with npm v7.\n\nlisp-modules \nOverview\nThe 90% use case example\nImporting packages from Quicklisp\nDefining packages manually inside Nixpkgs\nDefining packages manually outside Nixpkgs\nOverriding package attributes\nBuilding Wrappers\nAdding a new Lisp\n\nThis document describes the Nixpkgs infrastructure for building Common Lisp systems that use ASDF (Another System Definition Facility). It lives in pkgs/development/lisp-modules.\n\nOverview \n\nThe main entry point of the API are the Common Lisp implementation packages themselves (e.g. abcl, ccl, clasp-common-lisp, clisp, ecl, sbcl). They have the pkgs and withPackages attributes, which can be used to discover available packages and to build wrappers, respectively.\n\nThe pkgs attribute set contains packages that were automatically imported from Quicklisp, and any other manually defined ones. Not every package works for all the CL implementations (e.g. nyxt only makes sense for sbcl).\n\nThe withPackages function is of primary utility. It is used to build runnable wrappers, with a pinned and pre-built ASDF FASL available in the ASDF environment variable, and CL_SOURCE_REGISTRY/ASDF_OUTPUT_TRANSLATIONS configured to find the desired systems on runtime.\n\nIn addition, Lisps have the withOverrides function, which can be used to substitute any package in the scope of their pkgs. This will also be useful together with overrideLispAttrs when dealing with slashy systems, because they should stay in the main package and be built by specifying the systems argument to build-asdf-system.\n\nThe 90% use case example \n\nThe most common way to use the library is to run ad-hoc wrappers like this:\n\nnix-shell -p 'sbcl.withPackages (ps: with ps; [ alexandria ])'\n\nThen, in a shell:\n\n$ sbcl\n* (load (sb-ext:posix-getenv \"ASDF\"))\n* (asdf:load-system 'alexandria)\n\n\nAlso one can create a pkgs.mkShell environment in shell.nix/flake.nix:\n\nlet\n  sbcl' = sbcl.withPackages (ps: [ ps.alexandria ]);\nin mkShell {\n  packages = [ sbcl' ];\n}\n\n\nSuch a Lisp can be now used e.g. to compile your sources:\n\nbuildPhase = ''\n  ${sbcl'}/bin/sbcl --load my-build-file.lisp\n''\n\nImporting packages from Quicklisp \n\nTo save some work of writing Nix expressions, there is a script that imports all the packages distributed by Quicklisp into imported.nix. This works by parsing its releases.txt and systems.txt files, which are published every couple of months on quicklisp.org.\n\nThe import process is implemented in the import directory as Common Lisp code in the org.lispbuilds.nix ASDF system. To run the script, one can execute ql-import.lisp:\n\ncd pkgs/development/lisp-modules\nnix-shell --run 'sbcl --script ql-import.lisp'\n\n\nThe script will:\n\nDownload the latest Quicklisp systems.txt and releases.txt files\n\nGenerate a temporary SQLite database of all QL systems in packages.sqlite\n\nGenerate an imported.nix file from the database\n\n(The packages.sqlite file can be deleted at will, because it is regenerated each time the script runs.)\n\nThe maintainer’s job is to:\n\nRe-run the ql-import.lisp script when there is a new Quicklisp release\n\nAdd any missing native dependencies in ql.nix\n\nFor packages that still don’t build, package them manually in packages.nix\n\nAlso, the imported.nix file must not be edited manually! It should only be generated as described in this section (by running ql-import.lisp).\n\nAdding native dependencies\n\nThe Quicklisp files contain ASDF dependency data, but don’t include native library (CFFI) dependencies, and, in the case of ABCL, Java dependencies.\n\nThe ql.nix file contains a long list of overrides, where these dependencies can be added.\n\nPackages defined in packages.nix contain these dependencies naturally.\n\nTrusting systems.txt and releases.txt\n\nThe previous implementation of lisp-modules didn’t fully trust the Quicklisp data, because there were times where the dependencies specified were not complete and caused broken builds. It instead used a nix-shell environment to discover real dependencies by using the ASDF APIs.\n\nThe current implementation has chosen to trust this data, because it’s faster to parse a text file than to build each system to generate its Nix file, and because that way packages can be mass-imported. Because of that, there may come a day where some packages will break, due to bugs in Quicklisp. In that case, the fix could be a manual override in packages.nix and ql.nix.\n\nA known fact is that Quicklisp doesn’t include dependencies on slashy systems in its data. This is an example of a situation where such fixes were used, e.g. to replace the systems attribute of the affected packages. (See the definition of iolib).\n\nQuirks\n\nDuring Quicklisp import:\n\n+ in names is converted to _plus{_,}: cl+ssl->cl_plus_ssl, alexandria+->alexandria_plus\n\n. in names is converted to _dot_: iolib.base->iolib_dot_base\n\nnames starting with a number have a _ prepended (3d-vectors->_3d-vectors)\n\n_ in names is converted to __ for reversibility\n\nDefining packages manually inside Nixpkgs \n\nPackages that for some reason are not in Quicklisp, and so cannot be auto-imported, or don’t work straight from the import, are defined in the packages.nix file.\n\nIn that file, use the build-asdf-system function, which is a wrapper around mkDerivation for building ASDF systems. Various other hacks are present, such as build-with-compile-into-pwd for systems which create files during compilation (such as cl-unicode).\n\nThe build-asdf-system function is documented here. Also, packages.nix is full of examples of how to use it.\n\nDefining packages manually outside Nixpkgs \n\nLisp derivations (abcl, sbcl etc.) also export the buildASDFSystem function, which is similar to build-asdf-system from packages.nix, but is part of the public API.\n\nIt takes the following arguments:\n\npname: the package name\n\nversion: the package version\n\nsrc: the package source\n\npatches: patches to apply to the source before build\n\nnativeLibs: native libraries used by CFFI and grovelling\n\njavaLibs: Java libraries for ABCL\n\nlispLibs: dependencies on other packages build with buildASDFSystem\n\nsystems: list of systems to build\n\nIt can be used to define packages outside Nixpkgs, and, for example, add them into the package scope with withOverrides.\n\nIncluding an external package in scope\n\nA package defined outside Nixpkgs using buildASDFSystem can be woven into the Nixpkgs-provided scope like this:\n\nlet\n  alexandria = sbcl.buildASDFSystem rec {\n    pname = \"alexandria\";\n    version = \"1.4\";\n    src = fetchFromGitLab {\n      domain = \"gitlab.common-lisp.net\";\n      owner = \"alexandria\";\n      repo = \"alexandria\";\n      rev = \"v${version}\";\n      hash = \"sha256-1Hzxt65dZvgOFIljjjlSGgKYkj+YBLwJCACi5DZsKmQ=\";\n    };\n  };\n  sbcl' = sbcl.withOverrides (self: super: {\n    inherit alexandria;\n  });\nin sbcl'.pkgs.alexandria\n\nOverriding package attributes \n\nPackages export the overrideLispAttrs function, which can be used to build a new package with different parameters.\n\nExample of overriding alexandria:\n\nsbcl.pkgs.alexandria.overrideLispAttrs (oldAttrs: rec {\n  version = \"1.4\";\n  src = fetchFromGitLab {\n    domain = \"gitlab.common-lisp.net\";\n    owner = \"alexandria\";\n    repo = \"alexandria\";\n    rev = \"v${version}\";\n    hash = \"sha256-1Hzxt65dZvgOFIljjjlSGgKYkj+YBLwJCACi5DZsKmQ=\";\n  };\n})\n\nDealing with slashy systems\n\nSlashy (secondary) systems should not exist in their own packages! Instead, they should be included in the parent package as an extra entry in the systems argument to the build-asdf-system/buildASDFSystem functions.\n\nThe reason is that ASDF searches for a secondary system in the .asd of the parent package. Thus, having them separate would cause either one of them not to load cleanly, because one will contains FASLs of itself but not the other, and vice versa.\n\nTo package slashy systems, use overrideLispAttrs, like so:\n\necl.pkgs.alexandria.overrideLispAttrs (oldAttrs: {\n  systems = oldAttrs.systems ++ [ \"alexandria/tests\" ];\n  lispLibs = oldAttrs.lispLibs ++ [ ecl.pkgs.rt ];\n})\n\n\nSee the respective section on using withOverrides for how to weave it back into ecl.pkgs.\n\nNote that sometimes the slashy systems might not only have more dependencies than the main one, but create a circular dependency between .asd files. Unfortunately, in this case an adhoc solution becomes necessary.\n\nBuilding Wrappers \n\nWrappers can be built using the withPackages function of Common Lisp implementations (abcl, ecl, sbcl etc.):\n\nnix-shell -p 'sbcl.withPackages (ps: [ ps.alexandria ps.bordeaux-threads ])'\n\n\nSuch a wrapper can then be used like this:\n\n$ sbcl\n* (load (sb-ext:posix-getenv \"ASDF\"))\n* (asdf:load-system 'alexandria)\n* (asdf:load-system 'bordeaux-threads)\n\nLoading ASDF\n\nFor best results, avoid calling (require 'asdf) When using the library-generated wrappers.\n\nUse (load (ext:getenv \"ASDF\")) instead, supplying your implementation’s way of getting an environment variable for ext:getenv. This will load the (pre-compiled to FASL) Nixpkgs-provided version of ASDF.\n\nLoading systems\n\nThere, you can use asdf:load-system. This works by setting the right values for the CL_SOURCE_REGISTRY/ASDF_OUTPUT_TRANSLATIONS environment variables, so that systems are found in the Nix store and pre-compiled FASLs are loaded.\n\nAdding a new Lisp \n\nThe function wrapLisp is used to wrap Common Lisp implementations. It adds the pkgs, withPackages, withOverrides and buildASDFSystem attributes to the derivation.\n\nwrapLisp takes these arguments:\n\npkg: the Lisp package\n\nfaslExt: Implementation-specific extension for FASL files\n\nprogram: The name of executable file in ${pkg}/bin/ (Default: pkg.pname)\n\nflags: A list of flags to always pass to program (Default: [])\n\nasdf: The ASDF version to use (Default: pkgs.asdf_3_3)\n\npackageOverrides: Package overrides config (Default: (self: super: {}))\n\nThis example wraps CLISP:\n\nwrapLisp {\n  pkg = clisp;\n  faslExt = \"fas\";\n  flags = [\"-E\" \"UTF8\"];\n}\n\nUser’s Guide to Lua Infrastructure \nUsing Lua\nDeveloping with Lua\nLua Reference\nUsing Lua \nOverview of Lua\n\nSeveral versions of the Lua interpreter are available: luajit, lua 5.1, 5.2, 5.3. The attribute lua refers to the default interpreter, it is also possible to refer to specific versions, e.g. lua5_2 refers to Lua 5.2.\n\nLua libraries are in separate sets, with one set per interpreter version.\n\nThe interpreters have several common attributes. One of these attributes is pkgs, which is a package set of Lua libraries for this specific interpreter. E.g., the busted package corresponding to the default interpreter is lua.pkgs.busted, and the lua 5.2 version is lua5_2.pkgs.busted. The main package set contains aliases to these package sets, e.g. luaPackages refers to lua5_1.pkgs and lua52Packages to lua5_2.pkgs.\n\nInstalling Lua and packages\nLua environment defined in separate .nix file\n\nCreate a file, e.g. build.nix, with the following expression\n\nwith import <nixpkgs> {};\n\nlua5_2.withPackages (ps: with ps; [ busted luafilesystem ])\n\n\nand install it in your profile with\n\nnix-env -if build.nix\n\n\nNow you can use the Lua interpreter, as well as the extra packages (busted, luafilesystem) that you added to the environment.\n\nLua environment defined in ~/.config/nixpkgs/config.nix\n\nIf you prefer to, you could also add the environment as a package override to the Nixpkgs set, e.g. using config.nix,\n\n{ # ...\n\n  packageOverrides = pkgs: with pkgs; {\n    myLuaEnv = lua5_2.withPackages (ps: with ps; [ busted luafilesystem ]);\n  };\n}\n\n\nand install it in your profile with\n\nnix-env -iA nixpkgs.myLuaEnv\n\n\nThe environment is installed by referring to the attribute, and considering the nixpkgs channel was used.\n\nLua environment defined in /etc/nixos/configuration.nix\n\nFor the sake of completeness, here’s another example how to install the environment system-wide.\n\n{ # ...\n\n  environment.systemPackages = with pkgs; [\n    (lua.withPackages(ps: with ps; [ busted luafilesystem ]))\n  ];\n}\n\nHow to override a Lua package using overlays?\n\nUse the following overlay template:\n\nfinal: prev:\n{\n\n  lua = prev.lua.override {\n    packageOverrides = luaself: luaprev: {\n\n      luarocks-nix = luaprev.luarocks-nix.overrideAttrs(oa: {\n        pname = \"luarocks-nix\";\n        src = /home/my_luarocks/repository;\n      });\n  };\n\n  luaPackages = lua.pkgs;\n}\n\nTemporary Lua environment with nix-shell\n\nThere are two methods for loading a shell with Lua packages. The first and recommended method is to create an environment with lua.buildEnv or lua.withPackages and load that. E.g.\n\n$ nix-shell -p 'lua.withPackages(ps: with ps; [ busted luafilesystem ])'\n\n\nopens a shell from which you can launch the interpreter\n\n[nix-shell:~] lua\n\n\nThe other method, which is not recommended, does not create an environment and requires you to list the packages directly,\n\n$ nix-shell -p lua.pkgs.busted lua.pkgs.luafilesystem\n\n\nAgain, it is possible to launch the interpreter from the shell. The Lua interpreter has the attribute pkgs which contains all Lua libraries for that specific interpreter.\n\nDeveloping with Lua \n\nNow that you know how to get a working Lua environment with Nix, it is time to go forward and start actually developing with Lua. There are two ways to package lua software, either it is on luarocks and most of it can be taken care of by the luarocks2nix converter or the packaging has to be done manually. Let’s present the luarocks way first and the manual one in a second time.\n\nPackaging a library on luarocks\n\nLuarocks.org is the main repository of lua packages. The site proposes two types of packages, the rockspec and the src.rock (equivalent of a rockspec but with the source).\n\nLuarocks-based packages are generated in pkgs/development/lua-modules/generated-packages.nix from the whitelist maintainers/scripts/luarocks-packages.csv and updated by running the package luarocks-packages-updater:\n\n\nnix-shell -p luarocks-packages-updater --run luarocks-packages-updater\n\n\nluarocks2nix is a tool capable of generating nix derivations from both rockspec and src.rock (and favors the src.rock). The automation only goes so far though and some packages need to be customized. These customizations go in pkgs/development/lua-modules/overrides.nix. For instance if the rockspec defines external_dependencies, these need to be manually added to the overrides.nix.\n\nYou can try converting luarocks packages to nix packages with the command nix-shell -p luarocks-nix and then luarocks nix PKG_NAME.\n\nPackaging a library manually\n\nYou can develop your package as you usually would, just don’t forget to wrap it within a toLuaModule call, for instance\n\nmynewlib = toLuaModule ( stdenv.mkDerivation { ... });\n\n\nThere is also the buildLuaPackage function that can be used when lua modules are not packaged for luarocks. You can see a few examples at pkgs/top-level/lua-packages.nix.\n\nLua Reference \nLua interpreters\n\nVersions 5.1, 5.2, 5.3 and 5.4 of the lua interpreter are available as respectively lua5_1, lua5_2, lua5_3 and lua5_4. Luajit is available too. The Nix expressions for the interpreters can be found in pkgs/development/interpreters/lua-5.\n\nAttributes on lua interpreters packages\n\nEach interpreter has the following attributes:\n\ninterpreter. Alias for ${pkgs.lua}/bin/lua.\n\nbuildEnv. Function to build lua interpreter environments with extra packages bundled together. See section lua.buildEnv function for usage and documentation.\n\nwithPackages. Simpler interface to buildEnv.\n\npkgs. Set of Lua packages for that specific interpreter. The package set can be modified by overriding the interpreter and passing packageOverrides.\n\nbuildLuarocksPackage function\n\nThe buildLuarocksPackage function is implemented in pkgs/development/interpreters/lua-5/build-luarocks-package.nix The following is an example:\n\nluaposix = buildLuarocksPackage {\n  pname = \"luaposix\";\n  version = \"34.0.4-1\";\n\n  src = fetchurl {\n    url    = \"https://raw.githubusercontent.com/rocks-moonscript-org/moonrocks-mirror/master/luaposix-34.0.4-1.src.rock\";\n    hash = \"sha256-4mLJG8n4m6y4Fqd0meUDfsOb9RHSR0qa/KD5KCwrNXs=\";\n  };\n  disabled = (luaOlder \"5.1\") || (luaAtLeast \"5.4\");\n  propagatedBuildInputs = [ bit32 lua std_normalize ];\n\n  meta = with lib; {\n    homepage = \"https://github.com/luaposix/luaposix/\";\n    description = \"Lua bindings for POSIX\";\n    maintainers = with maintainers; [ vyp lblasc ];\n    license.fullName = \"MIT/X11\";\n  };\n};\n\n\nThe buildLuarocksPackage delegates most tasks to luarocks:\n\nit adds luarocks as an unpacker for src.rock files (zip files really).\n\nconfigurePhase writes a temporary luarocks configuration file which location is exported via the environment variable LUAROCKS_CONFIG.\n\nthe buildPhase does nothing.\n\ninstallPhase calls luarocks make --deps-mode=none --tree $out to build and install the package\n\nIn the postFixup phase, the wrapLuaPrograms bash function is called to wrap all programs in the $out/bin/* directory to include $PATH environment variable and add dependent libraries to script’s LUA_PATH and LUA_CPATH.\n\nBy default meta.platforms is set to the same value as the interpreter unless overridden otherwise.\n\nbuildLuaApplication function\n\nThe buildLuaApplication function is practically the same as buildLuaPackage. The difference is that buildLuaPackage by default prefixes the names of the packages with the version of the interpreter. Because with an application we’re not interested in multiple version the prefix is dropped.\n\nlua.withPackages function\n\nThe lua.withPackages takes a function as an argument that is passed the set of lua packages and returns the list of packages to be included in the environment. Using the withPackages function, the previous example for the luafilesystem environment can be written like this:\n\nwith import <nixpkgs> {};\n\nlua.withPackages (ps: [ps.luafilesystem])\n\n\nwithPackages passes the correct package set for the specific interpreter version as an argument to the function. In the above example, ps equals luaPackages. But you can also easily switch to using lua5_2:\n\nwith import <nixpkgs> {};\n\nlua5_2.withPackages (ps: [ps.lua])\n\n\nNow, ps is set to lua52Packages, matching the version of the interpreter.\n\nPossible Todos\n\nexport/use version specific variables such as LUA_PATH_5_2/LUAROCKS_CONFIG_5_2\n\nlet luarocks check for dependencies via exporting the different rocktrees in temporary config\n\nLua Contributing guidelines\n\nFollowing rules should be respected:\n\nMake sure libraries build for all Lua interpreters.\n\nCommit names of Lua libraries should reflect that they are Lua libraries, so write for example luaPackages.luafilesystem: 1.11 -> 1.12.\n\nMaven \nBuilding a package using maven.buildMavenPackage\nManually using mvn2nix\n\nMaven is a well-known build tool for the Java ecosystem however it has some challenges when integrating into the Nix build system.\n\nThe following provides a list of common patterns with how to package a Maven project (or any JVM language that can export to Maven) as a Nix package.\n\nBuilding a package using maven.buildMavenPackage \n\nConsider the following package:\n\n{ lib, fetchFromGitHub, jre, makeWrapper, maven }:\n\nmaven.buildMavenPackage rec {\n  pname = \"jd-cli\";\n  version = \"1.2.1\";\n\n  src = fetchFromGitHub {\n    owner = \"intoolswetrust\";\n    repo = pname;\n    rev = \"${pname}-${version}\";\n    hash = \"sha256-rRttA5H0A0c44loBzbKH7Waoted3IsOgxGCD2VM0U/Q=\";\n  };\n\n  mvnHash = \"sha256-kLpjMj05uC94/5vGMwMlFzLKNFOKeyNvq/vmB6pHTAo=\";\n\n  nativeBuildInputs = [ makeWrapper ];\n\n  installPhase = ''\n    mkdir -p $out/bin $out/share/jd-cli\n    install -Dm644 jd-cli/target/jd-cli.jar $out/share/jd-cli\n\n    makeWrapper ${jre}/bin/java $out/bin/jd-cli \\\n      --add-flags \"-jar $out/share/jd-cli/jd-cli.jar\"\n  '';\n\n  meta = with lib; {\n    description = \"Simple command line wrapper around JD Core Java Decompiler project\";\n    homepage = \"https://github.com/intoolswetrust/jd-cli\";\n    license = licenses.gpl3Plus;\n    maintainers = with maintainers; [ majiir ];\n  };\n}:\n\n\nThis package calls maven.buildMavenPackage to do its work. The primary difference from stdenv.mkDerivation is the mvnHash variable, which is a hash of all of the Maven dependencies.\n\nTip\n\nAfter setting maven.buildMavenPackage, we then do standard Java .jar installation by saving the .jar to $out/share/java and then making a wrapper which allows executing that file; see the section called “Java” for additional generic information about packaging Java applications.\n\nStable Maven plugins\n\nMaven defines default versions for its core plugins, e.g. maven-compiler-plugin. If your project does not override these versions, an upgrade of Maven will change the version of the used plugins, and therefore the derivation and hash.\n\nWhen maven is upgraded, mvnHash for the derivation must be updated as well: otherwise, the project will be built on the derivation of old plugins, and fail because the requested plugins are missing.\n\nThis clearly prevents automatic upgrades of Maven: a manual effort must be made throughout nixpkgs by any maintainer wishing to push the upgrades.\n\nTo make sure that your package does not add extra manual effort when upgrading Maven, explicitly define versions for all plugins. You can check if this is the case by adding the following plugin to your (parent) POM:\n\n<plugin>\n  <groupId>org.apache.maven.plugins</groupId>\n  <artifactId>maven-enforcer-plugin</artifactId>\n  <version>3.3.0</version>\n  <executions>\n    <execution>\n      <id>enforce-plugin-versions</id>\n      <goals>\n        <goal>enforce</goal>\n      </goals>\n      <configuration>\n        <rules>\n          <requirePluginVersions />\n        </rules>\n      </configuration>\n    </execution>\n  </executions>\n</plugin>\n\nManually using mvn2nix \nWarning\n\nThis way is no longer recommended; see the section called “Building a package using maven.buildMavenPackage” for the simpler and preferred way.\n\nFor the purposes of this example let’s consider a very basic Maven project with the following pom.xml with a single dependency on emoji-java.\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>io.github.fzakaria</groupId>\n  <artifactId>maven-demo</artifactId>\n  <version>1.0</version>\n  <packaging>jar</packaging>\n  <name>NixOS Maven Demo</name>\n\n  <dependencies>\n    <dependency>\n        <groupId>com.vdurmont</groupId>\n        <artifactId>emoji-java</artifactId>\n        <version>5.1.1</version>\n      </dependency>\n  </dependencies>\n</project>\n\n\nOur main class file will be very simple:\n\nimport com.vdurmont.emoji.EmojiParser;\n\npublic class Main {\n  public static void main(String[] args) {\n    String str = \"NixOS :grinning: is super cool :smiley:!\";\n    String result = EmojiParser.parseToUnicode(str);\n    System.out.println(result);\n  }\n}\n\n\nYou find this demo project at https://github.com/fzakaria/nixos-maven-example.\n\nSolving for dependencies\nbuildMaven with NixOS/mvn2nix-maven-plugin\n\nbuildMaven is an alternative method that tries to follow similar patterns of other programming languages by generating a lock file. It relies on the maven plugin mvn2nix-maven-plugin.\n\nFirst you generate a project-info.json file using the maven plugin.\n\nThis should be executed in the project’s source repository or be told which pom.xml to execute with.\n\n# run this step within the project's source repository\n❯ mvn org.nixos.mvn2nix:mvn2nix-maven-plugin:mvn2nix\n\n❯ cat project-info.json | jq | head\n{\n  \"project\": {\n    \"artifactId\": \"maven-demo\",\n    \"groupId\": \"org.nixos\",\n    \"version\": \"1.0\",\n    \"classifier\": \"\",\n    \"extension\": \"jar\",\n    \"dependencies\": [\n      {\n        \"artifactId\": \"maven-resources-plugin\",\n\n\nThis file is then given to the buildMaven function, and it returns 2 attributes.\n\nrepo: A Maven repository that is a symlink farm of all the dependencies found in the project-info.json\n\nbuild: A simple derivation that runs through mvn compile & mvn package to build the JAR. You may use this as inspiration for more complicated derivations.\n\nHere is an example of building the Maven repository\n\n{ pkgs ? import <nixpkgs> { } }:\nwith pkgs;\n(buildMaven ./project-info.json).repo\n\n\nThe benefit over the double invocation as we will see below, is that the /nix/store entry is a linkFarm of every package, so that changes to your dependency set doesn’t involve downloading everything from scratch.\n\n❯ tree $(nix-build --no-out-link build-maven-repository.nix) | head\n/nix/store/g87va52nkc8jzbmi1aqdcf2f109r4dvn-maven-repository\n├── antlr\n│   └── antlr\n│       └── 2.7.2\n│           ├── antlr-2.7.2.jar -> /nix/store/d027c8f2cnmj5yrynpbq2s6wmc9cb559-antlr-2.7.2.jar\n│           └── antlr-2.7.2.pom -> /nix/store/mv42fc5gizl8h5g5vpywz1nfiynmzgp2-antlr-2.7.2.pom\n├── avalon-framework\n│   └── avalon-framework\n│       └── 4.1.3\n│           ├── avalon-framework-4.1.3.jar -> /nix/store/iv5fp3955w3nq28ff9xfz86wvxbiw6n9-avalon-framework-4.1.3.jar\n\nDouble Invocation\nNote\n\nThis pattern is the simplest but may cause unnecessary rebuilds due to the output hash changing.\n\nThe double invocation is a simple way to get around the problem that nix-build may be sandboxed and have no Internet connectivity.\n\nIt treats the entire Maven repository as a single source to be downloaded, relying on Maven’s dependency resolution to satisfy the output hash. This is similar to fetchers like fetchgit, except it has to run a Maven build to determine what to download.\n\nThe first step will be to build the Maven project as a fixed-output derivation in order to collect the Maven repository – below is an example.\n\nNote\n\nTraditionally the Maven repository is at ~/.m2/repository. We will override this to be the $out directory.\n\n{ lib, stdenv, maven }:\nstdenv.mkDerivation {\n  name = \"maven-repository\";\n  buildInputs = [ maven ];\n  src = ./.; # or fetchFromGitHub, cleanSourceWith, etc\n  buildPhase = ''\n    mvn package -Dmaven.repo.local=$out\n  '';\n\n  # keep only *.{pom,jar,sha1,nbm} and delete all ephemeral files with lastModified timestamps inside\n  installPhase = ''\n    find $out -type f \\\n      -name \\*.lastUpdated -or \\\n      -name resolver-status.properties -or \\\n      -name _remote.repositories \\\n      -delete\n  '';\n\n  # don't do any fixup\n  dontFixup = true;\n  outputHashAlgo = \"sha256\";\n  outputHashMode = \"recursive\";\n  # replace this with the correct SHA256\n  outputHash = lib.fakeSha256;\n}\n\n\nThe build will fail, and tell you the expected outputHash to place. When you’ve set the hash, the build will return with a /nix/store entry whose contents are the full Maven repository.\n\nWarning\n\nSome additional files are deleted that would cause the output hash to change potentially on subsequent runs.\n\n❯ tree $(nix-build --no-out-link double-invocation-repository.nix) | head\n/nix/store/8kicxzp98j68xyi9gl6jda67hp3c54fq-maven-repository\n├── backport-util-concurrent\n│   └── backport-util-concurrent\n│       └── 3.1\n│           ├── backport-util-concurrent-3.1.pom\n│           └── backport-util-concurrent-3.1.pom.sha1\n├── classworlds\n│   └── classworlds\n│       ├── 1.1\n│       │   ├── classworlds-1.1.jar\n\n\nIf your package uses SNAPSHOT dependencies or version ranges; there is a strong likelihood that over-time your output hash will change since the resolved dependencies may change. Hence this method is less recommended then using buildMaven.\n\nBuilding a JAR\n\nRegardless of which strategy is chosen above, the step to build the derivation is the same.\n\n{ stdenv, maven, callPackage }:\n# pick a repository derivation, here we will use buildMaven\nlet repository = callPackage ./build-maven-repository.nix { };\nin stdenv.mkDerivation rec {\n  pname = \"maven-demo\";\n  version = \"1.0\";\n\n  src = builtins.fetchTarball \"https://github.com/fzakaria/nixos-maven-example/archive/main.tar.gz\";\n  buildInputs = [ maven ];\n\n  buildPhase = ''\n    echo \"Using repository ${repository}\"\n    mvn --offline -Dmaven.repo.local=${repository} package;\n  '';\n\n  installPhase = ''\n    install -Dm644 target/${pname}-${version}.jar $out/share/java\n  '';\n}\n\nTip\n\nWe place the library in $out/share/java since JDK package has a stdenv setup hook that adds any JARs in the share/java directories of the build inputs to the CLASSPATH environment.\n\n❯ tree $(nix-build --no-out-link build-jar.nix)\n/nix/store/7jw3xdfagkc2vw8wrsdv68qpsnrxgvky-maven-demo-1.0\n└── share\n    └── java\n        └── maven-demo-1.0.jar\n\n2 directories, 1 file\n\nRunnable JAR\n\nThe previous example builds a jar file but that’s not a file one can run.\n\nYou need to use it with java -jar $out/share/java/output.jar and make sure to provide the required dependencies on the classpath.\n\nThe following explains how to use makeWrapper in order to make the derivation produce an executable that will run the JAR file you created.\n\nWe will use the same repository we built above (either double invocation or buildMaven) to setup a CLASSPATH for our JAR.\n\nThe following two methods are more suited to Nix then building an UberJar which may be the more traditional approach.\n\nCLASSPATH\n\nThis method is ideal if you are providing a derivation for nixpkgs and don’t want to patch the project’s pom.xml.\n\nWe will read the Maven repository and flatten it to a single list. This list will then be concatenated with the CLASSPATH separator to create the full classpath.\n\nWe make sure to provide this classpath to the makeWrapper.\n\n{ stdenv, maven, callPackage, makeWrapper, jre }:\nlet\n  repository = callPackage ./build-maven-repository.nix { };\nin stdenv.mkDerivation rec {\n  pname = \"maven-demo\";\n  version = \"1.0\";\n\n  src = builtins.fetchTarball\n    \"https://github.com/fzakaria/nixos-maven-example/archive/main.tar.gz\";\n  nativeBuildInputs = [ makeWrapper ];\n  buildInputs = [ maven ];\n\n  buildPhase = ''\n    echo \"Using repository ${repository}\"\n    mvn --offline -Dmaven.repo.local=${repository} package;\n  '';\n\n  installPhase = ''\n    mkdir -p $out/bin\n\n    classpath=$(find ${repository} -name \"*.jar\" -printf ':%h/%f');\n    install -Dm644 target/${pname}-${version}.jar $out/share/java\n    # create a wrapper that will automatically set the classpath\n    # this should be the paths from the dependency derivation\n    makeWrapper ${jre}/bin/java $out/bin/${pname} \\\n          --add-flags \"-classpath $out/share/java/${pname}-${version}.jar:''${classpath#:}\" \\\n          --add-flags \"Main\"\n  '';\n}\n\nMANIFEST file via Maven Plugin\n\nThis method is ideal if you are the project owner and want to change your pom.xml to set the CLASSPATH within it.\n\nAugment the pom.xml to create a JAR with the following manifest:\n\n<build>\n  <plugins>\n    <plugin>\n        <artifactId>maven-jar-plugin</artifactId>\n        <configuration>\n            <archive>\n                <manifest>\n                    <addClasspath>true</addClasspath>\n                    <classpathPrefix>../../repository/</classpathPrefix>\n                    <classpathLayoutType>repository</classpathLayoutType>\n                    <mainClass>Main</mainClass>\n                </manifest>\n                <manifestEntries>\n                    <Class-Path>.</Class-Path>\n                </manifestEntries>\n            </archive>\n        </configuration>\n    </plugin>\n  </plugins>\n</build>\n\n\nThe above plugin instructs the JAR to look for the necessary dependencies in the lib/ relative folder. The layout of the folder is also in the maven repository style.\n\n❯ unzip -q -c $(nix-build --no-out-link runnable-jar.nix)/share/java/maven-demo-1.0.jar META-INF/MANIFEST.MF\n\nManifest-Version: 1.0\nArchiver-Version: Plexus Archiver\nBuilt-By: nixbld\nClass-Path: . ../../repository/com/vdurmont/emoji-java/5.1.1/emoji-jav\n a-5.1.1.jar ../../repository/org/json/json/20170516/json-20170516.jar\nCreated-By: Apache Maven 3.6.3\nBuild-Jdk: 1.8.0_265\nMain-Class: Main\n\n\nWe will modify the derivation above to add a symlink to our repository so that it’s accessible to our JAR during the installPhase.\n\n{ stdenv, maven, callPackage, makeWrapper, jre }:\n# pick a repository derivation, here we will use buildMaven\nlet repository = callPackage ./build-maven-repository.nix { };\nin stdenv.mkDerivation rec {\n  pname = \"maven-demo\";\n  version = \"1.0\";\n\n  src = builtins.fetchTarball\n    \"https://github.com/fzakaria/nixos-maven-example/archive/main.tar.gz\";\n  nativeBuildInputs = [ makeWrapper ];\n  buildInputs = [ maven ];\n\n  buildPhase = ''\n    echo \"Using repository ${repository}\"\n    mvn --offline -Dmaven.repo.local=${repository} package;\n  '';\n\n  installPhase = ''\n    mkdir -p $out/bin\n\n    # create a symbolic link for the repository directory\n    ln -s ${repository} $out/repository\n\n    install -Dm644 target/${pname}-${version}.jar $out/share/java\n    # create a wrapper that will automatically set the classpath\n    # this should be the paths from the dependency derivation\n    makeWrapper ${jre}/bin/java $out/bin/${pname} \\\n          --add-flags \"-jar $out/share/java/${pname}-${version}.jar\"\n  '';\n}\n\nNote\n\nOur script produces a dependency on jre rather than jdk to restrict the runtime closure necessary to run the application.\n\nThis will give you an executable shell-script that launches your JAR with all the dependencies available.\n\n❯ tree $(nix-build --no-out-link runnable-jar.nix)\n/nix/store/8d4c3ibw8ynsn01ibhyqmc1zhzz75s26-maven-demo-1.0\n├── bin\n│   └── maven-demo\n├── repository -> /nix/store/g87va52nkc8jzbmi1aqdcf2f109r4dvn-maven-repository\n└── share\n    └── java\n        └── maven-demo-1.0.jar\n\n❯ $(nix-build --no-out-link --option tarball-ttl 1 runnable-jar.nix)/bin/maven-demo\nNixOS 😀 is super cool 😃!\n\nNim \nOverview\nNim program packages in Nixpkgs\nNim library packages in Nixpkgs\nbuildNimPackage parameters\nOverview \n\nThe Nim compiler, a builder function, and some packaged libraries are available in Nixpkgs. Until now each compiler release has been effectively backwards compatible so only the latest version is available.\n\nNim program packages in Nixpkgs \n\nNim programs can be built using nimPackages.buildNimPackage. In the case of packages not containing exported library code the attribute nimBinOnly should be set to true.\n\nThe following example shows a Nim program that depends only on Nim libraries:\n\n{ lib, nimPackages, fetchFromGitHub }:\n\nnimPackages.buildNimPackage (finalAttrs: {\n  pname = \"ttop\";\n  version = \"1.0.1\";\n  nimBinOnly = true;\n\n  src = fetchFromGitHub {\n    owner = \"inv2004\";\n    repo = \"ttop\";\n    rev = \"v${finalAttrs.version}\";\n    hash = \"sha256-x4Uczksh6p3XX/IMrOFtBxIleVHdAPX9e8n32VAUTC4=\";\n  };\n\n  buildInputs = with nimPackages; [ asciigraph illwill parsetoml zippy ];\n\n})\n\nNim library packages in Nixpkgs \n\nNim libraries can also be built using nimPackages.buildNimPackage, but often the product of a fetcher is sufficient to satisfy a dependency. The fetchgit, fetchFromGitHub, and fetchNimble functions yield an output that can be discovered during the configurePhase of buildNimPackage.\n\nNim library packages are listed in pkgs/top-level/nim-packages.nix and implemented at pkgs/development/nim-packages.\n\nThe following example shows a Nim library that propagates a dependency on a non-Nim package:\n\n{ lib, buildNimPackage, fetchNimble, SDL2 }:\n\nbuildNimPackage (finalAttrs: {\n  pname = \"sdl2\";\n  version = \"2.0.4\";\n  src = fetchNimble {\n    inherit (finalAttrs) pname version;\n    hash = \"sha256-Vtcj8goI4zZPQs2TbFoBFlcR5UqDtOldaXSH/+/xULk=\";\n  };\n  propagatedBuildInputs = [ SDL2 ];\n})\n\nbuildNimPackage parameters \n\nAll parameters from stdenv.mkDerivation function are still supported. The following are specific to buildNimPackage:\n\nnimBinOnly ? false: If true then build only the programs listed in the Nimble file in the packages sources.\n\nnimbleFile: Specify the Nimble file location of the package being built rather than discover the file at build-time.\n\nnimRelease ? true: Build the package in release mode.\n\nnimDefines ? []: A list of Nim defines. Key-value tuples are not supported.\n\nnimFlags ? []: A list of command line arguments to pass to the Nim compiler. Use this to specify defines with arguments in the form of -d:${name}=${value}.\n\nnimDoc ? false`: Build and install HTML documentation.\n\nbuildInputs ? []: The packages listed here will be searched for *.nimble files which are used to populate the Nim library path. Otherwise the standard behavior is in effect.\n\nOCaml \nUser guide\nPackaging guide\nUser guide \n\nOCaml libraries are available in attribute sets of the form ocaml-ng.ocamlPackages_X_XX where X is to be replaced with the desired compiler version. For example, ocamlgraph compiled with OCaml 4.12 can be found in ocaml-ng.ocamlPackages_4_12.ocamlgraph. The compiler itself is also located in this set, under the name ocaml.\n\nIf you don’t care about the exact compiler version, ocamlPackages is a top-level alias pointing to a recent version of OCaml.\n\nOCaml applications are usually available top-level, and not inside ocamlPackages. Notable exceptions are build tools that must be built with the same compiler version as the compiler you intend to use like dune or ocaml-lsp.\n\nTo open a shell able to build a typical OCaml project, put the dependencies in buildInputs and add ocamlPackages.ocaml and ocamlPackages.findlib to nativeBuildInputs at least. For example:\n\nlet\n pkgs = import <nixpkgs> {};\n # choose the ocaml version you want to use\n ocamlPackages = pkgs.ocaml-ng.ocamlPackages_4_12;\nin\npkgs.mkShell {\n  # build tools\n  nativeBuildInputs = with ocamlPackages; [ ocaml findlib dune_2 ocaml-lsp ];\n  # dependencies\n  buildInputs = with ocamlPackages; [ ocamlgraph ];\n}\n\nPackaging guide \n\nOCaml libraries should be installed in $(out)/lib/ocaml/${ocaml.version}/site-lib/. Such directories are automatically added to the $OCAMLPATH environment variable when building another package that depends on them or when opening a nix-shell.\n\nGiven that most of the OCaml ecosystem is now built with dune, nixpkgs includes a convenience build support function called buildDunePackage that will build an OCaml package using dune, OCaml and findlib and any additional dependencies provided as buildInputs or propagatedBuildInputs.\n\nHere is a simple package example.\n\nIt defines an (optional) attribute minimalOCamlVersion (see note below) that will be used to throw a descriptive evaluation error if building with an older OCaml is attempted.\n\nIt uses the fetchFromGitHub fetcher to get its source.\n\nIt also accept duneVersion parameter (valid value are \"1\", \"2\", and \"3\"). The recommended practice it to set only if you don’t want the default value and/or it depends on something else like package version. You might see a not-supported argument useDune2. The behavior was useDune2 = true; => duneVersion = \"2\"; and useDune2 = false; => duneVersion = \"1\";. It was used at the time when dune3 didn’t existed.\n\nIt sets the optional doCheck attribute such that tests will be run with dune runtest -p angstrom after the build (dune build -p angstrom) is complete, but only if the Ocaml version is at at least \"4.05\".\n\nIt uses the package ocaml-syntax-shims as a build input, alcotest and ppx_let as check inputs (because they are needed to run the tests), and bigstringaf and result as propagated build inputs (thus they will also be available to libraries depending on this library).\n\nThe library will be installed using the angstrom.install file that dune generates.\n\n{ lib,\n  fetchFromGitHub,\n  buildDunePackage,\n  ocaml,\n  ocaml-syntax-shims,\n  alcotest,\n  result,\n  bigstringaf,\n  ppx_let }:\n\nbuildDunePackage rec {\n  pname = \"angstrom\";\n  version = \"0.15.0\";\n\n  minimalOCamlVersion = \"4.04\";\n\n  src = fetchFromGitHub {\n    owner  = \"inhabitedtype\";\n    repo   = pname;\n    rev    = version;\n    hash   = \"sha256-MK8o+iPGANEhrrTc1Kz9LBilx2bDPQt7Pp5P2libucI=\";\n  };\n\n  checkInputs = [ alcotest ppx_let ];\n  buildInputs = [ ocaml-syntax-shims ];\n  propagatedBuildInputs = [ bigstringaf result ];\n  doCheck = lib.versionAtLeast ocaml.version \"4.05\";\n\n  meta = {\n    homepage = \"https://github.com/inhabitedtype/angstrom\";\n    description = \"OCaml parser combinators built for speed and memory efficiency\";\n    license = lib.licenses.bsd3;\n    maintainers = with lib.maintainers; [ sternenseemann ];\n  };\n\n\nHere is a second example, this time using a source archive generated with dune-release. It is a good idea to use this archive when it is available as it will usually contain substituted variables such as a %%VERSION%% field. This library does not depend on any other OCaml library and no tests are run after building it.\n\n{ lib, fetchurl, buildDunePackage }:\n\nbuildDunePackage rec {\n  pname = \"wtf8\";\n  version = \"1.0.2\";\n\n  minimalOCamlVersion = \"4.02\";\n\n  src = fetchurl {\n    url = \"https://github.com/flowtype/ocaml-${pname}/releases/download/v${version}/${pname}-v${version}.tbz\";\n    hash = \"sha256-d5/3KUBAWRj8tntr4RkJ74KWW7wvn/B/m1nx0npnzyc=\";\n  };\n\n  meta = with lib; {\n    homepage = \"https://github.com/flowtype/ocaml-wtf8\";\n    description = \"WTF-8 is a superset of UTF-8 that allows unpaired surrogates.\";\n    license = licenses.mit;\n    maintainers = [ maintainers.eqyiel ];\n  };\n}\n\n\nNote about minimalOCamlVersion. A deprecated version of this argument was spelled minimumOCamlVersion; setting the old attribute wrongly modifies the derivation hash and is therefore inappropriate. As a technical dept, currently packaged libraries may still use the old spelling: maintainers are invited to fix this when updating packages. Massive renaming is strongly discouraged as it would be challenging to review, difficult to test, and will cause unnecessary rebuild.\n\nThe build will automatically fail if two distinct versions of the same library are added to buildInputs (which usually happens transitively because of propagatedBuildInputs). Set dontDetectOcamlConflicts to true to disable this behavior.\n\nOctave \nIntroduction\nStructure\nPackaging Octave Packages\nIntroduction \n\nOctave is a modular scientific programming language and environment. A majority of the packages supported by Octave from their website are packaged in nixpkgs.\n\nStructure \n\nAll Octave add-on packages are available in two ways:\n\nUnder the top-level Octave attribute, octave.pkgs.\n\nAs a top-level attribute, octavePackages.\n\nPackaging Octave Packages \n\nNixpkgs provides a function buildOctavePackage, a generic package builder function for any Octave package that complies with the Octave’s current packaging format.\n\nAll Octave packages are defined in pkgs/top-level/octave-packages.nix rather than pkgs/all-packages.nix. Each package is defined in their own file in the pkgs/development/octave-modules directory. Octave packages are made available through all-packages.nix through both the attribute octavePackages and octave.pkgs. You can test building an Octave package as follows:\n\n$ nix-build -A octavePackages.symbolic\n\n\nTo install it into your user profile, run this command from the root of the repository:\n\n$ nix-env -f. -iA octavePackages.symbolic\n\n\nYou can build Octave with packages by using the withPackages passed-through function.\n\n$ nix-shell -p 'octave.withPackages (ps: with ps; [ symbolic ])'\n\n\nThis will also work in a shell.nix file.\n\n{ pkgs ? import <nixpkgs> { }}:\n\npkgs.mkShell {\n  nativeBuildInputs = with pkgs; [\n    (octave.withPackages (opkgs: with opkgs; [ symbolic ]))\n  ];\n}\n\nbuildOctavePackage Steps\n\nThe buildOctavePackage does several things to make sure things work properly.\n\nSets the environment variable OCTAVE_HISTFILE to /dev/null during package compilation so that the commands run through the Octave interpreter directly are not logged.\n\nSkips the configuration step, because the packages are stored as gzipped tarballs, which Octave itself handles directly.\n\nChange the hierarchy of the tarball so that only a single directory is at the top-most level of the tarball.\n\nUse Octave itself to run the pkg build command, which unzips the tarball, extracts the necessary files written in Octave, and compiles any code written in C++ or Fortran, and places the fully compiled artifact in $out.\n\nbuildOctavePackage is built on top of stdenv in a standard way, allowing most things to be customized.\n\nHandling Dependencies\n\nIn Octave packages, there are four sets of dependencies that can be specified:\n\nnativeBuildInputs\n\nJust like other packages, nativeBuildInputs is intended for architecture-dependent build-time-only dependencies.\n\nbuildInputs\n\nLike other packages, buildInputs is intended for architecture-independent build-time-only dependencies.\n\npropagatedBuildInputs\n\nSimilar to other packages, propagatedBuildInputs is intended for packages that are required for both building and running of the package. See Symbolic for how this works and why it is needed.\n\nrequiredOctavePackages\n\nThis is a special dependency that ensures the specified Octave packages are dependent on others, and are made available simultaneously when loading them in Octave.\n\nInstalling Octave Packages\n\nBy default, the buildOctavePackage function does not install the requested package into Octave for use. The function will only build the requested package. This is due to Octave maintaining an text-based database about which packages are installed where. To this end, when all the requested packages have been built, the Octave package and all its add-on packages are put together into an environment, similar to Python.\n\nFirst, all the Octave binaries are wrapped with the environment variable OCTAVE_SITE_INITFILE set to a file in $out, which is required for Octave to be able to find the non-standard package database location.\n\nBecause of the way buildEnv works, all tarballs that are present (which should be all Octave packages to install) should be removed.\n\nThe path down to the default install location of Octave packages is recreated so that Nix-operated Octave can install the packages.\n\nInstall the packages into the $out environment while writing package entries to the database file. This database file is unique for each different (according to Nix) environment invocation.\n\nRewrite the Octave-wide startup file to read from the list of packages installed in that particular environment.\n\nWrap any programs that are required by the Octave packages so that they work with all the paths defined within the environment.\n\nPerl \nRunning Perl programs on the shell\nPackaging Perl programs\nRunning Perl programs on the shell \n\nWhen executing a Perl script, it is possible you get an error such as ./myscript.pl: bad interpreter: /usr/bin/perl: no such file or directory. This happens when the script expects Perl to be installed at /usr/bin/perl, which is not the case when using Perl from nixpkgs. You can fix the script by changing the first line to:\n\n#!/usr/bin/env perl\n\n\nto take the Perl installation from the PATH environment variable, or invoke Perl directly with:\n\n$ perl ./myscript.pl\n\n\nWhen the script is using a Perl library that is not installed globally, you might get an error such as Can't locate DB_File.pm in @INC (you may need to install the DB_File module). In that case, you can use nix-shell to start an ad-hoc shell with that library installed, for instance:\n\n$ nix-shell -p perl perlPackages.DBFile --run ./myscript.pl\n\n\nIf you are always using the script in places where nix-shell is available, you can embed the nix-shell invocation in the shebang like this:\n\n#!/usr/bin/env nix-shell\n#! nix-shell -i perl -p perl perlPackages.DBFile\n\nPackaging Perl programs \n\nNixpkgs provides a function buildPerlPackage, a generic package builder function for any Perl package that has a standard Makefile.PL. It’s implemented in pkgs/development/perl-modules/generic.\n\nPerl packages from CPAN are defined in pkgs/top-level/perl-packages.nix rather than pkgs/all-packages.nix. Most Perl packages are so straight-forward to build that they are defined here directly, rather than having a separate function for each package called from perl-packages.nix. However, more complicated packages should be put in a separate file, typically in pkgs/development/perl-modules. Here is an example of the former:\n\nClassC3 = buildPerlPackage rec {\n  pname = \"Class-C3\";\n  version = \"0.21\";\n  src = fetchurl {\n    url = \"mirror://cpan/authors/id/F/FL/FLORA/${pname}-${version}.tar.gz\";\n    hash = \"sha256-/5GE5xHT0uYGOQxroqj6LMU7CtKn2s6vMVoSXxL4iK4=\";\n  };\n};\n\n\nNote the use of mirror://cpan/, and the pname and version in the URL definition to ensure that the pname attribute is consistent with the source that we’re actually downloading. Perl packages are made available in all-packages.nix through the variable perlPackages. For instance, if you have a package that needs ClassC3, you would typically write\n\nfoo = import ../path/to/foo.nix {\n  inherit stdenv fetchurl ...;\n  inherit (perlPackages) ClassC3;\n};\n\n\nin all-packages.nix. You can test building a Perl package as follows:\n\n$ nix-build -A perlPackages.ClassC3\n\n\nTo install it with nix-env instead: nix-env -f. -iA perlPackages.ClassC3.\n\nSo what does buildPerlPackage do? It does the following:\n\nIn the configure phase, it calls perl Makefile.PL to generate a Makefile. You can set the variable makeMakerFlags to pass flags to Makefile.PL\n\nIt adds the contents of the PERL5LIB environment variable to #! .../bin/perl line of Perl scripts as -Idir flags. This ensures that a script can find its dependencies. (This can cause this shebang line to become too long for Darwin to handle; see the note below.)\n\nIn the fixup phase, it writes the propagated build inputs (propagatedBuildInputs) to the file $out/nix-support/propagated-user-env-packages. nix-env recursively installs all packages listed in this file when you install a package that has it. This ensures that a Perl package can find its dependencies.\n\nbuildPerlPackage is built on top of stdenv, so everything can be customised in the usual way. For instance, the BerkeleyDB module has a preConfigure hook to generate a configuration file used by Makefile.PL:\n\n{ buildPerlPackage, fetchurl, db }:\n\nbuildPerlPackage rec {\n  pname = \"BerkeleyDB\";\n  version = \"0.36\";\n\n  src = fetchurl {\n    url = \"mirror://cpan/authors/id/P/PM/PMQS/${pname}-${version}.tar.gz\";\n    hash = \"sha256-4Y+HGgGQqcOfdiKcFIyMrWBEccVNVAMDBWZlFTMorh8=\";\n  };\n\n  preConfigure = ''\n    echo \"LIB = ${db.out}/lib\" > config.in\n    echo \"INCLUDE = ${db.dev}/include\" >> config.in\n  '';\n}\n\n\nDependencies on other Perl packages can be specified in the buildInputs and propagatedBuildInputs attributes. If something is exclusively a build-time dependency, use buildInputs; if it’s (also) a runtime dependency, use propagatedBuildInputs. For instance, this builds a Perl module that has runtime dependencies on a bunch of other modules:\n\nClassC3Componentised = buildPerlPackage rec {\n  pname = \"Class-C3-Componentised\";\n  version = \"1.0004\";\n  src = fetchurl {\n    url = \"mirror://cpan/authors/id/A/AS/ASH/${pname}-${version}.tar.gz\";\n    hash = \"sha256-ASO9rV/FzJYZ0BH572Fxm2ZrFLMZLFATJng1NuU4FHc=\";\n  };\n  propagatedBuildInputs = [\n    ClassC3 ClassInspector TestException MROCompat\n  ];\n};\n\n\nOn Darwin, if a script has too many -Idir flags in its first line (its “shebang line”), it will not run. This can be worked around by calling the shortenPerlShebang function from the postInstall phase:\n\n{ lib, stdenv, buildPerlPackage, fetchurl, shortenPerlShebang }:\n\nImageExifTool = buildPerlPackage {\n  pname = \"Image-ExifTool\";\n  version = \"12.50\";\n\n  src = fetchurl {\n    url = \"https://exiftool.org/${pname}-${version}.tar.gz\";\n    hash = \"sha256-vOhB/FwQMC8PPvdnjDvxRpU6jAZcC6GMQfc0AH4uwKg=\";\n  };\n\n  nativeBuildInputs = lib.optional stdenv.isDarwin shortenPerlShebang;\n  postInstall = lib.optionalString stdenv.isDarwin ''\n    shortenPerlShebang $out/bin/exiftool\n  '';\n};\n\n\nThis will remove the -I flags from the shebang line, rewrite them in the use lib form, and put them on the next line instead. This function can be given any number of Perl scripts as arguments; it will modify them in-place.\n\nGeneration from CPAN\n\nNix expressions for Perl packages can be generated (almost) automatically from CPAN. This is done by the program nix-generate-from-cpan, which can be installed as follows:\n\n$ nix-env -f \"<nixpkgs>\" -iA nix-generate-from-cpan\n\n\nSubstitute <nixpkgs> by the path of a nixpkgs clone to use the latest version.\n\nThis program takes a Perl module name, looks it up on CPAN, fetches and unpacks the corresponding package, and prints a Nix expression on standard output. For example:\n\n$ nix-generate-from-cpan XML::Simple\n  XMLSimple = buildPerlPackage rec {\n    pname = \"XML-Simple\";\n    version = \"2.22\";\n    src = fetchurl {\n      url = \"mirror://cpan/authors/id/G/GR/GRANTM/XML-Simple-2.22.tar.gz\";\n      hash = \"sha256-uUUO8i6pZErl1q2ghtxDAPoQW+BQogMOvU79KMGY60k=\";\n    };\n    propagatedBuildInputs = [ XMLNamespaceSupport XMLSAX XMLSAXExpat ];\n    meta = {\n      description = \"An API for simple XML files\";\n      license = with lib.licenses; [ artistic1 gpl1Plus ];\n    };\n  };\n\n\nThe output can be pasted into pkgs/top-level/perl-packages.nix or wherever else you need it.\n\nCross-compiling modules\n\nNixpkgs has experimental support for cross-compiling Perl modules. In many cases, it will just work out of the box, even for modules with native extensions. Sometimes, however, the Makefile.PL for a module may (indirectly) import a native module. In that case, you will need to make a stub for that module that will satisfy the Makefile.PL and install it into lib/perl5/site_perl/cross_perl/${perl.version}. See the postInstall for DBI for an example.\n\nPHP \nUser Guide\nUser Guide \nOverview\n\nSeveral versions of PHP are available on Nix, each of which having a wide variety of extensions and libraries available.\n\nThe different versions of PHP that nixpkgs provides are located under attributes named based on major and minor version number; e.g., php81 is PHP 8.1.\n\nOnly versions of PHP that are supported by upstream for the entirety of a given NixOS release will be included in that release of NixOS. See PHP Supported Versions.\n\nThe attribute php refers to the version of PHP considered most stable and thoroughly tested in nixpkgs for any given release of NixOS - not necessarily the latest major release from upstream.\n\nAll available PHP attributes are wrappers around their respective binary PHP package and provide commonly used extensions this way. The real PHP 8.1 package, i.e. the unwrapped one, is available as php81.unwrapped; see the next section for more details.\n\nInteractive tools built on PHP are put in php.packages; composer is for example available at php.packages.composer.\n\nMost extensions that come with PHP, as well as some popular third-party ones, are available in php.extensions; for example, the opcache extension shipped with PHP is available at php.extensions.opcache and the third-party ImageMagick extension at php.extensions.imagick.\n\nInstalling PHP with extensions\n\nA PHP package with specific extensions enabled can be built using php.withExtensions. This is a function which accepts an anonymous function as its only argument; the function should accept two named parameters: enabled - a list of currently enabled extensions and all - the set of all extensions, and return a list of wanted extensions. For example, a PHP package with all default extensions and ImageMagick enabled:\n\nphp.withExtensions ({ enabled, all }:\n  enabled ++ [ all.imagick ])\n\n\nTo exclude some, but not all, of the default extensions, you can filter the enabled list like this:\n\nphp.withExtensions ({ enabled, all }:\n  (lib.filter (e: e != php.extensions.opcache) enabled)\n  ++ [ all.imagick ])\n\n\nTo build your list of extensions from the ground up, you can ignore enabled:\n\nphp.withExtensions ({ all, ... }: with all; [ imagick opcache ])\n\n\nphp.withExtensions provides extensions by wrapping a minimal php base package, providing a php.ini file listing all extensions to be loaded. You can access this package through the php.unwrapped attribute; useful if you, for example, need access to the dev output. The generated php.ini file can be accessed through the php.phpIni attribute.\n\nIf you want a PHP build with extra configuration in the php.ini file, you can use php.buildEnv. This function takes two named and optional parameters: extensions and extraConfig. extensions takes an extension specification equivalent to that of php.withExtensions, extraConfig a string of additional php.ini configuration parameters. For example, a PHP package with the opcache and ImageMagick extensions enabled, and memory_limit set to 256M:\n\nphp.buildEnv {\n  extensions = { all, ... }: with all; [ imagick opcache ];\n  extraConfig = \"memory_limit=256M\";\n}\n\nExample setup for phpfpm\n\nYou can use the previous examples in a phpfpm pool called foo as follows:\n\nlet\n  myPhp = php.withExtensions ({ all, ... }: with all; [ imagick opcache ]);\nin {\n  services.phpfpm.pools.\"foo\".phpPackage = myPhp;\n};\n\nlet\n  myPhp = php.buildEnv {\n    extensions = { all, ... }: with all; [ imagick opcache ];\n    extraConfig = \"memory_limit=256M\";\n  };\nin {\n  services.phpfpm.pools.\"foo\".phpPackage = myPhp;\n};\n\nExample usage with nix-shell\n\nThis brings up a temporary environment that contains a PHP interpreter with the extensions imagick and opcache enabled:\n\nnix-shell -p 'php.withExtensions ({ all, ... }: with all; [ imagick opcache ])'\n\nInstalling PHP packages with extensions\n\nAll interactive tools use the PHP package you get them from, so all packages at php.packages.* use the php package with its default extensions. Sometimes this default set of extensions isn’t enough and you may want to extend it. A common case of this is the composer package: a project may depend on certain extensions and composer won’t work with that project unless those extensions are loaded.\n\nExample of building composer with additional extensions:\n\n(php.withExtensions ({ all, enabled }:\n  enabled ++ (with all; [ imagick redis ]))\n).packages.composer\n\nOverriding PHP packages\n\nphp-packages.nix form a scope, allowing us to override the packages defined within. For example, to apply a patch to a mysqlnd extension, you can pass an overlay-style function to php’s packageOverrides argument:\n\nphp.override {\n  packageOverrides = final: prev: {\n    extensions = prev.extensions // {\n      mysqlnd = prev.extensions.mysqlnd.overrideAttrs (attrs: {\n        patches = attrs.patches or [] ++ [\n          …\n        ];\n      });\n    };\n  };\n}\n\nBuilding PHP projects\n\nWith Composer, you can effectively build PHP projects by streamlining dependency management. As the de-facto standard dependency manager for PHP, Composer enables you to declare and manage the libraries your project relies on, ensuring a more organized and efficient development process.\n\nComposer is not a package manager in the same sense as Yum or Apt are. Yes, it deals with “packages” or libraries, but it manages them on a per-project basis, installing them in a directory (e.g. vendor) inside your project. By default, it does not install anything globally. This idea is not new and Composer is strongly inspired by node’s npm and ruby’s bundler.\n\nCurrently, there is no other PHP tool that offers the same functionality as Composer. Consequently, incorporating a helper in Nix to facilitate building such applications is a logical choice.\n\nIn a Composer project, dependencies are defined in a composer.json file, while their specific versions are locked in a composer.lock file. Some Composer-based projects opt to include this composer.lock file in their source code, while others choose not to.\n\nIn Nix, there are multiple approaches to building a Composer-based project.\n\nOne such method is the php.buildComposerProject helper function, which serves as a wrapper around mkDerivation.\n\nUsing this function, you can build a PHP project that includes both a composer.json and composer.lock file. If the project specifies binaries using the bin attribute in composer.json, these binaries will be automatically linked and made accessible in the derivation. In this context, “binaries” refer to PHP scripts that are intended to be executable.\n\nTo use the helper effectively, add the vendorHash attribute, which enables the wrapper to handle the heavy lifting.\n\nInternally, the helper operates in three stages:\n\nIt constructs a composerRepository attribute derivation by creating a composer repository on the filesystem containing dependencies specified in composer.json. This process uses the function php.mkComposerRepository which in turn uses the php.composerHooks.composerRepositoryHook hook. Internally this function uses a custom Composer plugin to generate the repository.\n\nThe resulting composerRepository derivation is then used by the php.composerHooks.composerInstallHook hook, which is responsible for creating the final vendor directory.\n\nAny “binary” specified in the composer.json are linked and made accessible in the derivation.\n\nAs the autoloader optimization can be activated directly within the composer.json file, we do not enable any autoloader optimization flags.\n\nTo customize the PHP version, you can specify the php attribute. Similarly, if you wish to modify the Composer version, use the composer attribute. It is important to note that both attributes should be of the derivation type.\n\nHere’s an example of working code example using php.buildComposerProject:\n\n{ php, fetchFromGitHub }:\n\nphp.buildComposerProject (finalAttrs: {\n  pname = \"php-app\";\n  version = \"1.0.0\";\n\n  src = fetchFromGitHub {\n    owner = \"git-owner\";\n    repo = \"git-repo\";\n    rev = finalAttrs.version;\n    hash = \"sha256-VcQRSss2dssfkJ+iUb5qT+FJ10GHiFDzySigcmuVI+8=\";\n  };\n\n  # PHP version containing the `ast` extension enabled\n  php = php.buildEnv {\n    extensions = ({ enabled, all }: enabled ++ (with all; [\n      ast\n    ]));\n  };\n\n  # The composer vendor hash\n  vendorHash = \"sha256-86s/F+/5cBAwBqZ2yaGRM5rTGLmou5//aLRK5SA0WiQ=\";\n\n  # If the composer.lock file is missing from the repository, add it:\n  # composerLock = ./path/to/composer.lock;\n})\n\n\nIn case the file composer.lock is missing from the repository, it is possible to specify it using the composerLock attribute.\n\nThe other method is to use all these methods and hooks individually. This has the advantage of building a PHP library within another derivation very easily when necessary.\n\nHere’s a working code example to build a PHP library using mkDerivation and separate functions and hooks:\n\n{ stdenvNoCC, fetchFromGitHub, php }:\n\nstdenvNoCC.mkDerivation (finalAttrs:\nlet\n  src = fetchFromGitHub {\n    owner = \"git-owner\";\n    repo = \"git-repo\";\n    rev = finalAttrs.version;\n    hash = \"sha256-VcQRSss2dssfkJ+iUb5qT+FJ10GHiFDzySigcmuVI+8=\";\n  };\nin {\n  inherit src;\n  pname = \"php-app\";\n  version = \"1.0.0\";\n\n  buildInputs = [ php ];\n\n  nativeBuildInputs = [\n    php.packages.composer\n    # This hook will use the attribute `composerRepository`\n    php.composerHooks.composerInstallHook\n  ];\n\n  composerRepository = php.mkComposerRepository {\n    inherit (finalAttrs) src;\n    # Specifying a custom composer.lock since it is not present in the sources.\n    composerLock = ./composer.lock;\n    # The composer vendor hash\n    vendorHash = \"sha256-86s/F+/5cBAwBqZ2yaGRM5rTGLmou5//aLRK5SA0WiQ=\";\n  };\n})\n\npkg-config \nWriting packages providing pkg-config modules\nAccessing packages via pkg-config module name\n\npkg-config is a unified interface for declaring and querying built C/C++ libraries.\n\nNixpkgs provides a couple of facilities for working with this tool.\n\nWriting packages providing pkg-config modules \n\nPackages should set meta.pkgConfigModules with the list of package config modules they provide. They should also use testers.testMetaPkgConfig to check that the final built package matches that list. Additionally, the validatePkgConfig setup hook, will do extra checks on to-be-installed pkg-config modules.\n\nA good example of all these things is zlib:\n\n{ pkg-config, testers, ... }:\n\nstdenv.mkDerivation (finalAttrs: {\n  ...\n\n  nativeBuildInputs = [ pkg-config validatePkgConfig ];\n\n  passthru.tests.pkg-config = testers.testMetaPkgConfig finalAttrs.finalPackage;\n\n  meta = {\n    ...\n    pkgConfigModules = [ \"zlib\" ];\n  };\n})\n\nAccessing packages via pkg-config module name \nWithin Nixpkgs\n\nA setup hook is bundled in the pkg-config package to bring a derivation’s declared build inputs into the environment. This will populate environment variables like PKG_CONFIG_PATH, PKG_CONFIG_PATH_FOR_BUILD, and PKG_CONFIG_PATH_HOST based on:\n\nhow pkg-config itself is depended upon\n\nhow other dependencies are depended upon\n\nFor more details see the section on specifying dependencies in general.\n\nNormal pkg-config commands to look up dependencies by name will then work with those environment variables defined by the hook.\n\nExternally\n\nThe defaultPkgConfigPackages package set is a set of aliases, named after the modules they provide. This is meant to be used by language-to-nix integrations. Hand-written packages should use the normal Nixpkgs attribute name instead.\n\nPython \nReference\nUser Guide\nFAQ\nContributing\nPackage set maintenance\nCPython Update Schedule\nReference \nInterpreters\nPackage\tAliases\tInterpreter\npython27\tpython2, python\tCPython 2.7\npython38\t\tCPython 3.8\npython39\t\tCPython 3.9\npython310\t\tCPython 3.10\npython311\tpython3\tCPython 3.11\npython312\t\tCPython 3.12\npython313\t\tCPython 3.13\npypy27\tpypy2, pypy\tPyPy2.7\npypy39\tpypy3\tPyPy 3.9\n\nThe Nix expressions for the interpreters can be found in pkgs/development/interpreters/python.\n\nAll packages depending on any Python interpreter get appended out/{python.sitePackages} to $PYTHONPATH if such directory exists.\n\nMissing tkinter module standard library\n\nTo reduce closure size the Tkinter/tkinter is available as a separate package, pythonPackages.tkinter.\n\nAttributes on interpreters packages\n\nEach interpreter has the following attributes:\n\nlibPrefix. Name of the folder in ${python}/lib/ for corresponding interpreter.\n\ninterpreter. Alias for ${python}/bin/${executable}.\n\nbuildEnv. Function to build python interpreter environments with extra packages bundled together. See the section called “python.buildEnv function” for usage and documentation.\n\nwithPackages. Simpler interface to buildEnv. See the section called “python.withPackages function” for usage and documentation.\n\nsitePackages. Alias for lib/${libPrefix}/site-packages.\n\nexecutable. Name of the interpreter executable, e.g. python3.10.\n\npkgs. Set of Python packages for that specific interpreter. The package set can be modified by overriding the interpreter and passing packageOverrides.\n\nBuilding packages and applications\n\nPython libraries and applications that use setuptools or distutils are typically built with respectively the buildPythonPackage and buildPythonApplication functions. These two functions also support installing a wheel.\n\nAll Python packages reside in pkgs/top-level/python-packages.nix and all applications elsewhere. In case a package is used as both a library and an application, then the package should be in pkgs/top-level/python-packages.nix since only those packages are made available for all interpreter versions. The preferred location for library expressions is in pkgs/development/python-modules. It is important that these packages are called from pkgs/top-level/python-packages.nix and not elsewhere, to guarantee the right version of the package is built.\n\nBased on the packages defined in pkgs/top-level/python-packages.nix an attribute set is created for each available Python interpreter. The available sets are\n\npkgs.python27Packages\n\npkgs.python3Packages\n\npkgs.python38Packages\n\npkgs.python39Packages\n\npkgs.python310Packages\n\npkgs.python311Packages\n\npkgs.python312Packages\n\npkgs.python313Packages\n\npkgs.pypyPackages\n\nand the aliases\n\npkgs.python2Packages pointing to pkgs.python27Packages\n\npkgs.python3Packages pointing to pkgs.python311Packages\n\npkgs.pythonPackages pointing to pkgs.python2Packages\n\nbuildPythonPackage function\n\nThe buildPythonPackage function is implemented in pkgs/development/interpreters/python/mk-python-derivation.nix using setup hooks.\n\nThe following is an example:\n\n{ lib\n, buildPythonPackage\n, fetchPypi\n\n# build-system\n, setuptools-scm\n\n# dependencies\n, attrs\n, pluggy\n, py\n, setuptools\n, six\n\n# tests\n, hypothesis\n }:\n\nbuildPythonPackage rec {\n  pname = \"pytest\";\n  version = \"3.3.1\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-z4Q23FnYaVNG/NOrKW3kZCXsqwDWQJbOvnn7Ueyy65M=\";\n  };\n\n  postPatch = ''\n    # don't test bash builtins\n    rm testing/test_argcomplete.py\n  '';\n\n  nativeBuildInputs = [\n    setuptools-scm\n  ];\n\n  propagatedBuildInputs = [\n    attrs\n    py\n    setuptools\n    six\n    pluggy\n  ];\n\n  nativeCheckInputs = [\n    hypothesis\n  ];\n\n  meta = with lib; {\n    changelog = \"https://github.com/pytest-dev/pytest/releases/tag/${version}\";\n    description = \"Framework for writing tests\";\n    homepage = \"https://github.com/pytest-dev/pytest\";\n    license = licenses.mit;\n    maintainers = with maintainers; [ domenkozar lovek323 madjar lsix ];\n  };\n}\n\n\nThe buildPythonPackage mainly does four things:\n\nIn the buildPhase, it calls ${python.pythonOnBuildForHost.interpreter} setup.py bdist_wheel to build a wheel binary zipfile.\n\nIn the installPhase, it installs the wheel file using pip install *.whl.\n\nIn the postFixup phase, the wrapPythonPrograms bash function is called to wrap all programs in the $out/bin/* directory to include $PATH environment variable and add dependent libraries to script’s sys.path.\n\nIn the installCheck phase, ${python.interpreter} setup.py test is run.\n\nBy default tests are run because doCheck = true. Test dependencies, like e.g. the test runner, should be added to nativeCheckInputs.\n\nBy default meta.platforms is set to the same value as the interpreter unless overridden otherwise.\n\nbuildPythonPackage parameters\n\nAll parameters from stdenv.mkDerivation function are still supported. The following are specific to buildPythonPackage:\n\ncatchConflicts ? true: If true, abort package build if a package name appears more than once in dependency tree. Default is true.\n\ndisabled ? false: If true, package is not built for the particular Python interpreter version.\n\ndontWrapPythonPrograms ? false: Skip wrapping of Python programs.\n\npermitUserSite ? false: Skip setting the PYTHONNOUSERSITE environment variable in wrapped programs.\n\npyproject: Whether the pyproject format should be used. When set to true, pypaBuildHook will be used, and you can add the required build dependencies from build-system.requires to nativeBuildInputs. Note that the pyproject format falls back to using setuptools, so you can use pyproject = true even if the package only has a setup.py. When set to false, you can use the existing [hooks](#setup-hooks0 or provide your own logic to build the package. This can be useful for packages that don’t support the pyproject format. When unset, the legacy setuptools hooks are used for backwards compatibility.\n\nmakeWrapperArgs ? []: A list of strings. Arguments to be passed to makeWrapper, which wraps generated binaries. By default, the arguments to makeWrapper set PATH and PYTHONPATH environment variables before calling the binary. Additional arguments here can allow a developer to set environment variables which will be available when the binary is run. For example, makeWrapperArgs = [\"--set FOO BAR\" \"--set BAZ QUX\"].\n\nnamePrefix: Prepends text to ${name} parameter. In case of libraries, this defaults to \"python3.8-\" for Python 3.8, etc., and in case of applications to \"\".\n\npipInstallFlags ? []: A list of strings. Arguments to be passed to pip install. To pass options to python setup.py install, use --install-option. E.g., pipInstallFlags=[\"--install-option='--cpp_implementation'\"].\n\npipBuildFlags ? []: A list of strings. Arguments to be passed to pip wheel.\n\npypaBuildFlags ? []: A list of strings. Arguments to be passed to python -m build --wheel.\n\npythonPath ? []: List of packages to be added into $PYTHONPATH. Packages in pythonPath are not propagated (contrary to propagatedBuildInputs).\n\npreShellHook: Hook to execute commands before shellHook.\n\npostShellHook: Hook to execute commands after shellHook.\n\nremoveBinByteCode ? true: Remove bytecode from /bin. Bytecode is only created when the filenames end with .py.\n\nsetupPyGlobalFlags ? []: List of flags passed to setup.py command.\n\nsetupPyBuildFlags ? []: List of flags passed to setup.py build_ext command.\n\nThe stdenv.mkDerivation function accepts various parameters for describing build inputs (see “Specifying dependencies”). The following are of special interest for Python packages, either because these are primarily used, or because their behaviour is different:\n\nnativeBuildInputs ? []: Build-time only dependencies. Typically executables as well as the items listed in setup_requires.\n\nbuildInputs ? []: Build and/or run-time dependencies that need to be compiled for the host machine. Typically non-Python libraries which are being linked.\n\nnativeCheckInputs ? []: Dependencies needed for running the checkPhase. These are added to nativeBuildInputs when doCheck = true. Items listed in tests_require go here.\n\npropagatedBuildInputs ? []: Aside from propagating dependencies, buildPythonPackage also injects code into and wraps executables with the paths included in this list. Items listed in install_requires go here.\n\nOverriding Python packages\n\nThe buildPythonPackage function has a overridePythonAttrs method that can be used to override the package. In the following example we create an environment where we have the blaze package using an older version of pandas. We override first the Python interpreter and pass packageOverrides which contains the overrides for packages in the package set.\n\nwith import <nixpkgs> {};\n\n(let\n  python = let\n    packageOverrides = self: super: {\n      pandas = super.pandas.overridePythonAttrs(old: rec {\n        version = \"0.19.1\";\n        src =  fetchPypi {\n          pname = \"pandas\";\n          inherit version;\n          hash = \"sha256-JQn+rtpy/OA2deLszSKEuxyttqBzcAil50H+JDHUdCE=\";\n        };\n      });\n    };\n  in pkgs.python3.override {inherit packageOverrides; self = python;};\n\nin python.withPackages(ps: [ ps.blaze ])).env\n\n\nThe next example shows a non trivial overriding of the blas implementation to be used through out all of the Python package set:\n\npython3MyBlas = pkgs.python3.override {\n  packageOverrides = self: super: {\n    # We need toPythonModule for the package set to evaluate this\n    blas = super.toPythonModule(super.pkgs.blas.override {\n      blasProvider = super.pkgs.mkl;\n    });\n    lapack = super.toPythonModule(super.pkgs.lapack.override {\n      lapackProvider = super.pkgs.mkl;\n    });\n  };\n};\n\n\nThis is particularly useful for numpy and scipy users who want to gain speed with other blas implementations. Note that using scipy = super.scipy.override { blas = super.pkgs.mkl; }; will likely result in compilation issues, because scipy dependencies need to use the same blas implementation as well.\n\nbuildPythonApplication function\n\nThe buildPythonApplication function is practically the same as buildPythonPackage. The main purpose of this function is to build a Python package where one is interested only in the executables, and not importable modules. For that reason, when adding this package to a python.buildEnv, the modules won’t be made available.\n\nAnother difference is that buildPythonPackage by default prefixes the names of the packages with the version of the interpreter. Because this is irrelevant for applications, the prefix is omitted.\n\nWhen packaging a Python application with buildPythonApplication, it should be called with callPackage and passed python3 or python3Packages (possibly specifying an interpreter version), like this:\n\n{ lib\n, python3Packages\n, fetchPypi\n}:\n\npython3Packages.buildPythonApplication rec {\n  pname = \"luigi\";\n  version = \"2.7.9\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash  = \"sha256-Pe229rT0aHwA98s+nTHQMEFKZPo/yw6sot8MivFDvAw=\";\n  };\n\n  nativeBuildInputs = [\n    python3Packages.setuptools\n    python3Packages.wheel\n  ];\n\n  propagatedBuildInputs = [\n    python3Packages.tornado\n    python3Packages.python-daemon\n  ];\n\n  meta = with lib; {\n    # ...\n  };\n}\n\n\nThis is then added to all-packages.nix just as any other application would be.\n\nluigi = callPackage ../applications/networking/cluster/luigi { };\n\n\nSince the package is an application, a consumer doesn’t need to care about Python versions or modules, which is why they don’t go in python3Packages.\n\ntoPythonApplication function\n\nA distinction is made between applications and libraries, however, sometimes a package is used as both. In this case the package is added as a library to python-packages.nix and as an application to all-packages.nix. To reduce duplication the toPythonApplication can be used to convert a library to an application.\n\nThe Nix expression shall use buildPythonPackage and be called from python-packages.nix. A reference shall be created from all-packages.nix to the attribute in python-packages.nix, and the toPythonApplication shall be applied to the reference:\n\nyoutube-dl = with python3Packages; toPythonApplication youtube-dl;\n\ntoPythonModule function\n\nIn some cases, such as bindings, a package is created using stdenv.mkDerivation and added as attribute in all-packages.nix. The Python bindings should be made available from python-packages.nix. The toPythonModule function takes a derivation and makes certain Python-specific modifications.\n\nopencv = toPythonModule (pkgs.opencv.override {\n  enablePython = true;\n  pythonPackages = self;\n});\n\n\nDo pay attention to passing in the right Python version!\n\npython.buildEnv function\n\nPython environments can be created using the low-level pkgs.buildEnv function. This example shows how to create an environment that has the Pyramid Web Framework. Saving the following as default.nix\n\nwith import <nixpkgs> {};\n\npython3.buildEnv.override {\n  extraLibs = [ python3Packages.pyramid ];\n  ignoreCollisions = true;\n}\n\n\nand running nix-build will create\n\n/nix/store/cf1xhjwzmdki7fasgr4kz6di72ykicl5-python-2.7.8-env\n\n\nwith wrapped binaries in bin/.\n\nYou can also use the env attribute to create local environments with needed packages installed. This is somewhat comparable to virtualenv. For example, running nix-shell with the following shell.nix\n\nwith import <nixpkgs> {};\n\n(python3.buildEnv.override {\n  extraLibs = with python3Packages; [\n    numpy\n    requests\n  ];\n}).env\n\n\nwill drop you into a shell where Python will have the specified packages in its path.\n\npython.buildEnv arguments\n\nextraLibs: List of packages installed inside the environment.\n\npostBuild: Shell command executed after the build of environment.\n\nignoreCollisions: Ignore file collisions inside the environment (default is false).\n\npermitUserSite: Skip setting the PYTHONNOUSERSITE environment variable in wrapped binaries in the environment.\n\npython.withPackages function\n\nThe python.withPackages function provides a simpler interface to the python.buildEnv functionality. It takes a function as an argument that is passed the set of python packages and returns the list of the packages to be included in the environment. Using the withPackages function, the previous example for the Pyramid Web Framework environment can be written like this:\n\nwith import <nixpkgs> {};\n\npython.withPackages (ps: [ ps.pyramid ])\n\n\nwithPackages passes the correct package set for the specific interpreter version as an argument to the function. In the above example, ps equals pythonPackages. But you can also easily switch to using python3:\n\nwith import <nixpkgs> {};\n\npython3.withPackages (ps: [ ps.pyramid ])\n\n\nNow, ps is set to python3Packages, matching the version of the interpreter.\n\nAs python.withPackages uses python.buildEnv under the hood, it also supports the env attribute. The shell.nix file from the previous section can thus be also written like this:\n\nwith import <nixpkgs> {};\n\n(python3.withPackages (ps: with ps; [\n  numpy\n  requests\n])).env\n\n\nIn contrast to python.buildEnv, python.withPackages does not support the more advanced options such as ignoreCollisions = true or postBuild. If you need them, you have to use python.buildEnv.\n\nPython 2 namespace packages may provide __init__.py that collide. In that case python.buildEnv should be used with ignoreCollisions = true.\n\nSetup hooks\n\nThe following are setup hooks specifically for Python packages. Most of these are used in buildPythonPackage.\n\neggUnpackhook to move an egg to the correct folder so it can be installed with the eggInstallHook\n\neggBuildHook to skip building for eggs.\n\neggInstallHook to install eggs.\n\npipBuildHook to build a wheel using pip and PEP 517. Note a build system (e.g. setuptools or flit) should still be added as nativeBuildInput.\n\npypaBuildHook to build a wheel using pypa/build and PEP 517/518. Note a build system (e.g. setuptools or flit) should still be added as nativeBuildInput.\n\npipInstallHook to install wheels.\n\npytestCheckHook to run tests with pytest. See example usage.\n\npythonCatchConflictsHook to check whether a Python package is not already existing.\n\npythonImportsCheckHook to check whether importing the listed modules works.\n\npythonRelaxDepsHook will relax Python dependencies restrictions for the package. See example usage.\n\npythonRemoveBinBytecode to remove bytecode from the /bin folder.\n\nsetuptoolsBuildHook to build a wheel using setuptools.\n\nsetuptoolsCheckHook to run tests with python setup.py test.\n\nsphinxHook to build documentation and manpages using Sphinx.\n\nvenvShellHook to source a Python 3 venv at the venvDir location. A venv is created if it does not yet exist. postVenvCreation can be used to to run commands only after venv is first created.\n\nwheelUnpackHook to move a wheel to the correct folder so it can be installed with the pipInstallHook.\n\nunittestCheckHook will run tests with python -m unittest discover. See example usage.\n\nDevelopment mode\n\nDevelopment or editable mode is supported. To develop Python packages buildPythonPackage has additional logic inside shellPhase to run pip install -e . --prefix $TMPDIR/for the package.\n\nWarning: shellPhase is executed only if setup.py exists.\n\nGiven a default.nix:\n\nwith import <nixpkgs> {};\n\npython3Packages.buildPythonPackage {\n  name = \"myproject\";\n  buildInputs = with python3Packages; [ pyramid ];\n\n  src = ./.;\n}\n\n\nRunning nix-shell with no arguments should give you the environment in which the package would be built with nix-build.\n\nShortcut to setup environments with C headers/libraries and Python packages:\n\nnix-shell -p python3Packages.pyramid zlib libjpeg git\n\nNote\n\nThere is a boolean value lib.inNixShell set to true if nix-shell is invoked.\n\nUser Guide \nUsing Python\nOverview\n\nSeveral versions of the Python interpreter are available on Nix, as well as a high amount of packages. The attribute python3 refers to the default interpreter, which is currently CPython 3.11. The attribute python refers to CPython 2.7 for backwards-compatibility. It is also possible to refer to specific versions, e.g. python311 refers to CPython 3.11, and pypy refers to the default PyPy interpreter.\n\nPython is used a lot, and in different ways. This affects also how it is packaged. In the case of Python on Nix, an important distinction is made between whether the package is considered primarily an application, or whether it should be used as a library, i.e., of primary interest are the modules in site-packages that should be importable.\n\nIn the Nixpkgs tree Python applications can be found throughout, depending on what they do, and are called from the main package set. Python libraries, however, are in separate sets, with one set per interpreter version.\n\nThe interpreters have several common attributes. One of these attributes is pkgs, which is a package set of Python libraries for this specific interpreter. E.g., the toolz package corresponding to the default interpreter is python3.pkgs.toolz, and the CPython 3.11 version is python311.pkgs.toolz. The main package set contains aliases to these package sets, e.g. pythonPackages refers to python.pkgs and python311Packages to python311.pkgs.\n\nInstalling Python and packages\n\nThe Nix and NixOS manuals explain how packages are generally installed. In the case of Python and Nix, it is important to make a distinction between whether the package is considered an application or a library.\n\nApplications on Nix are typically installed into your user profile imperatively using nix-env -i, and on NixOS declaratively by adding the package name to environment.systemPackages in /etc/nixos/configuration.nix. Dependencies such as libraries are automatically installed and should not be installed explicitly.\n\nThe same goes for Python applications. Python applications can be installed in your profile, and will be wrapped to find their exact library dependencies, without impacting other applications or polluting your user environment.\n\nBut Python libraries you would like to use for development cannot be installed, at least not individually, because they won’t be able to find each other resulting in import errors. Instead, it is possible to create an environment with python.buildEnv or python.withPackages where the interpreter and other executables are wrapped to be able to find each other and all of the modules.\n\nIn the following examples we will start by creating a simple, ad-hoc environment with a nix-shell that has numpy and toolz in Python 3.11; then we will create a re-usable environment in a single-file Python script; then we will create a full Python environment for development with this same environment.\n\nPhilosophically, this should be familiar to users who are used to a venv style of development: individual projects create their own Python environments without impacting the global environment or each other.\n\nAd-hoc temporary Python environment with nix-shell\n\nThe simplest way to start playing with the way nix wraps and sets up Python environments is with nix-shell at the cmdline. These environments create a temporary shell session with a Python and a precise list of packages (plus their runtime dependencies), with no other Python packages in the Python interpreter’s scope.\n\nTo create a Python 3.11 session with numpy and toolz available, run:\n\n$ nix-shell -p 'python311.withPackages(ps: with ps; [ numpy toolz ])'\n\n\nBy default nix-shell will start a bash session with this interpreter in our PATH, so if we then run:\n\n[nix-shell:~/src/nixpkgs]$ python3\nPython 3.11.3 (main, Apr  4 2023, 22:36:41) [GCC 12.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import numpy; import toolz\n\n\nNote that no other modules are in scope, even if they were imperatively installed into our user environment as a dependency of a Python application:\n\n>>> import requests\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'requests'\n\n\nWe can add as many additional modules onto the nix-shell as we need, and we will still get 1 wrapped Python interpreter. We can start the interpreter directly like so:\n\n$ nix-shell -p \"python311.withPackages (ps: with ps; [ numpy toolz requests ])\" --run python3\nthis derivation will be built:\n  /nix/store/r19yf5qgfiakqlhkgjahbg3zg79549n4-python3-3.11.2-env.drv\nbuilding '/nix/store/r19yf5qgfiakqlhkgjahbg3zg79549n4-python3-3.11.2-env.drv'...\ncreated 273 symlinks in user environment\nPython 3.11.2 (main, Feb  7 2023, 13:52:42) [GCC 12.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import requests\n>>>\n\n\nNotice that this time it built a new Python environment, which now includes requests. Building an environment just creates wrapper scripts that expose the selected dependencies to the interpreter while re-using the actual modules. This means if any other env has installed requests or numpy in a different context, we don’t need to recompile them – we just recompile the wrapper script that sets up an interpreter pointing to them. This matters much more for “big” modules like pytorch or tensorflow.\n\nModule names usually match their names on pypi.org, but you can use the Nixpkgs search website to find them as well (along with non-python packages).\n\nAt this point we can create throwaway experimental Python environments with arbitrary dependencies. This is a good way to get a feel for how the Python interpreter and dependencies work in Nix and NixOS, but to do some actual development, we’ll want to make it a bit more persistent.\n\nRunning Python scripts and using nix-shell as shebang\n\nSometimes, we have a script whose header looks like this:\n\n#!/usr/bin/env python3\nimport numpy as np\na = np.array([1,2])\nb = np.array([3,4])\nprint(f\"The dot product of {a} and {b} is: {np.dot(a, b)}\")\n\n\nExecuting this script requires a python3 that has numpy. Using what we learned in the previous section, we could startup a shell and just run it like so:\n\n$ nix-shell -p 'python311.withPackages (ps: with ps; [ numpy ])' --run 'python3 foo.py'\nThe dot product of [1 2] and [3 4] is: 11\n\n\nBut if we maintain the script ourselves, and if there are more dependencies, it may be nice to encode those dependencies in source to make the script re-usable without that bit of knowledge. That can be done by using nix-shell as a shebang, like so:\n\n#!/usr/bin/env nix-shell\n#!nix-shell -i python3 -p \"python3.withPackages(ps: [ ps.numpy ])\"\nimport numpy as np\na = np.array([1,2])\nb = np.array([3,4])\nprint(f\"The dot product of {a} and {b} is: {np.dot(a, b)}\")\n\n\nThen we execute it, without requiring any environment setup at all!\n\n$ ./foo.py\nThe dot product of [1 2] and [3 4] is: 11\n\n\nIf the dependencies are not available on the host where foo.py is executed, it will build or download them from a Nix binary cache prior to starting up, prior that it is executed on a machine with a multi-user nix installation.\n\nThis provides a way to ship a self bootstrapping Python script, akin to a statically linked binary, where it can be run on any machine (provided nix is installed) without having to assume that numpy is installed globally on the system.\n\nBy default it is pulling the import checkout of Nixpkgs itself from our nix channel, which is nice as it cache aligns with our other package builds, but we can make it fully reproducible by pinning the nixpkgs import:\n\n#!/usr/bin/env nix-shell\n#!nix-shell -i python3 -p \"python3.withPackages (ps: [ ps.numpy ])\"\n#!nix-shell -I nixpkgs=https://github.com/NixOS/nixpkgs/archive/e51209796c4262bfb8908e3d6d72302fe4e96f5f.tar.gz\nimport numpy as np\na = np.array([1,2])\nb = np.array([3,4])\nprint(f\"The dot product of {a} and {b} is: {np.dot(a, b)}\")\n\n\nThis will execute with the exact same versions of Python 3.10, numpy, and system dependencies a year from now as it does today, because it will always use exactly git commit e51209796c4262bfb8908e3d6d72302fe4e96f5f of Nixpkgs for all of the package versions.\n\nThis is also a great way to ensure the script executes identically on different servers.\n\nLoad environment from .nix expression\n\nWe’ve now seen how to create an ad-hoc temporary shell session, and how to create a single script with Python dependencies, but in the course of normal development we’re usually working in an entire package repository.\n\nAs explained in the nix-shell section of the Nix manual, nix-shell can also load an expression from a .nix file. Say we want to have Python 3.11, numpy and toolz, like before, in an environment. We can add a shell.nix file describing our dependencies:\n\nwith import <nixpkgs> {};\n(python311.withPackages (ps: with ps; [\n  numpy\n  toolz\n])).env\n\n\nAnd then at the command line, just typing nix-shell produces the same environment as before. In a normal project, we’ll likely have many more dependencies; this can provide a way for developers to share the environments with each other and with CI builders.\n\nWhat’s happening here?\n\nWe begin with importing the Nix Packages collections. import <nixpkgs> imports the <nixpkgs> function, {} calls it and the with statement brings all attributes of nixpkgs in the local scope. These attributes form the main package set.\n\nThen we create a Python 3.11 environment with the withPackages function, as before.\n\nThe withPackages function expects us to provide a function as an argument that takes the set of all Python packages and returns a list of packages to include in the environment. Here, we select the packages numpy and toolz from the package set.\n\nTo combine this with mkShell you can:\n\nwith import <nixpkgs> {};\nlet\n  pythonEnv = python311.withPackages (ps: [\n    ps.numpy\n    ps.toolz\n  ]);\nin mkShell {\n  packages = [\n    pythonEnv\n\n    black\n    mypy\n\n    libffi\n    openssl\n  ];\n}\n\n\nThis will create a unified environment that has not just our Python interpreter and its Python dependencies, but also tools like black or mypy and libraries like libffi the openssl in scope. This is generic and can span any number of tools or languages across the Nixpkgs ecosystem.\n\nInstalling environments globally on the system\n\nUp to now, we’ve been creating environments scoped to an ad-hoc shell session, or a single script, or a single project. This is generally advisable, as it avoids pollution across contexts.\n\nHowever, sometimes we know we will often want a Python with some basic packages, and want this available without having to enter into a shell or build context. This can be useful to have things like vim/emacs editors and plugins or shell tools “just work” without having to set them up, or when running other software that expects packages to be installed globally.\n\nTo create your own custom environment, create a file in ~/.config/nixpkgs/overlays/ that looks like this:\n\n# ~/.config/nixpkgs/overlays/myEnv.nix\nself: super: {\n  myEnv = super.buildEnv {\n    name = \"myEnv\";\n    paths = [\n      # A Python 3 interpreter with some packages\n      (self.python3.withPackages (\n        ps: with ps; [\n          pyflakes\n          pytest\n          black\n        ]\n      ))\n\n      # Some other packages we'd like as part of this env\n      self.mypy\n      self.black\n      self.ripgrep\n      self.tmux\n    ];\n  };\n}\n\n\nYou can then build and install this to your profile with:\n\nnix-env -iA myEnv\n\n\nOne limitation of this is that you can only have 1 Python env installed globally, since they conflict on the python to load out of your PATH.\n\nIf you get a conflict or prefer to keep the setup clean, you can have nix-env atomically uninstall all other imperatively installed packages and replace your profile with just myEnv by using the --replace flag.\n\nEnvironment defined in /etc/nixos/configuration.nix\n\nFor the sake of completeness, here’s how to install the environment system-wide on NixOS.\n\n{ # ...\n\n  environment.systemPackages = with pkgs; [\n    (python310.withPackages(ps: with ps; [ numpy toolz ]))\n  ];\n}\n\nDeveloping with Python\n\nAbove, we were mostly just focused on use cases and what to do to get started creating working Python environments in nix.\n\nNow that you know the basics to be up and running, it is time to take a step back and take a deeper look at how Python packages are packaged on Nix. Then, we will look at how you can use development mode with your code.\n\nPython library packages in Nixpkgs\n\nWith Nix all packages are built by functions. The main function in Nix for building Python libraries is buildPythonPackage. Let’s see how we can build the toolz package.\n\n{ lib\n, buildPythonPackage\n, fetchPypi\n, setuptools\n, wheel\n}:\n\nbuildPythonPackage rec {\n  pname = \"toolz\";\n  version = \"0.10.0\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-CP3V73yWSArRHBLUct4hrNMjWZlvaaUlkpm1QP66RWA=\";\n  };\n\n  nativeBuildInputs = [\n    setuptools\n    wheel\n  ];\n\n  # has no tests\n  doCheck = false;\n\n  pythonImportsCheck = [\n    \"toolz.itertoolz\"\n    \"toolz.functoolz\"\n    \"toolz.dicttoolz\"\n  ];\n\n  meta = with lib; {\n    changelog = \"https://github.com/pytoolz/toolz/releases/tag/${version}\";\n    homepage = \"https://github.com/pytoolz/toolz\";\n    description = \"List processing tools and functional utilities\";\n    license = licenses.bsd3;\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n\n\nWhat happens here? The function buildPythonPackage is called and as argument it accepts a set. In this case the set is a recursive set, rec. One of the arguments is the name of the package, which consists of a basename (generally following the name on PyPi) and a version. Another argument, src specifies the source, which in this case is fetched from PyPI using the helper function fetchPypi. The argument doCheck is used to set whether tests should be run when building the package. Since there are no tests, we rely on pythonImportsCheck to test whether the package can be imported. Furthermore, we specify some meta information. The output of the function is a derivation.\n\nAn expression for toolz can be found in the Nixpkgs repository. As explained in the introduction of this Python section, a derivation of toolz is available for each interpreter version, e.g. python311.pkgs.toolz refers to the toolz derivation corresponding to the CPython 3.11 interpreter.\n\nThe above example works when you’re directly working on pkgs/top-level/python-packages.nix in the Nixpkgs repository. Often though, you will want to test a Nix expression outside of the Nixpkgs tree.\n\nThe following expression creates a derivation for the toolz package, and adds it along with a numpy package to a Python environment.\n\nwith import <nixpkgs> {};\n\n( let\n    my_toolz = python311.pkgs.buildPythonPackage rec {\n      pname = \"toolz\";\n      version = \"0.10.0\";\n      pyproject = true;\n\n      src = fetchPypi {\n        inherit pname version;\n        hash = \"sha256-CP3V73yWSArRHBLUct4hrNMjWZlvaaUlkpm1QP66RWA=\";\n      };\n\n      nativeBuildInputs = [\n        python311.pkgs.setuptools\n        python311.pkgs.wheel\n      ];\n\n      # has no tests\n      doCheck = false;\n\n      meta = {\n        homepage = \"https://github.com/pytoolz/toolz/\";\n        description = \"List processing tools and functional utilities\";\n        # [...]\n      };\n    };\n\n  in python311.withPackages (ps: with ps; [\n    numpy\n    my_toolz\n  ])\n).env\n\n\nExecuting nix-shell will result in an environment in which you can use Python 3.11 and the toolz package. As you can see we had to explicitly mention for which Python version we want to build a package.\n\nSo, what did we do here? Well, we took the Nix expression that we used earlier to build a Python environment, and said that we wanted to include our own version of toolz, named my_toolz. To introduce our own package in the scope of withPackages we used a let expression. You can see that we used ps.numpy to select numpy from the nixpkgs package set (ps). We did not take toolz from the Nixpkgs package set this time, but instead took our own version that we introduced with the let expression.\n\nHandling dependencies\n\nOur example, toolz, does not have any dependencies on other Python packages or system libraries. According to the manual, buildPythonPackage uses the arguments buildInputs and propagatedBuildInputs to specify dependencies. If something is exclusively a build-time dependency, then the dependency should be included in buildInputs, but if it is (also) a runtime dependency, then it should be added to propagatedBuildInputs. Test dependencies are considered build-time dependencies and passed to nativeCheckInputs.\n\nThe following example shows which arguments are given to buildPythonPackage in order to build datashape.\n\n{ lib\n, buildPythonPackage\n, fetchPypi\n\n# build dependencies\n, setuptools, wheel\n\n# dependencies\n, numpy, multipledispatch, python-dateutil\n\n# tests\n, pytest\n}:\n\nbuildPythonPackage rec {\n  pname = \"datashape\";\n  version = \"0.4.7\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-FLLvdm1MllKrgTGC6Gb0k0deZeVYvtCCLji/B7uhong=\";\n  };\n\n  nativeBuildInputs = [\n    setuptools\n    wheel\n  ];\n\n  propagatedBuildInputs = [\n    multipledispatch\n    numpy\n    python-dateutil\n  ];\n\n  nativeCheckInputs = [\n    pytest\n  ];\n\n  meta = with lib; {\n    changelog = \"https://github.com/blaze/datashape/releases/tag/${version}\";\n    homepage = \"https://github.com/ContinuumIO/datashape\";\n    description = \"A data description language\";\n    license = licenses.bsd2;\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n\n\nWe can see several runtime dependencies, numpy, multipledispatch, and python-dateutil. Furthermore, we have nativeCheckInputs with pytest. pytest is a test runner and is only used during the checkPhase and is therefore not added to propagatedBuildInputs.\n\nIn the previous case we had only dependencies on other Python packages to consider. Occasionally you have also system libraries to consider. E.g., lxml provides Python bindings to libxml2 and libxslt. These libraries are only required when building the bindings and are therefore added as buildInputs.\n\n{ lib\n, buildPythonPackage\n, fetchPypi\n, setuptools\n, wheel\n, libxml2\n, libxslt\n}:\n\nbuildPythonPackage rec {\n  pname = \"lxml\";\n  version = \"3.4.4\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-s9NiusRxFydHzaNRMjjxFcvWxfi45jGb9ql6eJJyQJk=\";\n  };\n\n  nativeBuildInputs = [\n    setuptools\n    wheel\n  ];\n\n  buildInputs = [\n    libxml2\n    libxslt\n  ];\n\n  meta = with lib; {\n    changelog = \"https://github.com/lxml/lxml/releases/tag/lxml-${version}\";\n    description = \"Pythonic binding for the libxml2 and libxslt libraries\";\n    homepage = \"https://lxml.de\";\n    license = licenses.bsd3;\n    maintainers = with maintainers; [ sjourdois ];\n  };\n}\n\n\nIn this example lxml and Nix are able to work out exactly where the relevant files of the dependencies are. This is not always the case.\n\nThe example below shows bindings to The Fastest Fourier Transform in the West, commonly known as FFTW. On Nix we have separate packages of FFTW for the different types of floats (\"single\", \"double\", \"long-double\"). The bindings need all three types, and therefore we add all three as buildInputs. The bindings don’t expect to find each of them in a different folder, and therefore we have to set LDFLAGS and CFLAGS.\n\n{ lib\n, buildPythonPackage\n, fetchPypi\n\n# build dependencies\n, setuptools\n, wheel\n\n# dependencies\n, fftw\n, fftwFloat\n, fftwLongDouble\n, numpy\n, scipy\n}:\n\nbuildPythonPackage rec {\n  pname = \"pyFFTW\";\n  version = \"0.9.2\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-9ru2r6kwhUCaskiFoaPNuJCfCVoUL01J40byvRt4kHQ=\";\n  };\n\n  nativeBuildInputs = [\n    setuptools\n    wheel\n  ];\n\n  buildInputs = [\n    fftw\n    fftwFloat\n    fftwLongDouble\n  ];\n\n  propagatedBuildInputs = [\n    numpy\n    scipy\n  ];\n\n  preConfigure = ''\n    export LDFLAGS=\"-L${fftw.dev}/lib -L${fftwFloat.out}/lib -L${fftwLongDouble.out}/lib\"\n    export CFLAGS=\"-I${fftw.dev}/include -I${fftwFloat.dev}/include -I${fftwLongDouble.dev}/include\"\n  '';\n\n  # Tests cannot import pyfftw. pyfftw works fine though.\n  doCheck = false;\n\n  meta = with lib; {\n    changelog = \"https://github.com/pyFFTW/pyFFTW/releases/tag/v${version}\";\n    description = \"A pythonic wrapper around FFTW, the FFT library, presenting a unified interface for all the supported transforms\";\n    homepage = \"http://hgomersall.github.com/pyFFTW\";\n    license = with licenses; [ bsd2 bsd3 ];\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n\n\nNote also the line doCheck = false;, we explicitly disabled running the test-suite.\n\nTesting Python Packages\n\nIt is highly encouraged to have testing as part of the package build. This helps to avoid situations where the package was able to build and install, but is not usable at runtime. Currently, all packages will use the test command provided by the setup.py (i.e. python setup.py test). However, this is currently deprecated https://github.com/pypa/setuptools/pull/1878 and your package should provide its own checkPhase.\n\nNote\n\nThe checkPhase for python maps to the installCheckPhase on a normal derivation. This is due to many python packages not behaving well to the pre-installed version of the package. Version info, and natively compiled extensions generally only exist in the install directory, and thus can cause issues when a test suite asserts on that behavior.\n\nNote\n\nTests should only be disabled if they don’t agree with nix (e.g. external dependencies, network access, flakey tests), however, as many tests should be enabled as possible. Failing tests can still be a good indication that the package is not in a valid state.\n\nUsing pytest\n\nPytest is the most common test runner for python repositories. A trivial test run would be:\n\n  nativeCheckInputs = [ pytest ];\n  checkPhase = ''\n    runHook preCheck\n\n    pytest\n\n    runHook postCheck\n  '';\n\n\nHowever, many repositories’ test suites do not translate well to nix’s build sandbox, and will generally need many tests to be disabled.\n\nTo filter tests using pytest, one can do the following:\n\n  nativeCheckInputs = [ pytest ];\n  # avoid tests which need additional data or touch network\n  checkPhase = ''\n    runHook preCheck\n\n    pytest tests/ --ignore=tests/integration -k 'not download and not update' --ignore=tests/test_failing.py\n\n    runHook postCheck\n  '';\n\n\n--ignore will tell pytest to ignore that file or directory from being collected as part of a test run. This is useful is a file uses a package which is not available in nixpkgs, thus skipping that test file is much easier than having to create a new package.\n\n-k is used to define a predicate for test names. In this example, we are filtering out tests which contain download or update in their test case name. Only one -k argument is allowed, and thus a long predicate should be concatenated with “\\” and wrapped to the next line.\n\nNote\n\nIn pytest==6.0.1, the use of “\\” to continue a line (e.g. -k 'not download \\') has been removed, in this case, it’s recommended to use pytestCheckHook.\n\nUsing pytestCheckHook\n\npytestCheckHook is a convenient hook which will substitute the setuptools test command for a checkPhase which runs pytest. This is also beneficial when a package may need many items disabled to run the test suite.\n\nUsing the example above, the analogous pytestCheckHook usage would be:\n\n  nativeCheckInputs = [\n    pytestCheckHook\n  ];\n\n  # requires additional data\n  pytestFlagsArray = [\n    \"tests/\"\n    \"--ignore=tests/integration\"\n  ];\n\n  disabledTests = [\n    # touches network\n    \"download\"\n    \"update\"\n  ];\n\n  disabledTestPaths = [\n    \"tests/test_failing.py\"\n  ];\n\n\nThis is especially useful when tests need to be conditionally disabled, for example:\n\n  disabledTests = [\n    # touches network\n    \"download\"\n    \"update\"\n  ] ++ lib.optionals (pythonAtLeast \"3.8\") [\n    # broken due to python3.8 async changes\n    \"async\"\n  ] ++ lib.optionals stdenv.isDarwin [\n    # can fail when building with other packages\n    \"socket\"\n  ];\n\n\nTrying to concatenate the related strings to disable tests in a regular checkPhase would be much harder to read. This also enables us to comment on why specific tests are disabled.\n\nUsing pythonImportsCheck\n\nAlthough unit tests are highly preferred to validate correctness of a package, not all packages have test suites that can be run easily, and some have none at all. To help ensure the package still works, pythonImportsCheck can attempt to import the listed modules.\n\n  pythonImportsCheck = [\n    \"requests\"\n    \"urllib\"\n  ];\n\n\nroughly translates to:\n\n  postCheck = ''\n    PYTHONPATH=$out/${python.sitePackages}:$PYTHONPATH\n    python -c \"import requests; import urllib\"\n  '';\n\n\nHowever, this is done in its own phase, and not dependent on whether doCheck = true;.\n\nThis can also be useful in verifying that the package doesn’t assume commonly present packages (e.g. setuptools).\n\nUsing pythonRelaxDepsHook\n\nIt is common for upstream to specify a range of versions for its package dependencies. This makes sense, since it ensures that the package will be built with a subset of packages that is well tested. However, this commonly causes issues when packaging in Nixpkgs, because the dependencies that this package may need are too new or old for the package to build correctly. We also cannot package multiple versions of the same package since this may cause conflicts in PYTHONPATH.\n\nOne way to side step this issue is to relax the dependencies. This can be done by either removing the package version range or by removing the package declaration entirely. This can be done using the pythonRelaxDepsHook hook. For example, given the following requirements.txt file:\n\npkg1<1.0\npkg2\npkg3>=1.0,<=2.0\n\n\nwe can do:\n\n  nativeBuildInputs = [\n    pythonRelaxDepsHook\n  ];\n  pythonRelaxDeps = [\n    \"pkg1\"\n    \"pkg3\"\n  ];\n  pythonRemoveDeps = [\n    \"pkg2\"\n  ];\n\n\nwhich would result in the following requirements.txt file:\n\npkg1\npkg3\n\n\nAnother option is to pass true, that will relax/remove all dependencies, for example:\n\n  nativeBuildInputs = [ pythonRelaxDepsHook ];\n  pythonRelaxDeps = true;\n\n\nwhich would result in the following requirements.txt file:\n\npkg1\npkg2\npkg3\n\n\nIn general you should always use pythonRelaxDeps, because pythonRemoveDeps will convert build errors into runtime errors. However pythonRemoveDeps may still be useful in exceptional cases, and also to remove dependencies wrongly declared by upstream (for example, declaring black as a runtime dependency instead of a dev dependency).\n\nKeep in mind that while the examples above are done with requirements.txt, pythonRelaxDepsHook works by modifying the resulting wheel file, so it should work with any of the existing hooks.\n\nUsing unittestCheckHook\n\nunittestCheckHook is a hook which will substitute the setuptools test command for a checkPhase which runs python -m unittest discover:\n\n  nativeCheckInputs = [\n    unittestCheckHook\n  ];\n\n  unittestFlagsArray = [\n    \"-s\" \"tests\" \"-v\"\n  ];\n\nUsing sphinxHook\n\nThe sphinxHook is a helpful tool to build documentation and manpages using the popular Sphinx documentation generator. It is setup to automatically find common documentation source paths and render them using the default html style.\n\n  outputs = [\n    \"out\"\n    \"doc\"\n  ];\n\n  nativeBuildInputs = [\n    sphinxHook\n  ];\n\n\nThe hook will automatically build and install the artifact into the doc output, if it exists. It also provides an automatic diversion for the artifacts of the man builder into the man target.\n\n  outputs = [\n    \"out\"\n    \"doc\"\n    \"man\"\n  ];\n\n  # Use multiple builders\n  sphinxBuilders = [\n    \"singlehtml\"\n    \"man\"\n  ];\n\n\nOverwrite sphinxRoot when the hook is unable to find your documentation source root.\n\n  # Configure sphinxRoot for uncommon paths\n  sphinxRoot = \"weird/docs/path\";\n\n\nThe hook is also available to packages outside the python ecosystem by referencing it using sphinxHook from top-level.\n\nDevelop local package\n\nAs a Python developer you’re likely aware of development mode (python setup.py develop); instead of installing the package this command creates a special link to the project code. That way, you can run updated code without having to reinstall after each and every change you make. Development mode is also available. Let’s see how you can use it.\n\nIn the previous Nix expression the source was fetched from a url. We can also refer to a local source instead using src = ./path/to/source/tree;\n\nIf we create a shell.nix file which calls buildPythonPackage, and if src is a local source, and if the local source has a setup.py, then development mode is activated.\n\nIn the following example, we create a simple environment that has a Python 3.11 version of our package in it, as well as its dependencies and other packages we like to have in the environment, all specified with propagatedBuildInputs. Indeed, we can just add any package we like to have in our environment to propagatedBuildInputs.\n\nwith import <nixpkgs> {};\nwith python311Packages;\n\nbuildPythonPackage rec {\n  name = \"mypackage\";\n  src = ./path/to/package/source;\n  propagatedBuildInputs = [\n    pytest\n    numpy\n    pkgs.libsndfile\n  ];\n}\n\n\nIt is important to note that due to how development mode is implemented on Nix it is not possible to have multiple packages simultaneously in development mode.\n\nOrganising your packages\n\nSo far we discussed how you can use Python on Nix, and how you can develop with it. We’ve looked at how you write expressions to package Python packages, and we looked at how you can create environments in which specified packages are available.\n\nAt some point you’ll likely have multiple packages which you would like to be able to use in different projects. In order to minimise unnecessary duplication we now look at how you can maintain a repository with your own packages. The important functions here are import and callPackage.\n\nIncluding a derivation using callPackage\n\nEarlier we created a Python environment using withPackages, and included the toolz package via a let expression. Let’s split the package definition from the environment definition.\n\nWe first create a function that builds toolz in ~/path/to/toolz/release.nix\n\n{ lib\n, buildPythonPackage\n, fetchPypi\n, setuptools\n, wheel\n}:\n\nbuildPythonPackage rec {\n  pname = \"toolz\";\n  version = \"0.10.0\";\n  pyproject = true;\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-CP3V73yWSArRHBLUct4hrNMjWZlvaaUlkpm1QP66RWA=\";\n  };\n\n  nativeBuildInputs = [\n    setuptools\n    wheel\n  ];\n\n  meta = with lib; {\n    changelog = \"https://github.com/pytoolz/toolz/releases/tag/${version}\";\n    homepage = \"https://github.com/pytoolz/toolz/\";\n    description = \"List processing tools and functional utilities\";\n    license = licenses.bsd3;\n    maintainers = with maintainers; [ fridh ];\n  };\n}\n\n\nIt takes an argument buildPythonPackage. We now call this function using callPackage in the definition of our environment\n\nwith import <nixpkgs> {};\n\n( let\n    toolz = callPackage /path/to/toolz/release.nix {\n      buildPythonPackage = python310\nPackages.buildPythonPackage;\n    };\n  in python310.withPackages (ps: [\n    ps.numpy\n    toolz\n  ])\n).env\n\n\nImportant to remember is that the Python version for which the package is made depends on the python derivation that is passed to buildPythonPackage. Nix tries to automatically pass arguments when possible, which is why generally you don’t explicitly define which python derivation should be used. In the above example we use buildPythonPackage that is part of the set python3Packages, and in this case the python3 interpreter is automatically used.\n\nFAQ \nHow to solve circular dependencies?\n\nConsider the packages A and B that depend on each other. When packaging B, a solution is to override package A not to depend on B as an input. The same should also be done when packaging A.\n\nHow to override a Python package?\n\nWe can override the interpreter and pass packageOverrides. In the following example we rename the pandas package and build it.\n\nwith import <nixpkgs> {};\n\n(let\n  python = let\n    packageOverrides = self: super: {\n      pandas = super.pandas.overridePythonAttrs(old: {name=\"foo\";});\n    };\n  in pkgs.python310.override {\n    inherit packageOverrides;\n  };\n\nin python.withPackages (ps: [\n  ps.pandas\n])).env\n\n\nUsing nix-build on this expression will build an environment that contains the package pandas but with the new name foo.\n\nAll packages in the package set will use the renamed package. A typical use case is to switch to another version of a certain package. For example, in the Nixpkgs repository we have multiple versions of django and scipy. In the following example we use a different version of scipy and create an environment that uses it. All packages in the Python package set will now use the updated scipy version.\n\nwith import <nixpkgs> {};\n\n( let\n    packageOverrides = self: super: {\n      scipy = super.scipy_0_17;\n    };\n  in (pkgs.python310.override {\n    inherit packageOverrides;\n  }).withPackages (ps: [\n    ps.blaze\n  ])\n).env\n\n\nThe requested package blaze depends on pandas which itself depends on scipy.\n\nIf you want the whole of Nixpkgs to use your modifications, then you can use overlays as explained in this manual. In the following example we build a inkscape using a different version of numpy.\n\nlet\n  pkgs = import <nixpkgs> {};\n  newpkgs = import pkgs.path { overlays = [ (self: super: {\n    python310 = let\n      packageOverrides = python-self: python-super: {\n        numpy = python-super.numpy_1_18;\n      };\n    in super.python310.override {inherit packageOverrides;};\n  } ) ]; };\nin newpkgs.inkscape\n\npython setup.py bdist_wheel cannot create .whl\n\nExecuting python setup.py bdist_wheel in a nix-shellfails with\n\nValueError: ZIP does not support timestamps before 1980\n\n\nThis is because files from the Nix store (which have a timestamp of the UNIX epoch of January 1, 1970) are included in the .ZIP, but .ZIP archives follow the DOS convention of counting timestamps from 1980.\n\nThe command bdist_wheel reads the SOURCE_DATE_EPOCH environment variable, which nix-shell sets to 1. Unsetting this variable or giving it a value corresponding to 1980 or later enables building wheels.\n\nUse 1980 as timestamp:\n\nnix-shell --run \"SOURCE_DATE_EPOCH=315532800 python3 setup.py bdist_wheel\"\n\n\nor the current time:\n\nnix-shell --run \"SOURCE_DATE_EPOCH=$(date +%s) python3 setup.py bdist_wheel\"\n\n\nor unset SOURCE_DATE_EPOCH:\n\nnix-shell --run \"unset SOURCE_DATE_EPOCH; python3 setup.py bdist_wheel\"\n\ninstall_data / data_files problems\n\nIf you get the following error:\n\ncould not create '/nix/store/6l1bvljpy8gazlsw2aw9skwwp4pmvyxw-python-2.7.8/etc':\nPermission denied\n\n\nThis is a known bug in setuptools. Setuptools install_data does not respect --prefix. An example of such package using the feature is pkgs/tools/X11/xpra/default.nix.\n\nAs workaround install it as an extra preInstall step:\n\n${python.pythonOnBuildForHost.interpreter} setup.py install_data --install-dir=$out --root=$out\nsed -i '/ = data\\_files/d' setup.py\n\nRationale of non-existent global site-packages\n\nOn most operating systems a global site-packages is maintained. This however becomes problematic if you want to run multiple Python versions or have multiple versions of certain libraries for your projects. Generally, you would solve such issues by creating virtual environments using virtualenv.\n\nOn Nix each package has an isolated dependency tree which, in the case of Python, guarantees the right versions of the interpreter and libraries or packages are available. There is therefore no need to maintain a global site-packages.\n\nIf you want to create a Python environment for development, then the recommended method is to use nix-shell, either with or without the python.buildEnv function.\n\nHow to consume Python modules using pip in a virtual environment like I am used to on other Operating Systems?\n\nWhile this approach is not very idiomatic from Nix perspective, it can still be useful when dealing with pre-existing projects or in situations where it’s not feasible or desired to write derivations for all required dependencies.\n\nThis is an example of a default.nix for a nix-shell, which allows to consume a virtual environment created by venv, and install Python modules through pip the traditional way.\n\nCreate this default.nix file, together with a requirements.txt and execute nix-shell.\n\nwith import <nixpkgs> { };\n\nlet\n  pythonPackages = python3Packages;\nin pkgs.mkShell rec {\n  name = \"impurePythonEnv\";\n  venvDir = \"./.venv\";\n  buildInputs = [\n    # A Python interpreter including the 'venv' module is required to bootstrap\n    # the environment.\n    pythonPackages.python\n\n    # This executes some shell code to initialize a venv in $venvDir before\n    # dropping into the shell\n    pythonPackages.venvShellHook\n\n    # Those are dependencies that we would like to use from nixpkgs, which will\n    # add them to PYTHONPATH and thus make them accessible from within the venv.\n    pythonPackages.numpy\n    pythonPackages.requests\n\n    # In this particular example, in order to compile any binary extensions they may\n    # require, the Python modules listed in the hypothetical requirements.txt need\n    # the following packages to be installed locally:\n    taglib\n    openssl\n    git\n    libxml2\n    libxslt\n    libzip\n    zlib\n  ];\n\n  # Run this command, only after creating the virtual environment\n  postVenvCreation = ''\n    unset SOURCE_DATE_EPOCH\n    pip install -r requirements.txt\n  '';\n\n  # Now we can execute any commands within the virtual environment.\n  # This is optional and can be left out to run pip manually.\n  postShellHook = ''\n    # allow pip to install wheels\n    unset SOURCE_DATE_EPOCH\n  '';\n\n}\n\n\nIn case the supplied venvShellHook is insufficient, or when Python 2 support is needed, you can define your own shell hook and adapt to your needs like in the following example:\n\nwith import <nixpkgs> { };\n\nlet\n  venvDir = \"./.venv\";\n  pythonPackages = python3Packages;\nin pkgs.mkShell rec {\n  name = \"impurePythonEnv\";\n  buildInputs = [\n    pythonPackages.python\n    # Needed when using python 2.7\n    # pythonPackages.virtualenv\n    # ...\n  ];\n\n  # This is very close to how venvShellHook is implemented, but\n  # adapted to use 'virtualenv'\n  shellHook = ''\n    SOURCE_DATE_EPOCH=$(date +%s)\n\n    if [ -d \"${venvDir}\" ]; then\n      echo \"Skipping venv creation, '${venvDir}' already exists\"\n    else\n      echo \"Creating new venv environment in path: '${venvDir}'\"\n      # Note that the module venv was only introduced in python 3, so for 2.7\n      # this needs to be replaced with a call to virtualenv\n      ${pythonPackages.python.interpreter} -m venv \"${venvDir}\"\n    fi\n\n    # Under some circumstances it might be necessary to add your virtual\n    # environment to PYTHONPATH, which you can do here too;\n    # PYTHONPATH=$PWD/${venvDir}/${pythonPackages.python.sitePackages}/:$PYTHONPATH\n\n    source \"${venvDir}/bin/activate\"\n\n    # As in the previous example, this is optional.\n    pip install -r requirements.txt\n  '';\n}\n\n\nNote that the pip install is an imperative action. So every time nix-shell is executed it will attempt to download the Python modules listed in requirements.txt. However these will be cached locally within the virtualenv folder and not downloaded again.\n\nHow to override a Python package from configuration.nix?\n\nIf you need to change a package’s attribute(s) from configuration.nix you could do:\n\n  nixpkgs.config.packageOverrides = super: {\n    python3 = super.python3.override {\n      packageOverrides = python-self: python-super: {\n        twisted = python-super.twisted.overridePythonAttrs (oldAttrs: {\n          src = super.fetchPypi {\n            pname = \"Twisted\";\n            version = \"19.10.0\";\n            hash = \"sha256-c5S6fycq5yKnTz2Wnc9Zm8TvCTvDkgOHSKSQ8XJKUV0=\";\n            extension = \"tar.bz2\";\n          };\n        });\n      };\n    };\n  };\n\n\npython3Packages.twisted is now globally overridden. All packages and also all NixOS services that reference twisted (such as services.buildbot-worker) now use the new definition. Note that python-super refers to the old package set and python-self to the new, overridden version.\n\nTo modify only a Python package set instead of a whole Python derivation, use this snippet:\n\n  myPythonPackages = python3Packages.override {\n    overrides = self: super: {\n      twisted = ...;\n    };\n  }\n\nHow to override a Python package using overlays?\n\nUse the following overlay template:\n\nself: super: {\n  python = super.python.override {\n    packageOverrides = python-self: python-super: {\n      twisted = python-super.twisted.overrideAttrs (oldAttrs: {\n        src = super.fetchPypi {\n          pname = \"Twisted\";\n          version = \"19.10.0\";\n          hash = \"sha256-c5S6fycq5yKnTz2Wnc9Zm8TvCTvDkgOHSKSQ8XJKUV0=\";\n          extension = \"tar.bz2\";\n        };\n      });\n    };\n  };\n}\n\nHow to override a Python package for all Python versions using extensions?\n\nThe following overlay overrides the call to buildPythonPackage for the foo package for all interpreters by appending a Python extension to the pythonPackagesExtensions list of extensions.\n\nfinal: prev: {\n  pythonPackagesExtensions = prev.pythonPackagesExtensions ++ [\n    (\n      python-final: python-prev: {\n        foo = python-prev.foo.overridePythonAttrs (oldAttrs: {\n          ...\n        });\n      }\n    )\n  ];\n}\n\nHow to use Intel’s MKL with numpy and scipy?\n\nMKL can be configured using an overlay. See the section “Using overlays to configure alternatives”.\n\nWhat inputs do setup_requires, install_requires and tests_require map to?\n\nIn a setup.py or setup.cfg it is common to declare dependencies:\n\nsetup_requires corresponds to nativeBuildInputs\n\ninstall_requires corresponds to propagatedBuildInputs\n\ntests_require corresponds to nativeCheckInputs\n\nHow to enable interpreter optimizations?\n\nThe Python interpreters are by default not built with optimizations enabled, because the builds are in that case not reproducible. To enable optimizations, override the interpreter of interest, e.g using\n\nlet\n  pkgs = import ./. {};\n  mypython = pkgs.python3.override {\n    enableOptimizations = true;\n    reproducibleBuild = false;\n    self = mypython;\n  };\nin mypython\n\nHow to add optional dependencies?\n\nSome packages define optional dependencies for additional features. With setuptools this is called extras_require and flit calls it extras-require, while PEP 621 calls these optional-dependencies. A method for supporting this is by declaring the extras of a package in its passthru, e.g. in case of the package dask\n\npassthru.optional-dependencies = {\n  complete = [ distributed ];\n};\n\n\nand letting the package requiring the extra add the list to its dependencies\n\npropagatedBuildInputs = [\n  ...\n] ++ dask.optional-dependencies.complete;\n\n\nNote this method is preferred over adding parameters to builders, as that can result in packages depending on different variants and thereby causing collisions.\n\nHow to contribute a Python package to nixpkgs?\n\nPackages inside nixpkgs must use the buildPythonPackage or buildPythonApplication function directly, because we can only provide security support for non-vendored dependencies.\n\nWe recommend nix-init for creating new python packages within nixpkgs, as it already prefetches the source, parses dependencies for common formats and prefills most things in meta.\n\nAre Python interpreters built deterministically?\n\nThe Python interpreters are now built deterministically. Minor modifications had to be made to the interpreters in order to generate deterministic bytecode. This has security implications and is relevant for those using Python in a nix-shell.\n\nWhen the environment variable DETERMINISTIC_BUILD is set, all bytecode will have timestamp 1. The buildPythonPackage function sets DETERMINISTIC_BUILD=1 and PYTHONHASHSEED=0. Both are also exported in nix-shell.\n\nHow to provide automatic tests to Python packages?\n\nIt is recommended to test packages as part of the build process. Source distributions (sdist) often include test files, but not always.\n\nBy default the command python setup.py test is run as part of the checkPhase, but often it is necessary to pass a custom checkPhase. An example of such a situation is when py.test is used.\n\nCommon issues\n\nNon-working tests can often be deselected. By default buildPythonPackage runs python setup.py test. which is deprecated. Most Python modules however do follow the standard test protocol where the pytest runner can be used instead. pytest supports the -k and --ignore parameters to ignore test methods or classes as well as whole files. For pytestCheckHook these are conveniently exposed as disabledTests and disabledTestPaths respectively.\n\nbuildPythonPackage {\n  # ...\n  nativeCheckInputs = [\n    pytestCheckHook\n  ];\n\n  disabledTests = [\n    \"function_name\"\n    \"other_function\"\n  ];\n\n  disabledTestPaths = [\n    \"this/file.py\"\n  ];\n}\n\n\nTests that attempt to access $HOME can be fixed by using the following work-around before running tests (e.g. preCheck): export HOME=$(mktemp -d)\n\nContributing \nContributing guidelines\n\nThe following rules are desired to be respected:\n\nPython libraries are called from python-packages.nix and packaged with buildPythonPackage. The expression of a library should be in pkgs/development/python-modules/<name>/default.nix.\n\nPython applications live outside of python-packages.nix and are packaged with buildPythonApplication.\n\nMake sure libraries build for all Python interpreters.\n\nBy default we enable tests. Make sure the tests are found and, in the case of libraries, are passing for all interpreters. If certain tests fail they can be disabled individually. Try to avoid disabling the tests altogether. In any case, when you disable tests, leave a comment explaining why.\n\nCommit names of Python libraries should reflect that they are Python libraries, so write for example python311Packages.numpy: 1.11 -> 1.12. It is highly recommended to specify the current default version to enable automatic build by ofborg.\n\nAttribute names in python-packages.nix as well as pnames should match the library’s name on PyPI, but be normalized according to PEP 0503. This means that characters should be converted to lowercase and . and _ should be replaced by a single - (foo-bar-baz instead of Foo__Bar.baz). If necessary, pname has to be given a different value within fetchPypi.\n\nPackages from sources such as GitHub and GitLab that do not exist on PyPI should not use a name that is already used on PyPI. When possible, they should use the package repository name prefixed with the owner (e.g. organization) name and using a - as delimiter.\n\nAttribute names in python-packages.nix should be sorted alphanumerically to avoid merge conflicts and ease locating attributes.\n\nPackage set maintenance \n\nThe whole Python package set has a lot of packages that do not see regular updates, because they either are a very fragile component in the Python ecosystem, like for example the hypothesis package, or packages that have no maintainer, so maintenance falls back to the package set maintainers.\n\nUpdating packages in bulk\n\nThere is a tool to update alot of python libraries in bulk, it exists at maintainers/scripts/update-python-libraries with this repository.\n\nIt can quickly update minor or major versions for all packages selected and create update commits, and supports the fetchPypi, fetchurl and fetchFromGitHub fetchers. When updating lots of packages that are hosted on GitHub, exporting a GITHUB_API_TOKEN is highly recommended.\n\nUpdating packages in bulk leads to lots of breakages, which is why a stabilization period on the python-unstable branch is required.\n\nIf a package is fragile and often breaks during these bulks updates, it may be reasonable to set passthru.skipBulkUpdate = true in the derivation. This decision should not be made on a whim and should always be supported by a qualifying comment.\n\nOnce the branch is sufficiently stable it should normally be merged into the staging branch.\n\nAn exemplary call to update all python libraries between minor versions would be:\n\n$ maintainers/scripts/update-python-libraries --target minor --commit --use-pkgs-prefix pkgs/development/python-modules/**/default.nix\n\nCPython Update Schedule \n\nWith PEP 602, CPython now follows a yearly release cadence. In nixpkgs, all supported interpreters are made available, but only the most recent two interpreters package sets are built; this is a compromise between being the latest interpreter, and what the majority of the Python packages support.\n\nNew CPython interpreters are released in October. Generally, it takes some time for the majority of active Python projects to support the latest stable interpreter. To help ease the migration for Nixpkgs users between Python interpreters the schedule below will be used:\n\nWhen\tEvent\nAfter YY.11 Release\tBump CPython package set window. The latest and previous latest stable should now be built.\nAfter YY.05 Release\tBump default CPython interpreter to latest stable.\n\nIn practice, this means that the Python community will have had a stable interpreter for ~2 months before attempting to update the package set. And this will allow for ~7 months for Python applications to support the latest interpreter.\n\nQt \nNix expression for a Qt package (default.nix)\nLocating runtime dependencies\n\nWriting Nix expressions for Qt libraries and applications is largely similar as for other C++ software. This section assumes some knowledge of the latter.\n\nThe major caveat with Qt applications is that Qt uses a plugin system to load additional modules at runtime, from a list of well-known locations. In Nixpkgs, we patch QtCore to instead use an environment variable, and wrap Qt applications to set it to the right paths. This effectively makes the runtime dependencies pure and explicit at build-time, at the cost of introducing an extra indirection.\n\nNix expression for a Qt package (default.nix) \n{ stdenv, lib, qtbase, wrapQtAppsHook }:\n\nstdenv.mkDerivation {\n  pname = \"myapp\";\n  version = \"1.0\";\n\n  buildInputs = [ qtbase ];\n  nativeBuildInputs = [ wrapQtAppsHook ];\n}\n\n\nIt is important to import Qt modules directly, that is: qtbase, qtdeclarative, etc. Do not import Qt package sets such as qt5 because the Qt versions of dependencies may not be coherent, causing build and runtime failures.\n\nAdditionally all Qt packages must include wrapQtAppsHook in nativeBuildInputs, or you must explicitly set dontWrapQtApps.\n\nLocating runtime dependencies \n\nQt applications must be wrapped to find runtime dependencies. Include wrapQtAppsHook in nativeBuildInputs:\n\n{ stdenv, wrapQtAppsHook }:\n\nstdenv.mkDerivation {\n  # ...\n  nativeBuildInputs = [ wrapQtAppsHook ];\n}\n\n\nAdd entries to qtWrapperArgs are to modify the wrappers created by wrapQtAppsHook:\n\n{ stdenv, wrapQtAppsHook }:\n\nstdenv.mkDerivation {\n  # ...\n  nativeBuildInputs = [ wrapQtAppsHook ];\n  qtWrapperArgs = [ ''--prefix PATH : /path/to/bin'' ];\n}\n\n\nThe entries are passed as arguments to wrapProgram.\n\nSet dontWrapQtApps to stop applications from being wrapped automatically. Wrap programs manually with wrapQtApp, using the syntax of wrapProgram:\n\n{ stdenv, lib, wrapQtAppsHook }:\n\nstdenv.mkDerivation {\n  # ...\n  nativeBuildInputs = [ wrapQtAppsHook ];\n  dontWrapQtApps = true;\n  preFixup = ''\n      wrapQtApp \"$out/bin/myapp\" --prefix PATH : /path/to/bin\n  '';\n}\n\nNote\n\nwrapQtAppsHook ignores files that are non-ELF executables. This means that scripts won’t be automatically wrapped so you’ll need to manually wrap them as previously mentioned. An example of when you’d always need to do this is with Python applications that use PyQt.\n\nR \nInstallation\nRStudio\nUpdating the package set\nInstallation \n\nDefine an environment for R that contains all the libraries that you’d like to use by adding the following snippet to your $HOME/.config/nixpkgs/config.nix file:\n\n{\n    packageOverrides = super: let self = super.pkgs; in\n    {\n\n        rEnv = super.rWrapper.override {\n            packages = with self.rPackages; [\n                devtools\n                ggplot2\n                reshape2\n                yaml\n                optparse\n                ];\n        };\n    };\n}\n\n\nThen you can use nix-env -f \"<nixpkgs>\" -iA rEnv to install it into your user profile. The set of available libraries can be discovered by running the command nix-env -f \"<nixpkgs>\" -qaP -A rPackages. The first column from that output is the name that has to be passed to rWrapper in the code snipped above.\n\nHowever, if you’d like to add a file to your project source to make the environment available for other contributors, you can create a default.nix file like so:\n\nwith import <nixpkgs> {};\n{\n  myProject = stdenv.mkDerivation {\n    name = \"myProject\";\n    version = \"1\";\n    src = if lib.inNixShell then null else nix;\n\n    buildInputs = with rPackages; [\n      R\n      ggplot2\n      knitr\n    ];\n  };\n}\n\n\nand then run nix-shell . to be dropped into a shell with those packages available.\n\nRStudio \n\nRStudio uses a standard set of packages and ignores any custom R environments or installed packages you may have. To create a custom environment, see rstudioWrapper, which functions similarly to rWrapper:\n\n{\n    packageOverrides = super: let self = super.pkgs; in\n    {\n\n        rstudioEnv = super.rstudioWrapper.override {\n            packages = with self.rPackages; [\n                dplyr\n                ggplot2\n                reshape2\n                ];\n        };\n    };\n}\n\n\nThen like above, nix-env -f \"<nixpkgs>\" -iA rstudioEnv will install this into your user profile.\n\nAlternatively, you can create a self-contained shell.nix without the need to modify any configuration files:\n\n{ pkgs ? import <nixpkgs> {}\n}:\n\npkgs.rstudioWrapper.override {\n  packages = with pkgs.rPackages; [ dplyr ggplot2 reshape2 ];\n}\n\n\n\nExecuting nix-shell will then drop you into an environment equivalent to the one above. If you need additional packages just add them to the list and re-enter the shell.\n\nUpdating the package set \n\nThere is a script and associated environment for regenerating the package sets and synchronising the rPackages tree to the current CRAN and matching BIOC release. These scripts are found in the pkgs/development/r-modules directory and executed as follows:\n\nnix-shell generate-shell.nix\n\nRscript generate-r-packages.R cran  > cran-packages.nix.new\nmv cran-packages.nix.new cran-packages.nix\n\nRscript generate-r-packages.R bioc  > bioc-packages.nix.new\nmv bioc-packages.nix.new bioc-packages.nix\n\nRscript generate-r-packages.R bioc-annotation > bioc-annotation-packages.nix.new\nmv bioc-annotation-packages.nix.new bioc-annotation-packages.nix\n\nRscript generate-r-packages.R bioc-experiment > bioc-experiment-packages.nix.new\nmv bioc-experiment-packages.nix.new bioc-experiment-packages.nix\n\n\ngenerate-r-packages.R <repo> reads <repo>-packages.nix, therefore the renaming.\n\nSome packages require overrides to specify external dependencies or other patches and special requirements. These overrides are specified in the pkgs/development/r-modules/default.nix file. As the *-packages.nix contents are automatically generated it should not be edited and broken builds should be addressed using overrides.\n\nRuby \nUsing Ruby\nDeveloping with Ruby\nUsing Ruby \n\nSeveral versions of Ruby interpreters are available on Nix, as well as over 250 gems and many applications written in Ruby. The attribute ruby refers to the default Ruby interpreter, which is currently MRI 2.6. It’s also possible to refer to specific versions, e.g. ruby_2_y, jruby, or mruby.\n\nIn the Nixpkgs tree, Ruby packages can be found throughout, depending on what they do, and are called from the main package set. Ruby gems, however are separate sets, and there’s one default set for each interpreter (currently MRI only).\n\nThere are two main approaches for using Ruby with gems. One is to use a specifically locked Gemfile for an application that has very strict dependencies. The other is to depend on the common gems, which we’ll explain further down, and rely on them being updated regularly.\n\nThe interpreters have common attributes, namely gems, and withPackages. So you can refer to ruby.gems.nokogiri, or ruby_2_7.gems.nokogiri to get the Nokogiri gem already compiled and ready to use.\n\nSince not all gems have executables like nokogiri, it’s usually more convenient to use the withPackages function like this: ruby.withPackages (p: with p; [ nokogiri ]). This will also make sure that the Ruby in your environment will be able to find the gem and it can be used in your Ruby code (for example via ruby or irb executables) via require \"nokogiri\" as usual.\n\nTemporary Ruby environment with nix-shell\n\nRather than having a single Ruby environment shared by all Ruby development projects on a system, Nix allows you to create separate environments per project. nix-shell gives you the possibility to temporarily load another environment akin to a combined chruby or rvm and bundle exec.\n\nThere are two methods for loading a shell with Ruby packages. The first and recommended method is to create an environment with ruby.withPackages and load that.\n\n$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\"\n\n\nThe other method, which is not recommended, is to create an environment and list all the packages directly.\n\n$ nix-shell -p ruby.gems.nokogiri ruby.gems.pry\n\n\nAgain, it’s possible to launch the interpreter from the shell. The Ruby interpreter has the attribute gems which contains all Ruby gems for that specific interpreter.\n\nLoad Ruby environment from .nix expression\n\nAs explained in the nix-shell section of the Nix manual, nix-shell can also load an expression from a .nix file. Say we want to have Ruby 2.6, nokogori, and pry. Consider a shell.nix file with:\n\nwith import <nixpkgs> {};\nruby.withPackages (ps: with ps; [ nokogiri pry ])\n\n\nWhat’s happening here?\n\nWe begin with importing the Nix Packages collections. import <nixpkgs> imports the <nixpkgs> function, {} calls it and the with statement brings all attributes of nixpkgs in the local scope. These attributes form the main package set.\n\nThen we create a Ruby environment with the withPackages function.\n\nThe withPackages function expects us to provide a function as an argument that takes the set of all ruby gems and returns a list of packages to include in the environment. Here, we select the packages nokogiri and pry from the package set.\n\nExecute command with --run\n\nA convenient flag for nix-shell is --run. It executes a command in the nix-shell. We can e.g. directly open a pry REPL:\n\n$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\" --run \"pry\"\n\n\nOr immediately require nokogiri in pry:\n\n$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\" --run \"pry -rnokogiri\"\n\n\nOr run a script using this environment:\n\n$ nix-shell -p \"ruby.withPackages (ps: with ps; [ nokogiri pry ])\" --run \"ruby example.rb\"\n\nUsing nix-shell as shebang\n\nIn fact, for the last case, there is a more convenient method. You can add a shebang to your script specifying which dependencies nix-shell needs. With the following shebang, you can just execute ./example.rb, and it will run with all dependencies.\n\n#! /usr/bin/env nix-shell\n#! nix-shell -i ruby -p \"ruby.withPackages (ps: with ps; [ nokogiri rest-client ])\"\n\nrequire 'nokogiri'\nrequire 'rest-client'\n\nbody = RestClient.get('http://example.com').body\nputs Nokogiri::HTML(body).at('h1').text\n\nDeveloping with Ruby \nUsing an existing Gemfile\n\nIn most cases, you’ll already have a Gemfile.lock listing all your dependencies. This can be used to generate a gemset.nix which is used to fetch the gems and combine them into a single environment. The reason why you need to have a separate file for this, is that Nix requires you to have a checksum for each input to your build. Since the Gemfile.lock that bundler generates doesn’t provide us with checksums, we have to first download each gem, calculate its SHA256, and store it in this separate file.\n\nSo the steps from having just a Gemfile to a gemset.nix are:\n\n$ bundle lock\n$ bundix\n\n\nIf you already have a Gemfile.lock, you can run bundix and it will work the same.\n\nTo update the gems in your Gemfile.lock, you may use the bundix -l flag, which will create a new Gemfile.lock in case the Gemfile has a more recent time of modification.\n\nOnce the gemset.nix is generated, it can be used in a bundlerEnv derivation. Here is an example you could use for your shell.nix:\n\n# ...\nlet\n  gems = bundlerEnv {\n    name = \"gems-for-some-project\";\n    gemdir = ./.;\n  };\nin mkShell { packages = [ gems gems.wrappedRuby ]; }\n\n\nWith this file in your directory, you can run nix-shell to build and use the gems. The important parts here are bundlerEnv and wrappedRuby.\n\nThe bundlerEnv is a wrapper over all the gems in your gemset. This means that all the /lib and /bin directories will be available, and the executables of all gems (even of indirect dependencies) will end up in your $PATH. The wrappedRuby provides you with all executables that come with Ruby itself, but wrapped so they can easily find the gems in your gemset.\n\nOne common issue that you might have is that you have Ruby 2.6, but also bundler in your gemset. That leads to a conflict for /bin/bundle and /bin/bundler. You can resolve this by wrapping either your Ruby or your gems in a lowPrio call. So in order to give the bundler from your gemset priority, it would be used like this:\n\n# ...\nmkShell { buildInputs = [ gems (lowPrio gems.wrappedRuby) ]; }\n\n\nSometimes a Gemfile references other files. Such as .ruby-version or vendored gems. When copying the Gemfile to the nix store we need to copy those files alongside. This can be done using extraConfigPaths. For example:\n\n  gems = bundlerEnv {\n    name = \"gems-for-some-project\";\n    gemdir = ./.;\n    extraConfigPaths = [ \"${./.}/.ruby-version\" ];\n  };\n\nGem-specific configurations and workarounds\n\nIn some cases, especially if the gem has native extensions, you might need to modify the way the gem is built.\n\nThis is done via a common configuration file that includes all of the workarounds for each gem.\n\nThis file lives at /pkgs/development/ruby-modules/gem-config/default.nix, since it already contains a lot of entries, it should be pretty easy to add the modifications you need for your needs.\n\nIn the meanwhile, or if the modification is for a private gem, you can also add the configuration to only your own environment.\n\nTwo places that allow this modification are the ruby derivation, or bundlerEnv.\n\nHere’s the ruby one:\n\n{ pg_version ? \"10\", pkgs ? import <nixpkgs> { } }:\nlet\n  myRuby = pkgs.ruby.override {\n    defaultGemConfig = pkgs.defaultGemConfig // {\n      pg = attrs: {\n        buildFlags =\n        [ \"--with-pg-config=${pkgs.\"postgresql_${pg_version}\"}/bin/pg_config\" ];\n      };\n    };\n  };\nin myRuby.withPackages (ps: with ps; [ pg ])\n\n\nAnd an example with bundlerEnv:\n\n{ pg_version ? \"10\", pkgs ? import <nixpkgs> { } }:\nlet\n  gems = pkgs.bundlerEnv {\n    name = \"gems-for-some-project\";\n    gemdir = ./.;\n    gemConfig = pkgs.defaultGemConfig // {\n      pg = attrs: {\n        buildFlags =\n        [ \"--with-pg-config=${pkgs.\"postgresql_${pg_version}\"}/bin/pg_config\" ];\n      };\n    };\n  };\nin mkShell { buildInputs = [ gems gems.wrappedRuby ]; }\n\n\nAnd finally via overlays:\n\n{ pg_version ? \"10\" }:\nlet\n  pkgs = import <nixpkgs> {\n    overlays = [\n      (self: super: {\n        defaultGemConfig = super.defaultGemConfig // {\n          pg = attrs: {\n            buildFlags = [\n              \"--with-pg-config=${\n                pkgs.\"postgresql_${pg_version}\"\n              }/bin/pg_config\"\n            ];\n          };\n        };\n      })\n    ];\n  };\nin pkgs.ruby.withPackages (ps: with ps; [ pg ])\n\n\nThen we can get whichever postgresql version we desire and the pg gem will always reference it correctly:\n\n$ nix-shell --argstr pg_version 9_4 --run 'ruby -rpg -e \"puts PG.library_version\"'\n90421\n\n$ nix-shell --run 'ruby -rpg -e \"puts PG.library_version\"'\n100007\n\n\nOf course for this use-case one could also use overlays since the configuration for pg depends on the postgresql alias, but for demonstration purposes this has to suffice.\n\nPlatform-specific gems\n\nRight now, bundix has some issues with pre-built, platform-specific gems: bundix PR #68. Until this is solved, you can tell bundler to not use platform-specific gems and instead build them from source each time:\n\nglobally (will be set in ~/.config/.bundle/config):\n\n$ bundle config set force_ruby_platform true\n\n\nlocally (will be set in <project-root>/.bundle/config):\n\n$ bundle config set --local force_ruby_platform true\n\nAdding a gem to the default gemset\n\nNow that you know how to get a working Ruby environment with Nix, it’s time to go forward and start actually developing with Ruby. We will first have a look at how Ruby gems are packaged on Nix. Then, we will look at how you can use development mode with your code.\n\nAll gems in the standard set are automatically generated from a single Gemfile. The dependency resolution is done with bundler and makes it more likely that all gems are compatible to each other.\n\nIn order to add a new gem to nixpkgs, you can put it into the /pkgs/development/ruby-modules/with-packages/Gemfile and run ./maintainers/scripts/update-ruby-packages.\n\nTo test that it works, you can then try using the gem with:\n\nNIX_PATH=nixpkgs=$PWD nix-shell -p \"ruby.withPackages (ps: with ps; [ name-of-your-gem ])\"\n\nPackaging applications\n\nA common task is to add a ruby executable to nixpkgs, popular examples would be chef, jekyll, or sass. A good way to do that is to use the bundlerApp function, that allows you to make a package that only exposes the listed executables, otherwise the package may cause conflicts through common paths like bin/rake or bin/bundler that aren’t meant to be used.\n\nThe absolute easiest way to do that is to write a Gemfile along these lines:\n\nsource 'https://rubygems.org' do\n  gem 'mdl'\nend\n\n\nIf you want to package a specific version, you can use the standard Gemfile syntax for that, e.g. gem 'mdl', '0.5.0', but if you want the latest stable version anyway, it’s easier to update by running the bundle lock and bundix steps again.\n\nNow you can also make a default.nix that looks like this:\n\n{ bundlerApp }:\n\nbundlerApp {\n  pname = \"mdl\";\n  gemdir = ./.;\n  exes = [ \"mdl\" ];\n}\n\n\nAll that’s left to do is to generate the corresponding Gemfile.lock and gemset.nix as described above in the Using an existing Gemfile section.\n\nPackaging executables that require wrapping\n\nSometimes your app will depend on other executables at runtime, and tries to find it through the PATH environment variable.\n\nIn this case, you can provide a postBuild hook to bundlerApp that wraps the gem in another script that prefixes the PATH.\n\nOf course you could also make a custom gemConfig if you know exactly how to patch it, but it’s usually much easier to maintain with a simple wrapper so the patch doesn’t have to be adjusted for each version.\n\nHere’s another example:\n\n{ lib, bundlerApp, makeWrapper, git, gnutar, gzip }:\n\nbundlerApp {\n  pname = \"r10k\";\n  gemdir = ./.;\n  exes = [ \"r10k\" ];\n\n  nativeBuildInputs = [ makeWrapper ];\n\n  postBuild = ''\n    wrapProgram $out/bin/r10k --prefix PATH : ${lib.makeBinPath [ git gnutar gzip ]}\n  '';\n}\n\nRust \nbuildRustPackage: Compiling Rust applications with Cargo\nbuildRustCrate: Compiling Rust crates using Nix instead of Cargo\nUsing community maintained Rust toolchains\nUsing git bisect on the Rust compiler\n\nTo install the rust compiler and cargo put\n\nenvironment.systemPackages = [\n  rustc\n  cargo\n];\n\n\ninto your configuration.nix or bring them into scope with nix-shell -p rustc cargo.\n\nFor other versions such as daily builds (beta and nightly), use either rustup from nixpkgs (which will manage the rust installation in your home directory), or use community maintained Rust toolchains.\n\nbuildRustPackage: Compiling Rust applications with Cargo \n\nRust applications are packaged by using the buildRustPackage helper from rustPlatform:\n\n{ lib, fetchFromGitHub, rustPlatform }:\n\nrustPlatform.buildRustPackage rec {\n  pname = \"ripgrep\";\n  version = \"12.1.1\";\n\n  src = fetchFromGitHub {\n    owner = \"BurntSushi\";\n    repo = pname;\n    rev = version;\n    hash = \"sha256-+s5RBC3XSgb8omTbUNLywZnP6jSxZBKSS1BmXOjRF8M=\";\n  };\n\n  cargoHash = \"sha256-jtBw4ahSl88L0iuCXxQgZVm1EcboWRJMNtjxLVTtzts=\";\n\n  meta = with lib; {\n    description = \"A fast line-oriented regex search tool, similar to ag and ack\";\n    homepage = \"https://github.com/BurntSushi/ripgrep\";\n    license = licenses.unlicense;\n    maintainers = [];\n  };\n}\n\n\nbuildRustPackage requires either the cargoSha256 or the cargoHash attribute which is computed over all crate sources of this package. cargoHash256 is used for traditional Nix SHA-256 hashes, such as the one in the example above. cargoHash should instead be used for SRI hashes. For example:\n\nException: If the application has cargo git dependencies, the cargoHash/cargoSha256 approach will not work, and you will need to copy the Cargo.lock file of the application to nixpkgs and continue with the next section for specifying the options of thecargoLock section.\n\n  cargoHash = \"sha256-l1vL2ZdtDRxSGvP0X/l3nMw8+6WF67KPutJEzUROjg8=\";\n\n\nBoth types of hashes are permitted when contributing to nixpkgs. The Cargo hash is obtained by inserting a fake checksum into the expression and building the package once. The correct checksum can then be taken from the failed build. A fake hash can be used for cargoSha256 as follows:\n\n  cargoSha256 = lib.fakeSha256;\n\n\nFor cargoHash you can use:\n\n  cargoHash = lib.fakeHash;\n\n\nPer the instructions in the Cargo Book best practices guide, Rust applications should always commit the Cargo.lock file in git to ensure a reproducible build. However, a few packages do not, and Nix depends on this file, so if it is missing you can use cargoPatches to apply it in the patchPhase. Consider sending a PR upstream with a note to the maintainer describing why it’s important to include in the application.\n\nThe fetcher will verify that the Cargo.lock file is in sync with the src attribute, and fail the build if not. It will also will compress the vendor directory into a tar.gz archive.\n\nThe tarball with vendored dependencies contains a directory with the package’s name, which is normally composed of pname and version. This means that the vendored dependencies hash (cargoSha256/cargoHash) is dependent on the package name and version. The cargoDepsName attribute can be used to use another name for the directory of vendored dependencies. For example, the hash can be made invariant to the version by setting cargoDepsName to pname:\n\nrustPlatform.buildRustPackage rec {\n  pname = \"broot\";\n  version = \"1.2.0\";\n\n  src = fetchCrate {\n    inherit pname version;\n    hash = \"sha256-aDQA4A5mScX9or3Lyiv/5GyAehidnpKKE0grhbP1Ctc=\";\n  };\n\n  cargoHash = \"sha256-tbrTbutUs5aPSV+yE0IBUZAAytgmZV7Eqxia7g+9zRs=\";\n  cargoDepsName = pname;\n\n  # ...\n}\n\nImporting a Cargo.lock file\n\nUsing cargoSha256 or cargoHash is tedious when using buildRustPackage within a project, since it requires that the hash is updated after every change to Cargo.lock. Therefore, buildRustPackage also supports vendoring dependencies directly from a Cargo.lock file using the cargoLock argument. For example:\n\nrustPlatform.buildRustPackage {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  cargoLock = {\n    lockFile = ./Cargo.lock;\n  };\n\n  # ...\n}\n\n\nThis will retrieve the dependencies using fixed-output derivations from the specified lockfile.\n\nOne caveat is that Cargo.lock cannot be patched in the patchPhase because it runs after the dependencies have already been fetched. If you need to patch or generate the lockfile you can alternatively set cargoLock.lockFileContents to a string of its contents:\n\nrustPlatform.buildRustPackage {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  cargoLock = let\n    fixupLockFile = path: f (builtins.readFile path);\n  in {\n    lockFileContents = fixupLockFile ./Cargo.lock;\n  };\n\n  # ...\n}\n\n\nNote that setting cargoLock.lockFile or cargoLock.lockFileContents doesn’t add a Cargo.lock to your src, and a Cargo.lock is still required to build a rust package. A simple fix is to use:\n\npostPatch = ''\n  ln -s ${./Cargo.lock} Cargo.lock\n'';\n\n\nThe output hash of each dependency that uses a git source must be specified in the outputHashes attribute. For example:\n\nrustPlatform.buildRustPackage rec {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  cargoLock = {\n    lockFile = ./Cargo.lock;\n    outputHashes = {\n      \"finalfusion-0.14.0\" = \"17f4bsdzpcshwh74w5z119xjy2if6l2wgyjy56v621skr2r8y904\";\n    };\n  };\n\n  # ...\n}\n\n\nIf you do not specify an output hash for a git dependency, building the package will fail and inform you of which crate needs to be added. To find the correct hash, you can first use lib.fakeSha256 or lib.fakeHash as a stub hash. Building the package (and thus the vendored dependencies) will then inform you of the correct hash.\n\nFor usage outside nixpkgs, allowBuiltinFetchGit could be used to avoid having to specify outputHashes. For example:\n\nrustPlatform.buildRustPackage rec {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  cargoLock = {\n    lockFile = ./Cargo.lock;\n    allowBuiltinFetchGit = true;\n  };\n\n  # ...\n}\n\nCargo features\n\nYou can disable default features using buildNoDefaultFeatures, and extra features can be added with buildFeatures.\n\nIf you want to use different features for check phase, you can use checkNoDefaultFeatures and checkFeatures. They are only passed to cargo test and not cargo build. If left unset, they default to buildNoDefaultFeatures and buildFeatures.\n\nFor example:\n\nrustPlatform.buildRustPackage rec {\n  pname = \"myproject\";\n  version = \"1.0.0\";\n\n  buildNoDefaultFeatures = true;\n  buildFeatures = [ \"color\" \"net\" ];\n\n  # disable network features in tests\n  checkFeatures = [ \"color\" ];\n\n  # ...\n}\n\nCross compilation\n\nBy default, Rust packages are compiled for the host platform, just like any other package is. The --target passed to rust tools is computed from this. By default, it takes the stdenv.hostPlatform.config and replaces components where they are known to differ. But there are ways to customize the argument:\n\nTo choose a different target by name, define stdenv.hostPlatform.rustc.config as that name (a string), and that name will be used instead.\n\nFor example:\n\nimport <nixpkgs> {\n  crossSystem = (import <nixpkgs/lib>).systems.examples.armhf-embedded // {\n    rustc.config = \"thumbv7em-none-eabi\";\n  };\n}\n\n\nwill result in:\n\n--target thumbv7em-none-eabi\n\n\nTo pass a completely custom target, define stdenv.hostPlatform.rustc.config with its name, and stdenv.hostPlatform.rustc.platform with the value. The value will be serialized to JSON in a file called ${stdenv.hostPlatform.rustc.config}.json, and the path of that file will be used instead.\n\nFor example:\n\nimport <nixpkgs> {\n  crossSystem = (import <nixpkgs/lib>).systems.examples.armhf-embedded // {\n    rustc.config = \"thumb-crazy\";\n    rustc.platform = { foo = \"\"; bar = \"\"; };\n  };\n}\n\n\nwill result in:\n\n--target /nix/store/asdfasdfsadf-thumb-crazy.json # contains {\"foo\":\"\",\"bar\":\"\"}\n\n\nNote that currently custom targets aren’t compiled with std, so cargo test will fail. This can be ignored by adding doCheck = false; to your derivation.\n\nRunning package tests\n\nWhen using buildRustPackage, the checkPhase is enabled by default and runs cargo test on the package to build. To make sure that we don’t compile the sources twice and to actually test the artifacts that will be used at runtime, the tests will be ran in the release mode by default.\n\nHowever, in some cases the test-suite of a package doesn’t work properly in the release mode. For these situations, the mode for checkPhase can be changed like so:\n\nrustPlatform.buildRustPackage {\n  /* ... */\n  checkType = \"debug\";\n}\n\n\nPlease note that the code will be compiled twice here: once in release mode for the buildPhase, and again in debug mode for the checkPhase.\n\nTest flags, e.g., --package foo, can be passed to cargo test via the cargoTestFlags attribute.\n\nAnother attribute, called checkFlags, is used to pass arguments to the test binary itself, as stated here.\n\nTests relying on the structure of the target/ directory\n\nSome tests may rely on the structure of the target/ directory. Those tests are likely to fail because we use cargo --target during the build. This means that the artifacts are stored in target/<architecture>/release/, rather than in target/release/.\n\nThis can only be worked around by patching the affected tests accordingly.\n\nDisabling package-tests\n\nIn some instances, it may be necessary to disable testing altogether (with doCheck = false;):\n\nIf no tests exist – the checkPhase should be explicitly disabled to skip unnecessary build steps to speed up the build.\n\nIf tests are highly impure (e.g. due to network usage).\n\nThere will obviously be some corner-cases not listed above where it’s sensible to disable tests. The above are just guidelines, and exceptions may be granted on a case-by-case basis.\n\nHowever, please check if it’s possible to disable a problematic subset of the test suite and leave a comment explaining your reasoning.\n\nThis can be achieved with --skip in checkFlags:\n\nrustPlatform.buildRustPackage {\n  /* ... */\n  checkFlags = [\n    # reason for disabling test\n    \"--skip=example::tests:example_test\"\n  ];\n}\n\nUsing cargo-nextest\n\nTests can be run with cargo-nextest by setting useNextest = true. The same options still apply, but nextest accepts a different set of arguments and the settings might need to be adapted to be compatible with cargo-nextest.\n\nrustPlatform.buildRustPackage {\n  /* ... */\n  useNextest = true;\n}\n\nSetting test-threads\n\nbuildRustPackage will use parallel test threads by default, sometimes it may be necessary to disable this so the tests run consecutively.\n\nrustPlatform.buildRustPackage {\n  /* ... */\n  dontUseCargoParallelTests = true;\n}\n\nBuilding a package in debug mode\n\nBy default, buildRustPackage will use release mode for builds. If a package should be built in debug mode, it can be configured like so:\n\nrustPlatform.buildRustPackage {\n  /* ... */\n  buildType = \"debug\";\n}\n\n\nIn this scenario, the checkPhase will be ran in debug mode as well.\n\nCustom build/install-procedures\n\nSome packages may use custom scripts for building/installing, e.g. with a Makefile. In these cases, it’s recommended to override the buildPhase/installPhase/checkPhase.\n\nOtherwise, some steps may fail because of the modified directory structure of target/.\n\nBuilding a crate with an absent or out-of-date Cargo.lock file\n\nbuildRustPackage needs a Cargo.lock file to get all dependencies in the source code in a reproducible way. If it is missing or out-of-date one can use the cargoPatches attribute to update or add it.\n\nrustPlatform.buildRustPackage rec {\n  (...)\n  cargoPatches = [\n    # a patch file to add/update Cargo.lock in the source code\n    ./add-Cargo.lock.patch\n  ];\n}\n\nCompiling non-Rust packages that include Rust code\n\nSeveral non-Rust packages incorporate Rust code for performance- or security-sensitive parts. rustPlatform exposes several functions and hooks that can be used to integrate Cargo in non-Rust packages.\n\nVendoring of dependencies\n\nSince network access is not allowed in sandboxed builds, Rust crate dependencies need to be retrieved using a fetcher. rustPlatform provides the fetchCargoTarball fetcher, which vendors all dependencies of a crate. For example, given a source path src containing Cargo.toml and Cargo.lock, fetchCargoTarball can be used as follows:\n\ncargoDeps = rustPlatform.fetchCargoTarball {\n  inherit src;\n  hash = \"sha256-BoHIN/519Top1NUBjpB/oEMqi86Omt3zTQcXFWqrek0=\";\n};\n\n\nThe src attribute is required, as well as a hash specified through one of the hash attribute. The following optional attributes can also be used:\n\nname: the name that is used for the dependencies tarball. If name is not specified, then the name cargo-deps will be used.\n\nsourceRoot: when the Cargo.lock/Cargo.toml are in a subdirectory, sourceRoot specifies the relative path to these files.\n\npatches: patches to apply before vendoring. This is useful when the Cargo.lock/Cargo.toml files need to be patched before vendoring.\n\nIf a Cargo.lock file is available, you can alternatively use the importCargoLock function. In contrast to fetchCargoTarball, this function does not require a hash (unless git dependencies are used) and fetches every dependency as a separate fixed-output derivation. importCargoLock can be used as follows:\n\ncargoDeps = rustPlatform.importCargoLock {\n  lockFile = ./Cargo.lock;\n};\n\n\nIf the Cargo.lock file includes git dependencies, then their output hashes need to be specified since they are not available through the lock file. For example:\n\ncargoDeps = rustPlatform.importCargoLock {\n  lockFile = ./Cargo.lock;\n  outputHashes = {\n    \"rand-0.8.3\" = \"0ya2hia3cn31qa8894s3av2s8j5bjwb6yq92k0jsnlx7jid0jwqa\";\n  };\n};\n\n\nIf you do not specify an output hash for a git dependency, building cargoDeps will fail and inform you of which crate needs to be added. To find the correct hash, you can first use lib.fakeSha256 or lib.fakeHash as a stub hash. Building cargoDeps will then inform you of the correct hash.\n\nHooks\n\nrustPlatform provides the following hooks to automate Cargo builds:\n\ncargoSetupHook: configure Cargo to use dependencies vendored through fetchCargoTarball. This hook uses the cargoDeps environment variable to find the vendored dependencies. If a project already vendors its dependencies, the variable cargoVendorDir can be used instead. When the Cargo.toml/Cargo.lock files are not in sourceRoot, then the optional cargoRoot is used to specify the Cargo root directory relative to sourceRoot.\n\ncargoBuildHook: use Cargo to build a crate. If the crate to be built is a crate in e.g. a Cargo workspace, the relative path to the crate to build can be set through the optional buildAndTestSubdir environment variable. Features can be specified with cargoBuildNoDefaultFeatures and cargoBuildFeatures. Additional Cargo build flags can be passed through cargoBuildFlags.\n\nmaturinBuildHook: use Maturin to build a Python wheel. Similar to cargoBuildHook, the optional variable buildAndTestSubdir can be used to build a crate in a Cargo workspace. Additional Maturin flags can be passed through maturinBuildFlags.\n\ncargoCheckHook: run tests using Cargo. The build type for checks can be set using cargoCheckType. Features can be specified with cargoCheckNoDefaultFeatures and cargoCheckFeatures. Additional flags can be passed to the tests using checkFlags and checkFlagsArray. By default, tests are run in parallel. This can be disabled by setting dontUseCargoParallelTests.\n\ncargoNextestHook: run tests using cargo-nextest. The same options for cargoCheckHook also applies to cargoNextestHook.\n\ncargoInstallHook: install binaries and static/shared libraries that were built using cargoBuildHook.\n\nbindgenHook: for crates which use bindgen as a build dependency, lets bindgen find libclang and libclang find the libraries in buildInputs.\n\nExamples\nPython package using setuptools-rust\n\nFor Python packages using setuptools-rust, you can use fetchCargoTarball and cargoSetupHook to retrieve and set up Cargo dependencies. The build itself is then performed by buildPythonPackage.\n\nThe following example outlines how the tokenizers Python package is built. Since the Python package is in the source/bindings/python directory of the tokenizers project’s source archive, we use sourceRoot to point the tooling to this directory:\n\n{ fetchFromGitHub\n, buildPythonPackage\n, cargo\n, rustPlatform\n, rustc\n, setuptools-rust\n}:\n\nbuildPythonPackage rec {\n  pname = \"tokenizers\";\n  version = \"0.10.0\";\n\n  src = fetchFromGitHub {\n    owner = \"huggingface\";\n    repo = pname;\n    rev = \"python-v${version}\";\n    hash = \"sha256-rQ2hRV52naEf6PvRsWVCTN7B1oXAQGmnpJw4iIdhamw=\";\n  };\n\n  cargoDeps = rustPlatform.fetchCargoTarball {\n    inherit src sourceRoot;\n    name = \"${pname}-${version}\";\n    hash = \"sha256-miW//pnOmww2i6SOGbkrAIdc/JMDT4FJLqdMFojZeoY=\";\n  };\n\n  sourceRoot = \"${src.name}/bindings/python\";\n\n  nativeBuildInputs = [\n    cargo\n    rustPlatform.cargoSetupHook\n    rustc\n    setuptools-rust\n  ];\n\n  # ...\n}\n\n\nIn some projects, the Rust crate is not in the main Python source directory. In such cases, the cargoRoot attribute can be used to specify the crate’s directory relative to sourceRoot. In the following example, the crate is in src/rust, as specified in the cargoRoot attribute. Note that we also need to specify the correct path for fetchCargoTarball.\n\n\n{ buildPythonPackage\n, fetchPypi\n, rustPlatform\n, setuptools-rust\n, openssl\n}:\n\nbuildPythonPackage rec {\n  pname = \"cryptography\";\n  version = \"3.4.2\"; # Also update the hash in vectors.nix\n\n  src = fetchPypi {\n    inherit pname version;\n    hash = \"sha256-xGDilsjLOnls3MfVbGKnj80KCUCczZxlis5PmHzpNcQ=\";\n  };\n\n  cargoDeps = rustPlatform.fetchCargoTarball {\n    inherit src;\n    sourceRoot = \"${pname}-${version}/${cargoRoot}\";\n    name = \"${pname}-${version}\";\n    hash = \"sha256-PS562W4L1NimqDV2H0jl5vYhL08H9est/pbIxSdYVfo=\";\n  };\n\n  cargoRoot = \"src/rust\";\n\n  # ...\n}\n\nPython package using maturin\n\nPython packages that use Maturin can be built with fetchCargoTarball, cargoSetupHook, and maturinBuildHook. For example, the following (partial) derivation builds the retworkx Python package. fetchCargoTarball and cargoSetupHook are used to fetch and set up the crate dependencies. maturinBuildHook is used to perform the build.\n\n{ lib\n, buildPythonPackage\n, rustPlatform\n, fetchFromGitHub\n}:\n\nbuildPythonPackage rec {\n  pname = \"retworkx\";\n  version = \"0.6.0\";\n\n  src = fetchFromGitHub {\n    owner = \"Qiskit\";\n    repo = \"retworkx\";\n    rev = version;\n    hash = \"sha256-11n30ldg3y3y6qxg3hbj837pnbwjkqw3nxq6frds647mmmprrd20=\";\n  };\n\n  cargoDeps = rustPlatform.fetchCargoTarball {\n    inherit src;\n    name = \"${pname}-${version}\";\n    hash = \"sha256-heOBK8qi2nuc/Ib+I/vLzZ1fUUD/G/KTw9d7M4Hz5O0=\";\n  };\n\n  format = \"pyproject\";\n\n  nativeBuildInputs = with rustPlatform; [ cargoSetupHook maturinBuildHook ];\n\n  # ...\n}\n\nbuildRustCrate: Compiling Rust crates using Nix instead of Cargo \nSimple operation\n\nWhen run, cargo build produces a file called Cargo.lock, containing pinned versions of all dependencies. Nixpkgs contains a tool called crate2Nix (nix-shell -p crate2nix), which can be used to turn a Cargo.lock into a Nix expression. That Nix expression calls rustc directly (hence bypassing Cargo), and can be used to compile a crate and all its dependencies.\n\nSee crate2nix’s documentation for instructions on how to use it.\n\nHandling external dependencies\n\nSome crates require external libraries. For crates from crates.io, such libraries can be specified in defaultCrateOverrides package in nixpkgs itself.\n\nStarting from that file, one can add more overrides, to add features or build inputs by overriding the hello crate in a separate file.\n\nwith import <nixpkgs> {};\n((import ./hello.nix).hello {}).override {\n  crateOverrides = defaultCrateOverrides // {\n    hello = attrs: { buildInputs = [ openssl ]; };\n  };\n}\n\n\nHere, crateOverrides is expected to be a attribute set, where the key is the crate name without version number and the value a function. The function gets all attributes passed to buildRustCrate as first argument and returns a set that contains all attribute that should be overwritten.\n\nFor more complicated cases, such as when parts of the crate’s derivation depend on the crate’s version, the attrs argument of the override above can be read, as in the following example, which patches the derivation:\n\nwith import <nixpkgs> {};\n((import ./hello.nix).hello {}).override {\n  crateOverrides = defaultCrateOverrides // {\n    hello = attrs: lib.optionalAttrs (lib.versionAtLeast attrs.version \"1.0\")  {\n      postPatch = ''\n        substituteInPlace lib/zoneinfo.rs \\\n          --replace \"/usr/share/zoneinfo\" \"${tzdata}/share/zoneinfo\"\n      '';\n    };\n  };\n}\n\n\nAnother situation is when we want to override a nested dependency. This actually works in the exact same way, since the crateOverrides parameter is forwarded to the crate’s dependencies. For instance, to override the build inputs for crate libc in the example above, where libc is a dependency of the main crate, we could do:\n\nwith import <nixpkgs> {};\n((import hello.nix).hello {}).override {\n  crateOverrides = defaultCrateOverrides // {\n    libc = attrs: { buildInputs = []; };\n  };\n}\n\nOptions and phases configuration\n\nActually, the overrides introduced in the previous section are more general. A number of other parameters can be overridden:\n\nThe version of rustc used to compile the crate:\n\n(hello {}).override { rust = pkgs.rust; };\n\n\nWhether to build in release mode or debug mode (release mode by default):\n\n(hello {}).override { release = false; };\n\n\nWhether to print the commands sent to rustc when building (equivalent to --verbose in cargo:\n\n(hello {}).override { verbose = false; };\n\n\nExtra arguments to be passed to rustc:\n\n(hello {}).override { extraRustcOpts = \"-Z debuginfo=2\"; };\n\n\nPhases, just like in any other derivation, can be specified using the following attributes: preUnpack, postUnpack, prePatch, patches, postPatch, preConfigure (in the case of a Rust crate, this is run before calling the “build” script), postConfigure (after the “build” script),preBuild, postBuild, preInstall and postInstall. As an example, here is how to create a new module before running the build script:\n\n(hello {}).override {\n  preConfigure = ''\n     echo \"pub const PATH=\\\"${hi.out}\\\";\" >> src/path.rs\"\n  '';\n};\n\nSetting Up nix-shell\n\nOftentimes you want to develop code from within nix-shell. Unfortunately buildRustCrate does not support common nix-shell operations directly (see this issue) so we will use stdenv.mkDerivation instead.\n\nUsing the example hello project above, we want to do the following:\n\nHave access to cargo and rustc\n\nHave the openssl library available to a crate through it’s normal compilation mechanism (pkg-config).\n\nA typical shell.nix might look like:\n\nwith import <nixpkgs> {};\n\nstdenv.mkDerivation {\n  name = \"rust-env\";\n  nativeBuildInputs = [\n    rustc cargo\n\n    # Example Build-time Additional Dependencies\n    pkg-config\n  ];\n  buildInputs = [\n    # Example Run-time Additional Dependencies\n    openssl\n  ];\n\n  # Set Environment Variables\n  RUST_BACKTRACE = 1;\n}\n\n\nYou should now be able to run the following:\n\n$ nix-shell --pure\n$ cargo build\n$ cargo test\n\nUsing community maintained Rust toolchains \nNote\n\nThe following projects cannot be used within Nixpkgs since Import From Derivation (IFD) is disallowed in Nixpkgs. To package things that require Rust nightly, RUSTC_BOOTSTRAP = true; can sometimes be used as a hack.\n\nThere are two community maintained approaches to Rust toolchain management:\n\noxalica’s Rust overlay\n\nfenix\n\nDespite their names, both projects provides a similar set of packages and overlays under different APIs.\n\nOxalica’s overlay allows you to select a particular Rust version without you providing a hash or a flake input, but comes with a larger git repository than fenix.\n\nFenix also provides rust-analyzer nightly in addition to the Rust toolchains.\n\nBoth oxalica’s overlay and fenix better integrate with nix and cache optimizations. Because of this and ergonomics, either of those community projects should be preferred to the Mozilla’s Rust overlay (nixpkgs-mozilla).\n\nThe following documentation demonstrates examples using fenix and oxalica’s Rust overlay with nix-shell and building derivations. More advanced usages like flake usage are documented in their own repositories.\n\nUsing Rust nightly with nix-shell\n\nHere is a simple shell.nix that provides Rust nightly (default profile) using fenix:\n\nwith import <nixpkgs> { };\nlet\n  fenix = callPackage\n    (fetchFromGitHub {\n      owner = \"nix-community\";\n      repo = \"fenix\";\n      # commit from: 2023-03-03\n      rev = \"e2ea04982b892263c4d939f1cc3bf60a9c4deaa1\";\n      hash = \"sha256-AsOim1A8KKtMWIxG+lXh5Q4P2bhOZjoUhFWJ1EuZNNk=\";\n    })\n    { };\nin\nmkShell {\n  name = \"rust-env\";\n  nativeBuildInputs = [\n    # Note: to use stable, just replace `default` with `stable`\n    fenix.default.toolchain\n\n    # Example Build-time Additional Dependencies\n    pkg-config\n  ];\n  buildInputs = [\n    # Example Run-time Additional Dependencies\n    openssl\n  ];\n\n  # Set Environment Variables\n  RUST_BACKTRACE = 1;\n}\n\n\nSave this to shell.nix, then run:\n\n$ rustc --version\nrustc 1.69.0-nightly (13471d3b2 2023-03-02)\n\n\nTo see that you are using nightly.\n\nOxalica’s Rust overlay has more complete examples of shell.nix (and cross compilation) under its examples directory.\n\nUsing Rust nightly in a derivation with buildRustPackage\n\nYou can also use Rust nightly to build rust packages using makeRustPlatform. The below snippet demonstrates invoking buildRustPackage with a Rust toolchain from oxalica’s overlay:\n\nwith import <nixpkgs>\n{\n  overlays = [\n    (import (fetchTarball \"https://github.com/oxalica/rust-overlay/archive/master.tar.gz\"))\n  ];\n};\nlet\n  rustPlatform = makeRustPlatform {\n    cargo = rust-bin.stable.latest.minimal;\n    rustc = rust-bin.stable.latest.minimal;\n  };\nin\n\nrustPlatform.buildRustPackage rec {\n  pname = \"ripgrep\";\n  version = \"12.1.1\";\n\n  src = fetchFromGitHub {\n    owner = \"BurntSushi\";\n    repo = \"ripgrep\";\n    rev = version;\n    hash = \"sha256-+s5RBC3XSgb8omTbUNLywZnP6jSxZBKSS1BmXOjRF8M=\";\n  };\n\n  cargoHash = \"sha256-l1vL2ZdtDRxSGvP0X/l3nMw8+6WF67KPutJEzUROjg8=\";\n\n  doCheck = false;\n\n  meta = with lib; {\n    description = \"A fast line-oriented regex search tool, similar to ag and ack\";\n    homepage = \"https://github.com/BurntSushi/ripgrep\";\n    license = with licenses; [ mit unlicense ];\n    maintainers = with maintainers; [];\n  };\n}\n\n\nFollow the below steps to try that snippet.\n\nsave the above snippet as default.nix in that directory\n\ncd into that directory and run nix-build\n\nFenix also has examples with buildRustPackage, crane, naersk, and cross compilation in its Examples section.\n\nUsing git bisect on the Rust compiler \n\nSometimes an upgrade of the Rust compiler (rustc) will break a downstream package. In these situations, being able to git bisect the rustc version history to find the offending commit is quite useful. Nixpkgs makes it easy to do this.\n\nFirst, roll back your nixpkgs to a commit in which its rustc used the most recent one which doesn’t have the problem. You’ll need to do this because of rustc’s extremely aggressive version-pinning.\n\nNext, add the following overlay, updating the Rust version to the one in your rolled-back nixpkgs, and replacing /git/scratch/rust with the path into which you have git cloned the rustc git repository:\n\n (final: prev: /*lib.optionalAttrs prev.stdenv.targetPlatform.isAarch64*/ {\n   rust_1_72 =\n     lib.updateManyAttrsByPath [{\n       path = [ \"packages\" \"stable\" ];\n       update = old: old.overrideScope(final: prev: {\n         rustc = prev.rustc.overrideAttrs (_: {\n           src = lib.cleanSource /git/scratch/rust;\n           # do *not* put passthru.isReleaseTarball=true here\n         });\n       });\n     }]\n       prev.rust_1_72;\n })\n\n\nIf the problem you’re troubleshooting only manifests when cross-compiling you can uncomment the lib.optionalAttrs in the example above, and replace isAarch64 with the target that is having problems. This will speed up your bisect quite a bit, since the host compiler won’t need to be rebuilt.\n\nNow, you can start a git bisect in the directory where you checked out the rustc source code. It is recommended to select the endpoint commits by searching backwards from origin/master for the commits which added the release notes for the versions in question. If you set the endpoints to commits on the release branches (i.e. the release tags), git-bisect will often get confused by the complex merge-commit structures it will need to traverse.\n\nThe command loop you’ll want to use for bisecting looks like this:\n\ngit bisect {good,bad}  # depending on result of last build\ngit submodule update --init\nCARGO_NET_OFFLINE=false cargo vendor \\\n  --sync ./src/tools/cargo/Cargo.toml \\\n  --sync ./src/tools/rust-analyzer/Cargo.toml \\\n  --sync ./compiler/rustc_codegen_cranelift/Cargo.toml \\\n  --sync ./src/bootstrap/Cargo.toml\nnix-build $NIXPKGS -A package-broken-by-rust-changes\n\n\nThe git submodule update --init and cargo vendor commands above require network access, so they can’t be performed from within the rustc derivation, unfortunately.\n\nSwift \nModule search paths\nCore libraries\nPackaging with SwiftPM\nConsiderations for custom build tools\n\nThe Swift compiler is provided by the swift package:\n\n# Compile and link a simple executable.\nnix-shell -p swift --run 'swiftc -' <<< 'print(\"Hello world!\")'\n# Run it!\n./main\n\n\nThe swift package also provides the swift command, with some caveats:\n\nSwift Package Manager (SwiftPM) is packaged separately as swiftpm. If you need functionality like swift build, swift run, swift test, you must also add the swiftpm package to your closure.\n\nOn Darwin, the swift repl command requires an Xcode installation. This is because it uses the system LLDB debugserver, which has special entitlements.\n\nModule search paths \n\nLike other toolchains in Nixpkgs, the Swift compiler executables are wrapped to help Swift find your application’s dependencies in the Nix store. These wrappers scan the buildInputs of your package derivation for specific directories where Swift modules are placed by convention, and automatically add those directories to the Swift compiler search paths.\n\nSwift follows different conventions depending on the platform. The wrappers look for the following directories:\n\nOn Darwin platforms: lib/swift/macosx (If not targeting macOS, replace macosx with the Xcode platform name.)\n\nOn other platforms: lib/swift/linux/x86_64 (Where linux and x86_64 are from lowercase uname -sm.)\n\nFor convenience, Nixpkgs also adds lib/swift to the search path. This can save a bit of work packaging Swift modules, because many Nix builds will produce output for just one target any way.\n\nCore libraries \n\nIn addition to the standard library, the Swift toolchain contains some additional ‘core libraries’ that, on Apple platforms, are normally distributed as part of the OS or Xcode. These are packaged separately in Nixpkgs, and can be found (for use in buildInputs) as:\n\nswiftPackages.Dispatch\n\nswiftPackages.Foundation\n\nswiftPackages.XCTest\n\nPackaging with SwiftPM \n\nNixpkgs includes a small helper swiftpm2nix that can fetch your SwiftPM dependencies for you, when you need to write a Nix expression to package your application.\n\nThe first step is to run the generator:\n\ncd /path/to/my/project\n# Enter a Nix shell with the required tools.\nnix-shell -p swift swiftpm swiftpm2nix\n# First, make sure the workspace is up-to-date.\nswift package resolve\n# Now generate the Nix code.\nswiftpm2nix\n\n\nThis produces some files in a directory nix, which will be part of your Nix expression. The next step is to write that expression:\n\n{ stdenv, swift, swiftpm, swiftpm2nix, fetchFromGitHub }:\n\nlet\n  # Pass the generated files to the helper.\n  generated = swiftpm2nix.helpers ./nix;\nin\n\nstdenv.mkDerivation rec {\n  pname = \"myproject\";\n  version = \"0.0.0\";\n\n  src = fetchFromGitHub {\n    owner = \"nixos\";\n    repo = pname;\n    rev = version;\n    hash = \"sha256-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=\";\n  };\n\n  # Including SwiftPM as a nativeBuildInput provides a buildPhase for you.\n  # This by default performs a release build using SwiftPM, essentially:\n  #   swift build -c release\n  nativeBuildInputs = [ swift swiftpm ];\n\n  # The helper provides a configure snippet that will prepare all dependencies\n  # in the correct place, where SwiftPM expects them.\n  configurePhase = generated.configure;\n\n  installPhase = ''\n    # This is a special function that invokes swiftpm to find the location\n    # of the binaries it produced.\n    binPath=\"$(swiftpmBinPath)\"\n    # Now perform any installation steps.\n    mkdir -p $out/bin\n    cp $binPath/myproject $out/bin/\n  '';\n}\n\nCustom build flags\n\nIf you’d like to build a different configuration than release:\n\nswiftpmBuildConfig = \"debug\";\n\n\nIt is also possible to provide additional flags to swift build:\n\nswiftpmFlags = [ \"--disable-dead-strip\" ];\n\n\nThe default buildPhase already passes -j for parallel building.\n\nIf these two customization options are insufficient, provide your own buildPhase that invokes swift build.\n\nRunning tests\n\nIncluding swiftpm in your nativeBuildInputs also provides a default checkPhase, but it must be enabled with:\n\ndoCheck = true;\n\n\nThis essentially runs: swift test -c release\n\nPatching dependencies\n\nIn some cases, it may be necessary to patch a SwiftPM dependency. SwiftPM dependencies are located in .build/checkouts, but the swiftpm2nix helper provides these as symlinks to read-only /nix/store paths. In order to patch them, we need to make them writable.\n\nA special function swiftpmMakeMutable is available to replace the symlink with a writable copy:\n\nconfigurePhase = generated.configure ++ ''\n  # Replace the dependency symlink with a writable copy.\n  swiftpmMakeMutable swift-crypto\n  # Now apply a patch.\n  patch -p1 -d .build/checkouts/swift-crypto -i ${./some-fix.patch}\n'';\n\nConsiderations for custom build tools \nLinking the standard library\n\nThe swift package has a separate lib output containing just the Swift standard library, to prevent Swift applications needing a dependency on the full Swift compiler at run-time. Linking with the Nixpkgs Swift toolchain already ensures binaries correctly reference the lib output.\n\nSometimes, Swift is used only to compile part of a mixed codebase, and the link step is manual. Custom build tools often locate the standard library relative to the swift compiler executable, and while the result will work, when this path ends up in the binary, it will have the Swift compiler as an unintended dependency.\n\nIn this case, you should investigate how your build process discovers the standard library, and override the path. The correct path will be something like: \"${swift.swift.lib}/${swift.swiftModuleSubdir}\"\n\nTeX Live \nUser’s guide (experimental new interface)\nUser’s guide\nCustom packages\n\nSince release 15.09 there is a new TeX Live packaging that lives entirely under attribute texlive.\n\nUser’s guide (experimental new interface) \n\nRelease 23.11 ships with a new interface that will eventually replace texlive.combine.\n\nFor basic usage, use some of the prebuilt environments available at the top level, such as texliveBasic, texliveSmall. For the full list of prebuilt environments, inspect texlive.schemes.\n\nPackages cannot be used directly but must be assembled in an environment. To create or add packages to an environment, use\n\ntexliveSmall.withPackages (ps: with ps; [ collection-langkorean algorithms cm-super ])\n\n\nThe function withPackages can be called multiple times to add more packages.\n\nNote. Within Nixpkgs, packages should only use prebuilt environments as inputs, such as texliveSmall or texliveInfraOnly, and should not depend directly on texlive. Further dependencies should be added by calling withPackages. This is to ensure that there is a consistent and simple way to override the inputs.\n\ntexlive.withPackages uses the same logic as buildEnv. Only parts of a package are installed in an environment: its ‘runtime’ files (tex output), binaries (out output), and support files (tlpkg output). Moreover, man and info pages are assembled into separate man and info outputs. To add only the TeX files of a package, or its documentation (texdoc output), just specify the outputs:\n\ntexlive.withPackages (ps: with ps; [\n  texdoc # recommended package to navigate the documentation\n  perlPackages.LaTeXML.tex # tex files of LaTeXML, omit binaries\n  cm-super\n  cm-super.texdoc # documentation of cm-super\n])\n\n\nAll packages distributed by TeX Live, which contains most of CTAN, are available and can be found under texlive.pkgs:\n\n$ nix repl\nnix-repl> :l <nixpkgs>\nnix-repl> texlive.pkgs.[TAB]\n\n\nNote that the packages in texlive.pkgs are only provided for search purposes and must not be used directly.\n\nExperimental and subject to change without notice: to add the documentation for all packages in the environment, use\n\ntexliveSmall.__overrideTeXConfig { withDocs = true; }\n\n\nThis can be applied before or after calling withPackages.\n\nThe function currently support the parameters withDocs, withSources, and requireTeXPackages.\n\nUser’s guide \n\nFor basic usage just pull texlive.combined.scheme-basic for an environment with basic LaTeX support.\n\nIt typically won’t work to use separately installed packages together. Instead, you can build a custom set of packages like this. Most CTAN packages should be available:\n\ntexlive.combine {\n  inherit (texlive) scheme-small collection-langkorean algorithms cm-super;\n}\n\n\nThere are all the schemes, collections and a few thousand packages, as defined upstream (perhaps with tiny differences).\n\nBy default you only get executables and files needed during runtime, and a little documentation for the core packages. To change that, you need to add pkgFilter function to combine.\n\ntexlive.combine {\n  # inherit (texlive) whatever-you-want;\n  pkgFilter = pkg:\n    pkg.tlType == \"run\" || pkg.tlType == \"bin\" || pkg.hasManpages || pkg.pname == \"cm-super\";\n  # elem tlType [ \"run\" \"bin\" \"doc\" \"source\" ]\n  # there are also other attributes: version, name\n}\n\n\nYou can list packages e.g. by nix repl.\n\n$ nix repl\nnix-repl> :l <nixpkgs>\nnix-repl> texlive.collection-[TAB]\n\n\nNote that the wrapper assumes that the result has a chance to be useful. For example, the core executables should be present, as well as some core data files. The supported way of ensuring this is by including some scheme, for example scheme-basic, into the combination.\n\nTeX Live packages are also available under texlive.pkgs as derivations with outputs out, tex, texdoc, texsource, tlpkg, man, info. They cannot be installed outside of texlive.combine but are available for other uses. To repackage a font, for instance, use\n\nstdenvNoCC.mkDerivation rec {\n  src = texlive.pkgs.iwona;\n\n  inherit (src) pname version;\n\n  installPhase = ''\n    runHook preInstall\n    install -Dm644 fonts/opentype/nowacki/iwona/*.otf -t $out/share/fonts/opentype\n    runHook postInstall\n  '';\n}\n\n\nSee biber, iwona for complete examples.\n\nCustom packages \n\nYou may find that you need to use an external TeX package. A derivation for such package has to provide the contents of the “texmf” directory in its output and provide the appropriate tlType attribute (one of \"run\", \"bin\", \"doc\", \"source\"). Dependencies on other TeX packages can be listed in the attribute tlDeps.\n\nSuch derivation must then be listed in the attribute pkgs of an attribute set passed to texlive.combine, for instance by passing extraPkgs = { pkgs = [ custom_package ]; };. Within Nixpkgs, pkgs should be part of the derivation itself, allowing users to call texlive.combine { inherit (texlive) scheme-small; inherit some_tex_package; }.\n\nHere is a (very verbose) example where the attribute pkgs is attached to the derivation itself, which requires creating a fixed point. See also the packages auctex, eukleides, mftrace for more examples.\n\nwith import <nixpkgs> {};\n\nlet\n  foiltex = stdenvNoCC.mkDerivation (finalAttrs: {\n    pname = \"latex-foiltex\";\n    version = \"2.1.4b\";\n    passthru = {\n      pkgs = [ finalAttrs.finalPackage ];\n      tlDeps = with texlive; [ latex ];\n      tlType = \"run\";\n    };\n\n    srcs = [\n      (fetchurl {\n        url = \"http://mirrors.ctan.org/macros/latex/contrib/foiltex/foiltex.dtx\";\n        hash = \"sha256-/2I2xHXpZi0S988uFsGuPV6hhMw8e0U5m/P8myf42R0=\";\n      })\n      (fetchurl {\n        url = \"http://mirrors.ctan.org/macros/latex/contrib/foiltex/foiltex.ins\";\n        hash = \"sha256-KTm3pkd+Cpu0nSE2WfsNEa56PeXBaNfx/sOO2Vv0kyc=\";\n      })\n    ];\n\n    unpackPhase = ''\n      runHook preUnpack\n\n      for _src in $srcs; do\n        cp \"$_src\" $(stripHash \"$_src\")\n      done\n\n      runHook postUnpack\n    '';\n\n    nativeBuildInputs = [ texlive.combined.scheme-small ];\n\n    dontConfigure = true;\n\n    buildPhase = ''\n      runHook preBuild\n\n      # Generate the style files\n      latex foiltex.ins\n\n      runHook postBuild\n    '';\n\n    installPhase = ''\n      runHook preInstall\n\n      path=\"$out/tex/latex/foiltex\"\n      mkdir -p \"$path\"\n      cp *.{cls,def,clo} \"$path/\"\n\n      runHook postInstall\n    '';\n\n    meta = with lib; {\n      description = \"A LaTeX2e class for overhead transparencies\";\n      license = licenses.unfreeRedistributable;\n      maintainers = with maintainers; [ veprbl ];\n      platforms = platforms.all;\n    };\n  });\n\n  latex_with_foiltex = texlive.combine {\n    inherit (texlive) scheme-small;\n    inherit foiltex;\n  };\nin\n  runCommand \"test.pdf\" {\n    nativeBuildInputs = [ latex_with_foiltex ];\n  } ''\ncat >test.tex <<EOF\n\\documentclass{foils}\n\n\\title{Presentation title}\n\\date{}\n\n\\begin{document}\n\\maketitle\n\\end{document}\nEOF\n  pdflatex test.tex\n  cp test.pdf $out\n''\n\nTitanium \nBuilding a Titanium app\nEmulating or simulating the app\n\nThe Nixpkgs repository contains facilities to deploy a variety of versions of the Titanium SDK versions, a cross-platform mobile app development framework using JavaScript as an implementation language, and includes a function abstraction making it possible to build Titanium applications for Android and iOS devices from source code.\n\nNot all Titanium features supported – currently, it can only be used to build Android and iOS apps.\n\nBuilding a Titanium app \n\nWe can build a Titanium app from source for Android or iOS and for debugging or release purposes by invoking the titaniumenv.buildApp {} function:\n\ntitaniumenv.buildApp {\n  name = \"myapp\";\n  src = ./myappsource;\n\n  preBuild = \"\";\n  target = \"android\"; # or 'iphone'\n  tiVersion = \"7.1.0.GA\";\n  release = true;\n\n  androidsdkArgs = {\n    platformVersions = [ \"25\" \"26\" ];\n  };\n  androidKeyStore = ./keystore;\n  androidKeyAlias = \"myfirstapp\";\n  androidKeyStorePassword = \"secret\";\n\n  xcodeBaseDir = \"/Applications/Xcode.app\";\n  xcodewrapperArgs = {\n    version = \"9.3\";\n  };\n  iosMobileProvisioningProfile = ./myprovisioning.profile;\n  iosCertificateName = \"My Company\";\n  iosCertificate = ./mycertificate.p12;\n  iosCertificatePassword = \"secret\";\n  iosVersion = \"11.3\";\n  iosBuildStore = false;\n\n  enableWirelessDistribution = true;\n  installURL = \"/installipa.php\";\n}\n\n\nThe titaniumenv.buildApp {} function takes the following parameters:\n\nThe name parameter refers to the name in the Nix store.\n\nThe src parameter refers to the source code location of the app that needs to be built.\n\npreRebuild contains optional build instructions that are carried out before the build starts.\n\ntarget indicates for which device the app must be built. Currently only ‘android’ and ‘iphone’ (for iOS) are supported.\n\ntiVersion can be used to optionally override the requested Titanium version in tiapp.xml. If not specified, it will use the version in tiapp.xml.\n\nrelease should be set to true when building an app for submission to the Google Playstore or Apple Appstore. Otherwise, it should be false.\n\nWhen the target has been set to android, we can configure the following parameters:\n\nThe androidSdkArgs parameter refers to an attribute set that propagates all parameters to the androidenv.composeAndroidPackages {} function. This can be used to install all relevant Android plugins that may be needed to perform the Android build. If no parameters are given, it will deploy the platform SDKs for API-levels 25 and 26 by default.\n\nWhen the release parameter has been set to true, you need to provide parameters to sign the app:\n\nandroidKeyStore is the path to the keystore file\n\nandroidKeyAlias is the key alias\n\nandroidKeyStorePassword refers to the password to open the keystore file.\n\nWhen the target has been set to iphone, we can configure the following parameters:\n\nThe xcodeBaseDir parameter refers to the location where Xcode has been installed. When none value is given, the above value is the default.\n\nThe xcodewrapperArgs parameter passes arbitrary parameters to the xcodeenv.composeXcodeWrapper {} function. This can, for example, be used to adjust the default version of Xcode.\n\nWhen release has been set to true, you also need to provide the following parameters:\n\niosMobileProvisioningProfile refers to a mobile provisioning profile needed for signing.\n\niosCertificateName refers to the company name in the P12 certificate.\n\niosCertificate refers to the path to the P12 file.\n\niosCertificatePassword contains the password to open the P12 file.\n\niosVersion refers to the iOS SDK version to use. It defaults to the latest version.\n\niosBuildStore should be set to true when building for the Apple Appstore submission. For enterprise or ad-hoc builds it should be set to false.\n\nWhen enableWirelessDistribution has been enabled, you must also provide the path of the PHP script (installURL) (that is included with the iOS build environment) to enable wireless ad-hoc installations.\n\nEmulating or simulating the app \n\nIt is also possible to simulate the correspond iOS simulator build by using xcodeenv.simulateApp {} and emulate an Android APK by using androidenv.emulateApp {}.\n\nVim \nCustom configuration\nManaging plugins with Vim packages\nManaging plugins with vim-plug\nAdding new plugins to nixpkgs\nUpdating plugins in nixpkgs\nHow to maintain an out-of-tree overlay of vim plugins ?\n\nBoth Neovim and Vim can be configured to include your favorite plugins and additional libraries.\n\nLoading can be deferred; see examples.\n\nAt the moment we support two different methods for managing plugins:\n\nVim packages (recommended)\n\nvim-plug (vim only)\n\nRight now two Vim packages are available: vim which has most features that require extra dependencies disabled and vim-full which has them configurable and enabled by default.\n\nNote\n\nvim_configurable is a deprecated alias for vim-full and refers to the fact that its build-time features are configurable. It has nothing to do with user configuration, and both the vim and vim-full packages can be customized as explained in the next section.\n\nCustom configuration \n\nAdding custom .vimrc lines can be done using the following code:\n\nvim-full.customize {\n  # `name` optionally specifies the name of the executable and package\n  name = \"vim-with-plugins\";\n\n  vimrcConfig.customRC = ''\n    set hidden\n  '';\n}\n\n\nThis configuration is used when Vim is invoked with the command specified as name, in this case vim-with-plugins. You can also omit name to customize Vim itself. See the definition of vimUtils.makeCustomizable for all supported options.\n\nFor Neovim the configure argument can be overridden to achieve the same:\n\nneovim.override {\n  configure = {\n    customRC = ''\n      # here your custom configuration goes!\n    '';\n  };\n}\n\n\nIf you want to use neovim-qt as a graphical editor, you can configure it by overriding Neovim in an overlay or passing it an overridden Neovim:\n\nneovim-qt.override {\n  neovim = neovim.override {\n    configure = {\n      customRC = ''\n        # your custom configuration\n      '';\n    };\n  };\n}\n\nManaging plugins with Vim packages \n\nTo store your plugins in Vim packages (the native Vim plugin manager, see :help packages) the following example can be used:\n\nvim-full.customize {\n  vimrcConfig.packages.myVimPackage = with pkgs.vimPlugins; {\n    # loaded on launch\n    start = [ youcompleteme fugitive ];\n    # manually loadable by calling `:packadd $plugin-name`\n    # however, if a Vim plugin has a dependency that is not explicitly listed in\n    # opt that dependency will always be added to start to avoid confusion.\n    opt = [ phpCompletion elm-vim ];\n    # To automatically load a plugin when opening a filetype, add vimrc lines like:\n    # autocmd FileType php :packadd phpCompletion\n  };\n}\n\n\nmyVimPackage is an arbitrary name for the generated package. You can choose any name you like. For Neovim the syntax is:\n\nneovim.override {\n  configure = {\n    customRC = ''\n      # here your custom configuration goes!\n    '';\n    packages.myVimPackage = with pkgs.vimPlugins; {\n      # see examples below how to use custom packages\n      start = [ ];\n      # If a Vim plugin has a dependency that is not explicitly listed in\n      # opt that dependency will always be added to start to avoid confusion.\n      opt = [ ];\n    };\n  };\n}\n\n\nThe resulting package can be added to packageOverrides in ~/.nixpkgs/config.nix to make it installable:\n\n{\n  packageOverrides = pkgs: with pkgs; {\n    myVim = vim-full.customize {\n      # `name` specifies the name of the executable and package\n      name = \"vim-with-plugins\";\n      # add here code from the example section\n    };\n    myNeovim = neovim.override {\n      configure = {\n      # add code from the example section here\n      };\n    };\n  };\n}\n\n\nAfter that you can install your special grafted myVim or myNeovim packages.\n\nWhat if your favourite Vim plugin isn’t already packaged?\n\nIf one of your favourite plugins isn’t packaged, you can package it yourself:\n\n{ config, pkgs, ... }:\n\nlet\n  easygrep = pkgs.vimUtils.buildVimPlugin {\n    name = \"vim-easygrep\";\n    src = pkgs.fetchFromGitHub {\n      owner = \"dkprice\";\n      repo = \"vim-easygrep\";\n      rev = \"d0c36a77cc63c22648e792796b1815b44164653a\";\n      hash = \"sha256-bL33/S+caNmEYGcMLNCanFZyEYUOUmSsedCVBn4tV3g=\";\n    };\n  };\nin\n{\n  environment.systemPackages = [\n    (\n      pkgs.neovim.override {\n        configure = {\n          packages.myPlugins = with pkgs.vimPlugins; {\n          start = [\n            vim-go # already packaged plugin\n            easygrep # custom package\n          ];\n          opt = [];\n        };\n        # ...\n      };\n     }\n    )\n  ];\n}\n\n\nIf your package requires building specific parts, use instead pkgs.vimUtils.buildVimPlugin.\n\nSpecificities for some plugins\nTreesitter\n\nBy default nvim-treesitter encourages you to download, compile and install the required Treesitter grammars at run time with :TSInstall. This works poorly on NixOS. Instead, to install the nvim-treesitter plugins with a set of precompiled grammars, you can use nvim-treesitter.withPlugins function:\n\n(pkgs.neovim.override {\n  configure = {\n    packages.myPlugins = with pkgs.vimPlugins; {\n      start = [\n        (nvim-treesitter.withPlugins (\n          plugins: with plugins; [\n            nix\n            python\n          ]\n        ))\n      ];\n    };\n  };\n})\n\n\nTo enable all grammars packaged in nixpkgs, use pkgs.vimPlugins.nvim-treesitter.withAllGrammars.\n\nManaging plugins with vim-plug \n\nTo use vim-plug to manage your Vim plugins the following example can be used:\n\nvim-full.customize {\n  vimrcConfig.packages.myVimPackage = with pkgs.vimPlugins; {\n    # loaded on launch\n    plug.plugins = [ youcompleteme fugitive phpCompletion elm-vim ];\n  };\n}\n\n\nNote: this is not possible anymore for Neovim.\n\nAdding new plugins to nixpkgs \n\nNix expressions for Vim plugins are stored in pkgs/applications/editors/vim/plugins. For the vast majority of plugins, Nix expressions are automatically generated by running nix-shell -p vimPluginsUpdater --run vim-plugins-updater. This creates a generated.nix file based on the plugins listed in vim-plugin-names.\n\nAfter running the updater, if nvim-treesitter received an update, also run nvim-treesitter/update.py to update the tree sitter grammars for nvim-treesitter.\n\nSome plugins require overrides in order to function properly. Overrides are placed in overrides.nix. Overrides are most often required when a plugin requires some dependencies, or extra steps are required during the build process. For example deoplete-fish requires both deoplete-nvim and vim-fish, and so the following override was added:\n\ndeoplete-fish = super.deoplete-fish.overrideAttrs(old: {\n  dependencies = with super; [ deoplete-nvim vim-fish ];\n});\n\n\nSometimes plugins require an override that must be changed when the plugin is updated. This can cause issues when Vim plugins are auto-updated but the associated override isn’t updated. For these plugins, the override should be written so that it specifies all information required to install the plugin, and running ./update.py doesn’t change the derivation for the plugin. Manually updating the override is required to update these types of plugins. An example of such a plugin is LanguageClient-neovim.\n\nTo add a new plugin, run ./update.py add \"[owner]/[name]\". NOTE: This script automatically commits to your git repository. Be sure to check out a fresh branch before running.\n\nFinally, there are some plugins that are also packaged in nodePackages because they have Javascript-related build steps, such as running webpack. Those plugins are not listed in vim-plugin-names or managed by update.py at all, and are included separately in overrides.nix. Currently, all these plugins are related to the coc.nvim ecosystem of the Language Server Protocol integration with Vim/Neovim.\n\nUpdating plugins in nixpkgs \n\nRun the update script with a GitHub API token that has at least public_repo access. Running the script without the token is likely to result in rate-limiting (429 errors). For steps on creating an API token, please refer to GitHub’s token documentation.\n\nGITHUB_API_TOKEN=my_token ./pkgs/applications/editors/vim/plugins/update.py\n\n\nAlternatively, set the number of processes to a lower count to avoid rate-limiting.\n\n\nnix-shell -p vimPluginsUpdater --run 'vim-plugins-updater --proc 1'\n\nHow to maintain an out-of-tree overlay of vim plugins ? \n\nYou can use the updater script to generate basic packages out of a custom vim plugin list:\n\nnix-shell -p vimPluginsUpdater --run vim-plugins-updater -i vim-plugin-names -o generated.nix --no-commit\n\n\nwith the contents of vim-plugin-names being for example:\n\nrepo,branch,alias\npwntester/octo.nvim,,\n\n\nYou can then reference the generated vim plugins via:\n\nmyVimPlugins = pkgs.vimPlugins.extend (\n  (pkgs.callPackage ./generated.nix {})\n);\n\nPackages \n\nTable of Contents\n\nCitrix Workspace\ndarwin.linux-builder\nDLib\nEclipse\nElm\nEmacs\nFirefox\nFish\nFUSE\nibus-engines.typing-booster\nKakoune\nLinux kernel\nLocales\n/etc files\nNginx\nOpenGL\nInteractive shell helpers\nSteam\nCataclysm: Dark Days Ahead\nUrxvt\nWeeChat\nX.org\n\nThis chapter contains information about how to use and maintain the Nix expressions for a number of specific packages, such as the Linux kernel or X.org.\n\nCitrix Workspace \nBasic usage\nCitrix Self-service\nCustom certificates\n\nThe Citrix Workspace App is a remote desktop viewer which provides access to XenDesktop installations.\n\nBasic usage \n\nThe tarball archive needs to be downloaded manually, as the license agreements of the vendor for Citrix Workspace needs to be accepted first. Then run nix-prefetch-url file://$PWD/linuxx64-$version.tar.gz. With the archive available in the store, the package can be built and installed with Nix.\n\nCitrix Self-service \n\nThe self-service is an application managing Citrix desktops and applications. Please note that this feature only works with at least citrix_workspace_20_06_0 and later versions.\n\nIn order to set this up, you first have to download the .cr file from the Netscaler Gateway. After that, you can configure the selfservice like this:\n\n$ storebrowse -C ~/Downloads/receiverconfig.cr\n$ selfservice\n\nCustom certificates \n\nThe Citrix Workspace App in nixpkgs trusts several certificates from the Mozilla database by default. However, several companies using Citrix might require their own corporate certificate. On distros with imperative packaging, these certs can be stored easily in $ICAROOT, however this directory is a store path in nixpkgs. In order to work around this issue, the package provides a simple mechanism to add custom certificates without rebuilding the entire package using symlinkJoin:\n\nwith import <nixpkgs> { config.allowUnfree = true; };\nlet\n  extraCerts = [\n    ./custom-cert-1.pem\n    ./custom-cert-2.pem # ...\n  ];\nin citrix_workspace.override { inherit extraCerts; }\n\ndarwin.linux-builder \nExample flake usage\nReconfiguring the remote builder\nTroubleshooting the generated configuration\n\ndarwin.linux-builder provides a way to bootstrap a Linux remote builder on a macOS machine.\n\nThis requires macOS version 12.4 or later.\n\nThe remote builder runs on host port 31022 by default. You can change it by overriding virtualisation.darwin-builder.hostPort. See the example.\n\nYou will also need to be a trusted user for your Nix installation. In other words, your /etc/nix/nix.conf should have something like:\n\nextra-trusted-users = <your username goes here>\n\n\nTo launch the remote builder, run the following flake:\n\n$ nix run nixpkgs#darwin.linux-builder\n\n\nThat will prompt you to enter your sudo password:\n\n+ sudo --reset-timestamp /nix/store/…-install-credentials.sh ./keys\nPassword:\n\n\n… so that it can install a private key used to ssh into the build server. After that the script will launch the virtual machine and automatically log you in as the builder user:\n\n<<< Welcome to NixOS 22.11.20220901.1bd8d11 (aarch64) - ttyAMA0 >>>\n\nRun 'nixos-help' for the NixOS manual.\n\nnixos login: builder (automatic login)\n\n\n[builder@nixos:~]$\n\n\nNote: When you need to stop the VM, run shutdown now as the builder user.\n\nTo delegate builds to the remote builder, add the following options to your nix.conf file:\n\n# - Replace ${ARCH} with either aarch64 or x86_64 to match your host machine\n# - Replace ${MAX_JOBS} with the maximum number of builds (pick 4 if you're not sure)\nbuilders = ssh-ng://builder@linux-builder ${ARCH}-linux /etc/nix/builder_ed25519 ${MAX_JOBS} - - - c3NoLWVkMjU1MTkgQUFBQUMzTnphQzFsWkRJMU5URTVBQUFBSUpCV2N4Yi9CbGFxdDFhdU90RStGOFFVV3JVb3RpQzVxQkorVXVFV2RWQ2Igcm9vdEBuaXhvcwo=\n\n# Not strictly necessary, but this will reduce your disk utilization\nbuilders-use-substitutes = true\n\n\nTo allow Nix to connect to a remote builder not running on port 22, you will also need to create a new file at /etc/ssh/ssh_config.d/100-linux-builder.conf:\n\nHost linux-builder\n  Hostname localhost\n  HostKeyAlias linux-builder\n  Port 31022\n\n\n… and then restart your Nix daemon to apply the change:\n\n$ sudo launchctl kickstart -k system/org.nixos.nix-daemon\n\nExample flake usage \n{\n  inputs = {\n    nixpkgs.url = \"github:nixos/nixpkgs/nixpkgs-22.11-darwin\";\n    darwin.url = \"github:lnl7/nix-darwin/master\";\n    darwin.inputs.nixpkgs.follows = \"nixpkgs\";\n  };\n\n  outputs = { self, darwin, nixpkgs, ... }@inputs:\n  let\n\n    inherit (darwin.lib) darwinSystem;\n    system = \"aarch64-darwin\";\n    pkgs = nixpkgs.legacyPackages.\"${system}\";\n    linuxSystem = builtins.replaceStrings [ \"darwin\" ] [ \"linux\" ] system;\n\n    darwin-builder = nixpkgs.lib.nixosSystem {\n      system = linuxSystem;\n      modules = [\n        \"${nixpkgs}/nixos/modules/profiles/macos-builder.nix\"\n        { virtualisation.host.pkgs = pkgs; }\n      ];\n    };\n  in {\n\n    darwinConfigurations = {\n      machine1 = darwinSystem {\n        inherit system;\n        modules = [\n          {\n            nix.distributedBuilds = true;\n            nix.buildMachines = [{\n              hostName = \"ssh://builder@localhost\";\n              system = linuxSystem;\n              maxJobs = 4;\n              supportedFeatures = [ \"kvm\" \"benchmark\" \"big-parallel\" ];\n            }];\n\n            launchd.daemons.darwin-builder = {\n              command = \"${darwin-builder.config.system.build.macos-builder-installer}/bin/create-builder\";\n              serviceConfig = {\n                KeepAlive = true;\n                RunAtLoad = true;\n                StandardOutPath = \"/var/log/darwin-builder.log\";\n                StandardErrorPath = \"/var/log/darwin-builder.log\";\n              };\n            };\n          }\n        ];\n      };\n    };\n\n  };\n}\n\nReconfiguring the remote builder \n\nInitially you should not change the remote builder configuration else you will not be able to use the binary cache. However, after you have the remote builder running locally you may use it to build a modified remote builder with additional storage or memory.\n\nTo do this, you just need to set the virtualisation.darwin-builder.* parameters as in the example below and rebuild.\n\n    darwin-builder = nixpkgs.lib.nixosSystem {\n      system = linuxSystem;\n      modules = [\n        \"${nixpkgs}/nixos/modules/profiles/macos-builder.nix\"\n        {\n          virtualisation.host.pkgs = pkgs;\n          virtualisation.darwin-builder.diskSize = 5120;\n          virtualisation.darwin-builder.memorySize = 1024;\n          virtualisation.darwin-builder.hostPort = 33022;\n          virtualisation.darwin-builder.workingDirectory = \"/var/lib/darwin-builder\";\n        }\n      ];\n\n\nYou may make any other changes to your VM in this attribute set. For example, you could enable Docker or X11 forwarding to your Darwin host.\n\nTroubleshooting the generated configuration \n\nThe linux-builder package exposes the attributes nixosConfig and nixosOptions that allow you to inspect the generated NixOS configuration in the nix repl. For example:\n\n$ nix repl --file ~/src/nixpkgs --argstr system aarch64-darwin\n\nnix-repl> darwin.linux-builder.nixosConfig.nix.package\n«derivation /nix/store/...-nix-2.17.0.drv»\n\nnix-repl> :p darwin.linux-builder.nixosOptions.virtualisation.memorySize.definitionsWithLocations\n[ { file = \"/home/user/src/nixpkgs/nixos/modules/profiles/macos-builder.nix\"; value = 3072; } ]\n\n\nDLib \nCompiling without AVX support\n\nDLib is a modern, C++-based toolkit which provides several machine learning algorithms.\n\nCompiling without AVX support \n\nEspecially older CPUs don’t support AVX (Advanced Vector Extensions) instructions that are used by DLib to optimize their algorithms.\n\nOn the affected hardware errors like Illegal instruction will occur. In those cases AVX support needs to be disabled:\n\nself: super: { dlib = super.dlib.override { avxSupport = false; }; }\n\nEclipse \n\nThe Nix expressions related to the Eclipse platform and IDE are in pkgs/applications/editors/eclipse.\n\nNixpkgs provides a number of packages that will install Eclipse in its various forms. These range from the bare-bones Eclipse Platform to the more fully featured Eclipse SDK or Scala-IDE packages and multiple version are often available. It is possible to list available Eclipse packages by issuing the command:\n\n$ nix-env -f '<nixpkgs>' -qaP -A eclipses --description\n\n\nOnce an Eclipse variant is installed, it can be run using the eclipse command, as expected. From within Eclipse, it is then possible to install plugins in the usual manner by either manually specifying an Eclipse update site or by installing the Marketplace Client plugin and using it to discover and install other plugins. This installation method provides an Eclipse installation that closely resemble a manually installed Eclipse.\n\nIf you prefer to install plugins in a more declarative manner, then Nixpkgs also offer a number of Eclipse plugins that can be installed in an Eclipse environment. This type of environment is created using the function eclipseWithPlugins found inside the nixpkgs.eclipses attribute set. This function takes as argument { eclipse, plugins ? [], jvmArgs ? [] } where eclipse is a one of the Eclipse packages described above, plugins is a list of plugin derivations, and jvmArgs is a list of arguments given to the JVM running the Eclipse. For example, say you wish to install the latest Eclipse Platform with the popular Eclipse Color Theme plugin and also allow Eclipse to use more RAM. You could then add:\n\npackageOverrides = pkgs: {\n  myEclipse = with pkgs.eclipses; eclipseWithPlugins {\n    eclipse = eclipse-platform;\n    jvmArgs = [ \"-Xmx2048m\" ];\n    plugins = [ plugins.color-theme ];\n  };\n}\n\n\nto your Nixpkgs configuration (~/.config/nixpkgs/config.nix) and install it by running nix-env -f '<nixpkgs>' -iA myEclipse and afterward run Eclipse as usual. It is possible to find out which plugins are available for installation using eclipseWithPlugins by running:\n\n$ nix-env -f '<nixpkgs>' -qaP -A eclipses.plugins --description\n\n\nIf there is a need to install plugins that are not available in Nixpkgs then it may be possible to define these plugins outside Nixpkgs using the buildEclipseUpdateSite and buildEclipsePlugin functions found in the nixpkgs.eclipses.plugins attribute set. Use the buildEclipseUpdateSite function to install a plugin distributed as an Eclipse update site. This function takes { name, src } as argument, where src indicates the Eclipse update site archive. All Eclipse features and plugins within the downloaded update site will be installed. When an update site archive is not available, then the buildEclipsePlugin function can be used to install a plugin that consists of a pair of feature and plugin JARs. This function takes an argument { name, srcFeature, srcPlugin } where srcFeature and srcPlugin are the feature and plugin JARs, respectively.\n\nExpanding the previous example with two plugins using the above functions, we have:\n\npackageOverrides = pkgs: {\n  myEclipse = with pkgs.eclipses; eclipseWithPlugins {\n    eclipse = eclipse-platform;\n    jvmArgs = [ \"-Xmx2048m\" ];\n    plugins = [\n      plugins.color-theme\n      (plugins.buildEclipsePlugin {\n        name = \"myplugin1-1.0\";\n        srcFeature = fetchurl {\n          url = \"http://…/features/myplugin1.jar\";\n          hash = \"sha256-123…\";\n        };\n        srcPlugin = fetchurl {\n          url = \"http://…/plugins/myplugin1.jar\";\n          hash = \"sha256-123…\";\n        };\n      });\n      (plugins.buildEclipseUpdateSite {\n        name = \"myplugin2-1.0\";\n        src = fetchurl {\n          stripRoot = false;\n          url = \"http://…/myplugin2.zip\";\n          hash = \"sha256-123…\";\n        };\n      });\n    ];\n  };\n}\n\nElm \n\nTo start a development environment, run:\n\nnix-shell -p elmPackages.elm elmPackages.elm-format\n\n\nTo update the Elm compiler, see nixpkgs/pkgs/development/compilers/elm/README.md.\n\nTo package Elm applications, read about elm2nix.\n\nEmacs \nConfiguring Emacs\nConfiguring Emacs \n\nThe Emacs package comes with some extra helpers to make it easier to configure. emacs.pkgs.withPackages allows you to manage packages from ELPA. This means that you will not have to install that packages from within Emacs. For instance, if you wanted to use company counsel, flycheck, ivy, magit, projectile, and use-package you could use this as a ~/.config/nixpkgs/config.nix override:\n\n{\n  packageOverrides = pkgs: with pkgs; {\n    myEmacs = emacs.pkgs.withPackages (epkgs: (with epkgs.melpaStablePackages; [\n      company\n      counsel\n      flycheck\n      ivy\n      magit\n      projectile\n      use-package\n    ]));\n  }\n}\n\n\nYou can install it like any other packages via nix-env -iA myEmacs. However, this will only install those packages. It will not configure them for us. To do this, we need to provide a configuration file. Luckily, it is possible to do this from within Nix! By modifying the above example, we can make Emacs load a custom config file. The key is to create a package that provides a default.el file in /share/emacs/site-start/. Emacs knows to load this file automatically when it starts.\n\n{\n  packageOverrides = pkgs: with pkgs; rec {\n    myEmacsConfig = writeText \"default.el\" ''\n      (eval-when-compile\n        (require 'use-package))\n\n      ;; load some packages\n\n      (use-package company\n        :bind (\"<C-tab>\" . company-complete)\n        :diminish company-mode\n        :commands (company-mode global-company-mode)\n        :defer 1\n        :config\n        (global-company-mode))\n\n      (use-package counsel\n        :commands (counsel-descbinds)\n        :bind (([remap execute-extended-command] . counsel-M-x)\n               (\"C-x C-f\" . counsel-find-file)\n               (\"C-c g\" . counsel-git)\n               (\"C-c j\" . counsel-git-grep)\n               (\"C-c k\" . counsel-ag)\n               (\"C-x l\" . counsel-locate)\n               (\"M-y\" . counsel-yank-pop)))\n\n      (use-package flycheck\n        :defer 2\n        :config (global-flycheck-mode))\n\n      (use-package ivy\n        :defer 1\n        :bind ((\"C-c C-r\" . ivy-resume)\n               (\"C-x C-b\" . ivy-switch-buffer)\n               :map ivy-minibuffer-map\n               (\"C-j\" . ivy-call))\n        :diminish ivy-mode\n        :commands ivy-mode\n        :config\n        (ivy-mode 1))\n\n      (use-package magit\n        :defer\n        :if (executable-find \"git\")\n        :bind ((\"C-x g\" . magit-status)\n               (\"C-x G\" . magit-dispatch-popup))\n        :init\n        (setq magit-completing-read-function 'ivy-completing-read))\n\n      (use-package projectile\n        :commands projectile-mode\n        :bind-keymap (\"C-c p\" . projectile-command-map)\n        :defer 5\n        :config\n        (projectile-global-mode))\n    '';\n\n    myEmacs = emacs.pkgs.withPackages (epkgs: (with epkgs.melpaStablePackages; [\n      (runCommand \"default.el\" {} ''\n         mkdir -p $out/share/emacs/site-lisp\n         cp ${myEmacsConfig} $out/share/emacs/site-lisp/default.el\n       '')\n      company\n      counsel\n      flycheck\n      ivy\n      magit\n      projectile\n      use-package\n    ]));\n  };\n}\n\n\nThis provides a fairly full Emacs start file. It will load in addition to the user’s personal config. You can always disable it by passing -q to the Emacs command.\n\nSometimes emacs.pkgs.withPackages is not enough, as this package set has some priorities imposed on packages (with the lowest priority assigned to GNU-devel ELPA, and the highest for packages manually defined in pkgs/applications/editors/emacs/elisp-packages/manual-packages). But you can’t control these priorities when some package is installed as a dependency. You can override it on a per-package-basis, providing all the required dependencies manually, but it’s tedious and there is always a possibility that an unwanted dependency will sneak in through some other package. To completely override such a package, you can use overrideScope.\n\noverrides = self: super: rec {\n  haskell-mode = self.melpaPackages.haskell-mode;\n  ...\n};\n((emacsPackagesFor emacs).overrideScope overrides).withPackages\n  (p: with p; [\n    # here both these package will use haskell-mode of our own choice\n    ghc-mod\n    dante\n  ])\n\nFirefox \nBuild wrapped Firefox with extensions and policies\nTroubleshooting\nBuild wrapped Firefox with extensions and policies \n\nThe wrapFirefox function allows to pass policies, preferences and extensions that are available to Firefox. With the help of fetchFirefoxAddon this allows to build a Firefox version that already comes with add-ons pre-installed:\n\n{\n  # Nix firefox addons only work with the firefox-esr package.\n  myFirefox = wrapFirefox firefox-esr-unwrapped {\n    nixExtensions = [\n      (fetchFirefoxAddon {\n        name = \"ublock\"; # Has to be unique!\n        url = \"https://addons.mozilla.org/firefox/downloads/file/3679754/ublock_origin-1.31.0-an+fx.xpi\";\n        hash = \"sha256-2e73AbmYZlZXCP5ptYVcFjQYdjDp4iPoEPEOSCVF5sA=\";\n      })\n    ];\n\n    extraPolicies = {\n      CaptivePortal = false;\n      DisableFirefoxStudies = true;\n      DisablePocket = true;\n      DisableTelemetry = true;\n      DisableFirefoxAccounts = true;\n      FirefoxHome = {\n        Pocket = false;\n        Snippets = false;\n      };\n      UserMessaging = {\n        ExtensionRecommendations = false;\n        SkipOnboarding = true;\n      };\n      SecurityDevices = {\n        # Use a proxy module rather than `nixpkgs.config.firefox.smartcardSupport = true`\n        \"PKCS#11 Proxy Module\" = \"${pkgs.p11-kit}/lib/p11-kit-proxy.so\";\n      };\n    };\n\n    extraPrefs = ''\n      // Show more ssl cert infos\n      lockPref(\"security.identityblock.show_extended_validation\", true);\n    '';\n  };\n}\n\n\nIf nixExtensions != null, then all manually installed add-ons will be uninstalled from your browser profile. To view available enterprise policies, visit enterprise policies or type into the Firefox URL bar: about:policies#documentation. Nix installed add-ons do not have a valid signature, which is why signature verification is disabled. This does not compromise security because downloaded add-ons are checksummed and manual add-ons can’t be installed. Also, make sure that the name field of fetchFirefoxAddon is unique. If you remove an add-on from the nixExtensions array, rebuild and start Firefox: the removed add-on will be completely removed with all of its settings.\n\nTroubleshooting \n\nIf add-ons are marked as broken or the signature is invalid, make sure you have Firefox ESR installed. Normal Firefox does not provide the ability anymore to disable signature verification for add-ons thus nix add-ons get disabled by the normal Firefox binary.\n\nIf add-ons do not appear installed despite being defined in your nix configuration file, reset the local add-on state of your Firefox profile by clicking Help -> More Troubleshooting Information -> Refresh Firefox. This can happen if you switch from manual add-on mode to nix add-on mode and then back to manual mode and then again to nix add-on mode.\n\nFish \nVendor Fish scripts\nPackaging Fish plugins\nFish wrapper\n\nFish is a “smart and user-friendly command line shell” with support for plugins.\n\nVendor Fish scripts \n\nAny package may ship its own Fish completions, configuration snippets, and functions. Those should be installed to $out/share/fish/vendor_{completions,conf,functions}.d respectively.\n\nWhen the programs.fish.enable and programs.fish.vendor.{completions,config,functions}.enable options from the NixOS Fish module are set to true, those paths are symlinked in the current system environment and automatically loaded by Fish.\n\nPackaging Fish plugins \n\nWhile packages providing standalone executables belong to the top level, packages which have the sole purpose of extending Fish belong to the fishPlugins scope and should be registered in pkgs/shells/fish/plugins/default.nix.\n\nThe buildFishPlugin utility function can be used to automatically copy Fish scripts from $src/{completions,conf,conf.d,functions} to the standard vendor installation paths. It also sets up the test environment so that the optional checkPhase is executed in a Fish shell with other already packaged plugins and package-local Fish functions specified in checkPlugins and checkFunctionDirs respectively.\n\nSee pkgs/shells/fish/plugins/pure.nix for an example of Fish plugin package using buildFishPlugin and running unit tests with the fishtape test runner.\n\nFish wrapper \n\nThe wrapFish package is a wrapper around Fish which can be used to create Fish shells initialized with some plugins as well as completions, configuration snippets and functions sourced from the given paths. This provides a convenient way to test Fish plugins and scripts without having to alter the environment.\n\nwrapFish {\n  pluginPkgs = with fishPlugins; [ pure foreign-env ];\n  completionDirs = [];\n  functionDirs = [];\n  confDirs = [ \"/path/to/some/fish/init/dir/\" ];\n}\n\nFUSE \n\nSome packages rely on FUSE to provide support for additional filesystems not supported by the kernel.\n\nIn general, FUSE software are primarily developed for Linux but many of them can also run on macOS. Nixpkgs supports FUSE packages on macOS, but it requires macFUSE to be installed outside of Nix. macFUSE currently isn’t packaged in Nixpkgs mainly because it includes a kernel extension, which isn’t supported by Nix outside of NixOS.\n\nIf a package fails to run on macOS with an error message similar to the following, it’s a likely sign that you need to have macFUSE installed.\n\ndyld: Library not loaded: /usr/local/lib/libfuse.2.dylib\nReferenced from: /nix/store/w8bi72bssv0bnxhwfw3xr1mvn7myf37x-sshfs-fuse-2.10/bin/sshfs\nReason: image not found\n[1]    92299 abort      /nix/store/w8bi72bssv0bnxhwfw3xr1mvn7myf37x-sshfs-fuse-2.10/bin/sshfs\n\n\nPackage maintainers may often encounter the following error when building FUSE packages on macOS:\n\nchecking for fuse.h... no\nconfigure: error: No fuse.h found.\n\n\nThis happens on autoconf based projects that use AC_CHECK_HEADERS or AC_CHECK_LIBS to detect libfuse, and will occur even when the fuse package is included in buildInputs. It happens because libfuse headers throw an error on macOS if the FUSE_USE_VERSION macro is undefined. Many projects do define FUSE_USE_VERSION, but only inside C source files. This results in the above error at configure time because the configure script would attempt to compile sample FUSE programs without defining FUSE_USE_VERSION.\n\nThere are two possible solutions for this problem in Nixpkgs:\n\nPass FUSE_USE_VERSION to the configure script by adding CFLAGS=-DFUSE_USE_VERSION=25 in configureFlags. The actual value would have to match the definition used in the upstream source code.\n\nRemove AC_CHECK_HEADERS / AC_CHECK_LIBS for libfuse.\n\nHowever, a better solution might be to fix the build script upstream to use PKG_CHECK_MODULES instead. This approach wouldn’t suffer from the problem that AC_CHECK_HEADERS/AC_CHECK_LIBS has at the price of introducing a dependency on pkg-config.\n\nibus-engines.typing-booster \nActivating the engine\nUsing custom hunspell dictionaries\nBuilt-in emoji picker\n\nThis package is an ibus-based completion method to speed up typing.\n\nActivating the engine \n\nIBus needs to be configured accordingly to activate typing-booster. The configuration depends on the desktop manager in use. For detailed instructions, please refer to the upstream docs.\n\nOn NixOS, you need to explicitly enable ibus with given engines before customizing your desktop to use typing-booster. This can be achieved using the ibus module:\n\n{ pkgs, ... }: {\n  i18n.inputMethod = {\n    enabled = \"ibus\";\n    ibus.engines = with pkgs.ibus-engines; [ typing-booster ];\n  };\n}\n\nUsing custom hunspell dictionaries \n\nThe IBus engine is based on hunspell to support completion in many languages. By default, the dictionaries de-de, en-us, fr-moderne es-es, it-it, sv-se and sv-fi are in use. To add another dictionary, the package can be overridden like this:\n\nibus-engines.typing-booster.override { langs = [ \"de-at\" \"en-gb\" ]; }\n\n\nNote: each language passed to langs must be an attribute name in pkgs.hunspellDicts.\n\nBuilt-in emoji picker \n\nThe ibus-engines.typing-booster package contains a program named emoji-picker. To display all emojis correctly, a special font such as noto-fonts-color-emoji is needed:\n\nOn NixOS, it can be installed using the following expression:\n\n{ pkgs, ... }: {\n  fonts.packages = with pkgs; [ noto-fonts-color-emoji ];\n}\n\nKakoune \n\nKakoune can be built to autoload plugins:\n\n(kakoune.override {\n  plugins = with pkgs.kakounePlugins; [ parinfer-rust ];\n})\n\nLinux kernel \n\nThe Nix expressions to build the Linux kernel are in pkgs/os-specific/linux/kernel.\n\nThe function that builds the kernel has an argument kernelPatches which should be a list of {name, patch, extraConfig} attribute sets, where name is the name of the patch (which is included in the kernel’s meta.description attribute), patch is the patch itself (possibly compressed), and extraConfig (optional) is a string specifying extra options to be concatenated to the kernel configuration file (.config).\n\nThe kernel derivation exports an attribute features specifying whether optional functionality is or isn’t enabled. This is used in NixOS to implement kernel-specific behaviour. For instance, if the kernel has the iwlwifi feature (i.e., has built-in support for Intel wireless chipsets), then NixOS doesn’t have to build the external iwlwifi package:\n\nmodulesTree = [kernel]\n  ++ pkgs.lib.optional (!kernel.features ? iwlwifi) kernelPackages.iwlwifi\n  ++ ...;\n\n\nHow to add a new (major) version of the Linux kernel to Nixpkgs:\n\nCopy the old Nix expression (e.g., linux-2.6.21.nix) to the new one (e.g., linux-2.6.22.nix) and update it.\n\nAdd the new kernel to the kernels attribute set in linux-kernels.nix (e.g., create an attribute kernel_2_6_22).\n\nNow we’re going to update the kernel configuration. First unpack the kernel. Then for each supported platform (i686, x86_64, uml) do the following:\n\nMake a copy from the old config (e.g., config-2.6.21-i686-smp) to the new one (e.g., config-2.6.22-i686-smp).\n\nCopy the config file for this platform (e.g., config-2.6.22-i686-smp) to .config in the kernel source tree.\n\nRun make oldconfig ARCH={i386,x86_64,um} and answer all questions. (For the uml configuration, also add SHELL=bash.) Make sure to keep the configuration consistent between platforms (i.e., don’t enable some feature on i686 and disable it on x86_64).\n\nIf needed, you can also run make menuconfig:\n\n$ nix-env -f \"<nixpkgs>\" -iA ncurses\n$ export NIX_CFLAGS_LINK=-lncurses\n$ make menuconfig ARCH=arch\n\n\nCopy .config over the new config file (e.g., config-2.6.22-i686-smp).\n\nTest building the kernel: nix-build -A linuxKernel.kernels.kernel_2_6_22. If it compiles, ship it! For extra credit, try booting NixOS with it.\n\nIt may be that the new kernel requires updating the external kernel modules and kernel-dependent packages listed in the linuxPackagesFor function in linux-kernels.nix (such as the NVIDIA drivers, AUFS, etc.). If the updated packages aren’t backwards compatible with older kernels, you may need to keep the older versions around.\n\nLocales \n\nTo allow simultaneous use of packages linked against different versions of glibc with different locale archive formats, Nixpkgs patches glibc to rely on LOCALE_ARCHIVE environment variable.\n\nOn non-NixOS distributions, this variable is obviously not set. This can cause regressions in language support or even crashes in some Nixpkgs-provided programs. The simplest way to mitigate this problem is exporting the LOCALE_ARCHIVE variable pointing to ${glibcLocales}/lib/locale/locale-archive. The drawback (and the reason this is not the default) is the relatively large (a hundred MiB) size of the full set of locales. It is possible to build a custom set of locales by overriding parameters allLocales and locales of the package.\n\n/etc files \n\nCertain calls in glibc require access to runtime files found in /etc such as /etc/protocols or /etc/services – getprotobyname is one such function.\n\nOn non-NixOS distributions these files are typically provided by packages (i.e., netbase) if not already pre-installed in your distribution. This can cause non-reproducibility for code if they rely on these files being present.\n\nIf iana-etc is part of your buildInputs, then it will set the environment variables NIX_ETC_PROTOCOLS and NIX_ETC_SERVICES to the corresponding files in the package through a setup hook.\n\n> nix-shell -p iana-etc\n\n[nix-shell:~]$ env | grep NIX_ETC\nNIX_ETC_SERVICES=/nix/store/aj866hr8fad8flnggwdhrldm0g799ccz-iana-etc-20210225/etc/services\nNIX_ETC_PROTOCOLS=/nix/store/aj866hr8fad8flnggwdhrldm0g799ccz-iana-etc-20210225/etc/protocols\n\n\nNixpkg’s version of glibc has been patched to check for the existence of these environment variables. If the environment variables are not set, then it will attempt to find the files at the default location within /etc.\n\nNginx \nETags on static files served from the Nix store\n\nNginx is a reverse proxy and lightweight webserver.\n\nETags on static files served from the Nix store \n\nHTTP has a couple of different mechanisms for caching to prevent clients from having to download the same content repeatedly if a resource has not changed since the last time it was requested. When nginx is used as a server for static files, it implements the caching mechanism based on the Last-Modified response header automatically; unfortunately, it works by using filesystem timestamps to determine the value of the Last-Modified header. This doesn’t give the desired behavior when the file is in the Nix store because all file timestamps are set to 0 (for reasons related to build reproducibility).\n\nFortunately, HTTP supports an alternative (and more effective) caching mechanism: the ETag response header. The value of the ETag header specifies some identifier for the particular content that the server is sending (e.g., a hash). When a client makes a second request for the same resource, it sends that value back in an If-None-Match header. If the ETag value is unchanged, then the server does not need to resend the content.\n\nAs of NixOS 19.09, the nginx package in Nixpkgs is patched such that when nginx serves a file out of /nix/store, the hash in the store path is used as the ETag header in the HTTP response, thus providing proper caching functionality. This happens automatically; you do not need to do modify any configuration to get this behavior.\n\nOpenGL \nNixOS Desktop\nNix on GNU/Linux\n\nOpenGL support varies depending on which hardware is used and which drivers are available and loaded.\n\nBroadly, we support both GL vendors: Mesa and NVIDIA.\n\nNixOS Desktop \n\nThe NixOS desktop or other non-headless configurations are the primary target for OpenGL libraries and applications. The current solution for discovering which drivers are available is based on libglvnd. libglvnd performs “vendor-neutral dispatch”, trying a variety of techniques to find the system’s GL implementation. In practice, this will be either via standard GLX for X11 users or EGL for Wayland users, and supporting either NVIDIA or Mesa extensions.\n\nNix on GNU/Linux \n\nIf you are using a non-NixOS GNU/Linux/X11 desktop with free software video drivers, consider launching OpenGL-dependent programs from Nixpkgs with Nixpkgs versions of libglvnd and mesa.drivers in LD_LIBRARY_PATH. For Mesa drivers, the Linux kernel version doesn’t have to match nixpkgs.\n\nFor proprietary video drivers, you might have luck with also adding the corresponding video driver package.\n\nInteractive shell helpers \n\nSome packages provide the shell integration to be more useful. But unlike other systems, nix doesn’t have a standard share directory location. This is why a bunch PACKAGE-share scripts are shipped that print the location of the corresponding shared folder. Current list of such packages is as following:\n\nfzf : fzf-share\n\nE.g. fzf can then be used in the .bashrc like this:\n\nsource \"$(fzf-share)/completion.bash\"\nsource \"$(fzf-share)/key-bindings.bash\"\n\nSteam \nSteam in Nix\nHow to play\nTroubleshooting\nsteam-run\nSteam in Nix \n\nSteam is distributed as a .deb file, for now only as an i686 package (the amd64 package only has documentation). When unpacked, it has a script called steam that in Ubuntu (their target distro) would go to /usr/bin. When run for the first time, this script copies some files to the user’s home, which include another script that is the ultimate responsible for launching the steam binary, which is also in $HOME.\n\nNix problems and constraints:\n\nWe don’t have /bin/bash and many scripts point there. Same thing for /usr/bin/python.\n\nWe don’t have the dynamic loader in /lib.\n\nThe steam.sh script in $HOME cannot be patched, as it is checked and rewritten by steam.\n\nThe steam binary cannot be patched, it’s also checked.\n\nThe current approach to deploy Steam in NixOS is composing a FHS-compatible chroot environment, as documented here. This allows us to have binaries in the expected paths without disrupting the system, and to avoid patching them to work in a non FHS environment.\n\nHow to play \n\nUse programs.steam.enable = true; if you want to add steam to systemPackages and also enable a few workarounds as well as Steam controller support or other Steam supported controllers such as the DualShock 4 or Nintendo Switch Pro Controller.\n\nTroubleshooting \n\nSteam fails to start. What do I do?\n\nTry to run\n\nstrace steam\n\n\nto see what is causing steam to fail.\n\nUsing the FOSS Radeon or nouveau (nvidia) drivers\n\nThe newStdcpp parameter was removed since NixOS 17.09 and should not be needed anymore.\n\nSteam ships statically linked with a version of libcrypto that conflicts with the one dynamically loaded by radeonsi_dri.so. If you get the error:\n\nsteam.sh: line 713: 7842 Segmentation fault (core dumped)\n\n\nhave a look at this pull request.\n\nJava\n\nThere is no java in steam chrootenv by default. If you get a message like:\n\n/home/foo/.local/share/Steam/SteamApps/common/towns/towns.sh: line 1: java: command not found\n\n\nyou need to add:\n\nsteam.override { withJava = true; };\n\nsteam-run \n\nThe FHS-compatible chroot used for Steam can also be used to run other Linux games that expect a FHS environment. To use it, install the steam-run package and run the game with:\n\nsteam-run ./foo\n\nCataclysm: Dark Days Ahead \nHow to install Cataclysm DDA\nImportant note for overriding packages\nCustomizing with mods\nHow to install Cataclysm DDA \n\nTo install the latest stable release of Cataclysm DDA to your profile, execute nix-env -f \"<nixpkgs>\" -iA cataclysm-dda. For the curses build (build without tiles), install cataclysmDDA.stable.curses. Note: cataclysm-dda is an alias to cataclysmDDA.stable.tiles.\n\nIf you like access to a development build of your favorite git revision, override cataclysm-dda-git (or cataclysmDDA.git.curses if you like curses build):\n\ncataclysm-dda-git.override {\n  version = \"YYYY-MM-DD\";\n  rev = \"YOUR_FAVORITE_REVISION\";\n  sha256 = \"CHECKSUM_OF_THE_REVISION\";\n}\n\n\nThe sha256 checksum can be obtained by\n\nnix-prefetch-url --unpack \"https://github.com/CleverRaven/Cataclysm-DDA/archive/${YOUR_FAVORITE_REVISION}.tar.gz\"\n\n\nThe default configuration directory is ~/.cataclysm-dda. If you prefer $XDG_CONFIG_HOME/cataclysm-dda, override the derivation:\n\ncataclysm-dda.override {\n  useXdgDir = true;\n}\n\nImportant note for overriding packages \n\nAfter applying overrideAttrs, you need to fix passthru.pkgs and passthru.withMods attributes either manually or by using attachPkgs:\n\nlet\n  # You enabled parallel building.\n  myCDDA = cataclysm-dda-git.overrideAttrs (_: {\n    enableParallelBuilding = true;\n  });\n\n  # Unfortunately, this refers to the package before overriding and\n  # parallel building is still disabled.\n  badExample = myCDDA.withMods (_: []);\n\n  inherit (cataclysmDDA) attachPkgs pkgs wrapCDDA;\n\n  # You can fix it by hand\n  goodExample1 = myCDDA.overrideAttrs (old: {\n    passthru = old.passthru // {\n      pkgs = pkgs.override { build = goodExample1; };\n      withMods = wrapCDDA goodExample1;\n    };\n  });\n\n  # or by using a helper function `attachPkgs`.\n  goodExample2 = attachPkgs pkgs myCDDA;\nin\n\n# badExample                     # parallel building disabled\n# goodExample1.withMods (_: [])  # parallel building enabled\ngoodExample2.withMods (_: [])    # parallel building enabled\n\nCustomizing with mods \n\nTo install Cataclysm DDA with mods of your choice, you can use withMods attribute:\n\ncataclysm-dda.withMods (mods: with mods; [\n  tileset.UndeadPeople\n])\n\n\nAll mods, soundpacks, and tilesets available in nixpkgs are found in cataclysmDDA.pkgs.\n\nHere is an example to modify existing mods and/or add more mods not available in nixpkgs:\n\nlet\n  customMods = self: super: lib.recursiveUpdate super {\n    # Modify existing mod\n    tileset.UndeadPeople = super.tileset.UndeadPeople.overrideAttrs (old: {\n      # If you like to apply a patch to the tileset for example\n      patches = [ ./path/to/your.patch ];\n    });\n\n    # Add another mod\n    mod.Awesome = cataclysmDDA.buildMod {\n      modName = \"Awesome\";\n      version = \"0.x\";\n      src = fetchFromGitHub {\n        owner = \"Someone\";\n        repo = \"AwesomeMod\";\n        rev = \"...\";\n        hash = \"...\";\n      };\n      # Path to be installed in the unpacked source (default: \".\")\n      modRoot = \"contents/under/this/path/will/be/installed\";\n    };\n\n    # Add another soundpack\n    soundpack.Fantastic = cataclysmDDA.buildSoundPack {\n      # ditto\n    };\n\n    # Add another tileset\n    tileset.SuperDuper = cataclysmDDA.buildTileSet {\n      # ditto\n    };\n  };\nin\ncataclysm-dda.withMods (mods: with mods.extend customMods; [\n  tileset.UndeadPeople\n  mod.Awesome\n  soundpack.Fantastic\n  tileset.SuperDuper\n])\n\nUrxvt \nConfiguring urxvt\nPackaging urxvt plugins\n\nUrxvt, also known as rxvt-unicode, is a highly customizable terminal emulator.\n\nConfiguring urxvt \n\nIn nixpkgs, urxvt is provided by the package rxvt-unicode. It can be configured to include your choice of plugins, reducing its closure size from the default configuration which includes all available plugins. To make use of this functionality, use an overlay or directly install an expression that overrides its configuration, such as:\n\nrxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    plugins = with availablePlugins; [ perls resize-font vtwheel ];\n  };\n}\n\n\nIf the configure function returns an attrset without the plugins attribute, availablePlugins will be used automatically.\n\nIn order to add plugins but also keep all default plugins installed, it is possible to use the following method:\n\nrxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    plugins = (builtins.attrValues availablePlugins) ++ [ custom-plugin ];\n  };\n}\n\n\nTo get a list of all the plugins available, open the Nix REPL and run\n\n$ nix repl\n:l <nixpkgs>\nmap (p: p.name) pkgs.rxvt-unicode.plugins\n\n\nAlternatively, if your shell is bash or zsh and have completion enabled, type nixpkgs.rxvt-unicode.plugins.<tab>.\n\nIn addition to plugins the options extraDeps and perlDeps can be used to install extra packages. extraDeps can be used, for example, to provide xsel (a clipboard manager) to the clipboard plugin, without installing it globally:\n\nrxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    pluginsDeps = [ xsel ];\n  };\n}\n\n\nperlDeps is a handy way to provide Perl packages to your custom plugins (in $HOME/.urxvt/ext). For example, if you need AnyEvent you can do:\n\nrxvt-unicode.override {\n  configure = { availablePlugins, ... }: {\n    perlDeps = with perlPackages; [ AnyEvent ];\n  };\n}\n\nPackaging urxvt plugins \n\nUrxvt plugins resides in pkgs/applications/misc/rxvt-unicode-plugins. To add a new plugin, create an expression in a subdirectory and add the package to the set in pkgs/applications/misc/rxvt-unicode-plugins/default.nix.\n\nA plugin can be any kind of derivation, the only requirement is that it should always install perl scripts in $out/lib/urxvt/perl. Look for existing plugins for examples.\n\nIf the plugin is itself a Perl package that needs to be imported from other plugins or scripts, add the following passthrough:\n\npassthru.perlPackages = [ \"self\" ];\n\n\nThis will make the urxvt wrapper pick up the dependency and set up the Perl path accordingly.\n\nWeeChat \n\nWeeChat can be configured to include your choice of plugins, reducing its closure size from the default configuration which includes all available plugins. To make use of this functionality, install an expression that overrides its configuration, such as:\n\nweechat.override {configure = {availablePlugins, ...}: {\n    plugins = with availablePlugins; [ python perl ];\n  }\n}\n\n\nIf the configure function returns an attrset without the plugins attribute, availablePlugins will be used automatically.\n\nThe plugins currently available are python, perl, ruby, guile, tcl and lua.\n\nThe Python and Perl plugins allows the addition of extra libraries. For instance, the inotify.py script in weechat-scripts requires D-Bus or libnotify, and the fish.py script requires pycrypto. To use these scripts, use the plugin’s withPackages attribute:\n\nweechat.override { configure = {availablePlugins, ...}: {\n    plugins = with availablePlugins; [\n            (python.withPackages (ps: with ps; [ pycrypto python-dbus ]))\n        ];\n    };\n}\n\n\nIn order to also keep all default plugins installed, it is possible to use the following method:\n\nweechat.override { configure = { availablePlugins, ... }: {\n  plugins = builtins.attrValues (availablePlugins // {\n    python = availablePlugins.python.withPackages (ps: with ps; [ pycrypto python-dbus ]);\n  });\n}; }\n\n\nWeeChat allows to set defaults on startup using the --run-command. The configure method can be used to pass commands to the program:\n\nweechat.override {\n  configure = { availablePlugins, ... }: {\n    init = ''\n      /set foo bar\n      /server add libera irc.libera.chat\n    '';\n  };\n}\n\n\nFurther values can be added to the list of commands when running weechat --run-command \"your-commands\".\n\nAdditionally, it’s possible to specify scripts to be loaded when starting weechat. These will be loaded before the commands from init:\n\nweechat.override {\n  configure = { availablePlugins, ... }: {\n    scripts = with pkgs.weechatScripts; [\n      weechat-xmpp weechat-matrix-bridge wee-slack\n    ];\n    init = ''\n      /set plugins.var.python.jabber.key \"val\"\n    '':\n  };\n}\n\n\nIn nixpkgs there’s a subpackage which contains derivations for WeeChat scripts. Such derivations expect a passthru.scripts attribute, which contains a list of all scripts inside the store path. Furthermore, all scripts have to live in $out/share. An exemplary derivation looks like this:\n\n{ stdenv, fetchurl }:\n\nstdenv.mkDerivation {\n  name = \"exemplary-weechat-script\";\n  src = fetchurl {\n    url = \"https://scripts.tld/your-scripts.tar.gz\";\n    hash = \"...\";\n  };\n  passthru.scripts = [ \"foo.py\" \"bar.lua\" ];\n  installPhase = ''\n    mkdir $out/share\n    cp foo.py $out/share\n    cp bar.lua $out/share\n  '';\n}\n\nX.org \nKatamari Tarballs\nIndividual Tarballs\nGenerating Nix Expressions\nOverriding the Generator\n\nThe Nix expressions for the X.org packages reside in pkgs/servers/x11/xorg/default.nix. This file is automatically generated from lists of tarballs in an X.org release. As such it should not be modified directly; rather, you should modify the lists, the generator script or the file pkgs/servers/x11/xorg/overrides.nix, in which you can override or add to the derivations produced by the generator.\n\nKatamari Tarballs \n\nX.org upstream releases used to include katamari releases, which included a holistic recommended version for each tarball, up until 7.7. To create a list of tarballs in a katamari release:\n\nexport release=\"X11R7.7\"\nexport url=\"mirror://xorg/$release/src/everything/\"\ncat $(PRINT_PATH=1 nix-prefetch-url $url | tail -n 1) \\\n  | perl -e 'while (<>) { if (/(href|HREF)=\"([^\"]*.bz2)\"/) { print \"$ENV{'url'}$2\\n\"; }; }' \\\n  | sort > \"tarballs-$release.list\"\n\nIndividual Tarballs \n\nThe upstream release process for X11R7.8 does not include a planned katamari. Instead, each component of X.org is released as its own tarball. We maintain pkgs/servers/x11/xorg/tarballs.list as a list of tarballs for each individual package. This list includes X.org core libraries and protocol descriptions, extra newer X11 interface libraries, like xorg.libxcb, and classic utilities which are largely unused but still available if needed, like xorg.imake.\n\nGenerating Nix Expressions \n\nThe generator is invoked as follows:\n\ncd pkgs/servers/x11/xorg\n<tarballs.list perl ./generate-expr-from-tarballs.pl\n\n\nFor each of the tarballs in the .list files, the script downloads it, unpacks it, and searches its configure.ac and *.pc.in files for dependencies. This information is used to generate default.nix. The generator caches downloaded tarballs between runs. Pay close attention to the NOT FOUND: $NAME messages at the end of the run, since they may indicate missing dependencies. (Some might be optional dependencies, however.)\n\nOverriding the Generator \n\nIf the expression for a package requires derivation attributes that the generator cannot figure out automatically (say, patches or a postInstall hook), you should modify pkgs/servers/x11/xorg/overrides.nix.\n\nDevelopment of Nixpkgs \n\nThis section shows you how Nixpkgs is being developed and how you can interact with the contributors and the latest updates. If you are interested in contributing yourself, see CONTRIBUTING.md.\n\nTable of Contents\n\nOpening issues\nOpening issues \n\nMake sure you have a GitHub account\n\nMake sure there is no open issue on the topic\n\nSubmit a new issue by choosing the kind of topic and fill out the template\n\nContributing to Nixpkgs \n\nTable of Contents\n\nQuick Start to Adding a Package\nCoding conventions\nSubmitting changes\nVulnerability Roundup\nReviewing contributions\nContributing to Nixpkgs documentation\nQuick Start to Adding a Package \n\nThis section has been moved to pkgs/README.md.\n\nCoding conventions \n\nTable of Contents\n\nSyntax\nPackage naming\nFile naming and organisation\nFetching Sources\nObtaining source hash\nPatches\nPackage tests\n\nThis section has been moved to CONTRIBUTING.md.\n\nSyntax \n\nThis section has been moved to CONTRIBUTING.md.\n\nPackage naming \n\nThis section has been moved to pkgs/README.md.\n\nFile naming and organisation \nVersioning\n\nThis section has been moved to CONTRIBUTING.md.\n\nVersioning \n\nThis section has been moved to pkgs/README.md.\n\nFetching Sources \n\nThis section has been moved to pkgs/README.md.\n\nObtaining source hash \nObtaining hashes securely\n\nThis section has been moved to pkgs/README.md.\n\nObtaining hashes securely \n\nThis section has been moved to pkgs/README.md.\n\nPatches \n\nThis section has been moved to pkgs/README.md.\n\nPackage tests \nWriting inline package tests\nWriting larger package tests\nRunning package tests\nExamples of package tests\nLinking NixOS module tests to a package\nImport From Derivation\n\nThis section has been moved to pkgs/README.md.\n\nWriting inline package tests \n\nThis section has been moved to pkgs/README.md.\n\nWriting larger package tests \n\nThis section has been moved to pkgs/README.md.\n\nRunning package tests \n\nThis section has been moved to pkgs/README.md.\n\nExamples of package tests \n\nThis section has been moved to pkgs/README.md.\n\nLinking NixOS module tests to a package \n\nThis section has been moved to pkgs/README.md.\n\nImport From Derivation \n\nThis section has been moved to pkgs/README.md.\n\nSubmitting changes \n\nTable of Contents\n\nSubmitting changes\nSubmitting security fixes\nDeprecating/removing packages\nPull Request Template\nHotfixing pull requests\nCommit policy\n\nThis section has been moved to CONTRIBUTING.md.\n\nSubmitting changes \n\nThis section has been moved to CONTRIBUTING.md.\n\nSubmitting security fixes \n\nThis section has been moved to pkgs/README.md.\n\nDeprecating/removing packages \nSteps to remove a package from Nixpkgs\n\nThis section has been moved to pkgs/README.md.\n\nSteps to remove a package from Nixpkgs \n\nThis section has been moved to pkgs/README.md.\n\nPull Request Template \nTested using sandboxing\nBuilt on platform(s)\nTested via one or more NixOS test(s) if existing and applicable for the change (look inside nixos/tests)\nTested compilation of all pkgs that depend on this change using nixpkgs-review\nTested execution of all binary files (usually in ./result/bin/)\nMeets Nixpkgs contribution standards\n\nThis section has been moved to CONTRIBUTING.md.\n\nTested using sandboxing \n\nThis section has been moved to CONTRIBUTING.md.\n\nBuilt on platform(s) \n\nThis section has been moved to CONTRIBUTING.md.\n\nTested via one or more NixOS test(s) if existing and applicable for the change (look inside nixos/tests) \n\nThis section has been moved to CONTRIBUTING.md.\n\nTested compilation of all pkgs that depend on this change using nixpkgs-review \n\nThis section has been moved to CONTRIBUTING.md.\n\nTested execution of all binary files (usually in ./result/bin/) \n\nThis section has been moved to CONTRIBUTING.md.\n\nMeets Nixpkgs contribution standards \n\nThis section has been moved to CONTRIBUTING.md.\n\nHotfixing pull requests \n\nThis section has been moved to CONTRIBUTING.md.\n\nCommit policy \nBranches\n\nThis section has been moved to CONTRIBUTING.md.\n\nBranches \n\nThis section has been moved to CONTRIBUTING.md.\n\nMaster branch\n\nThis section has been moved to CONTRIBUTING.md.\n\nStaging branch\n\nThis section has been moved to CONTRIBUTING.md.\n\nStaging-next branch\n\nThis section has been moved to CONTRIBUTING.md.\n\nStable release branches\n\nThis section has been moved to CONTRIBUTING.md.\n\nAutomatically backporting a Pull Request\n\nThis section has been moved to CONTRIBUTING.md.\n\nManually backporting changes\n\nThis section has been moved to CONTRIBUTING.md.\n\nAcceptable backport criteria\n\nThis section has been moved to CONTRIBUTING.md.\n\nVulnerability Roundup \n\nTable of Contents\n\nIssues\nTriaging and Fixing\n\nThis section has been moved to pkgs/README.md.\n\nIssues \n\nThis section has been moved to pkgs/README.md.\n\nTriaging and Fixing \n\nThis section has been moved to pkgs/README.md.\n\nReviewing contributions \n\nTable of Contents\n\nPackage updates\nNew packages\nModule updates\nNew modules\nIndividual maintainer list\nMaintainer teams\nOther submissions\nMerging pull requests\n\nThis section has been moved to CONTRIBUTING.md.\n\nPackage updates \n\nThis section has been moved to pkgs/README.md.\n\nNew packages \n\nThis section has been moved to pkgs/README.md.\n\nModule updates \n\nThis section has been moved to nixos/README.md.\n\nNew modules \n\nThis section has been moved to nixos/README.md.\n\nIndividual maintainer list \n\nThis section has been moved to maintainers/README.md.\n\nMaintainer teams \n\nThis section has been moved to maintainers/README.md.\n\nOther submissions \n\nThis section has been moved to CONTRIBUTING.md.\n\nMerging pull requests \n\nThis section has been moved to CONTRIBUTING.md.\n\nContributing to Nixpkgs documentation \n\nTable of Contents\n\ndevmode\nSyntax\n\nThis section has been moved to doc/README.md.\n\ndevmode \n\nThis section has been moved to doc/README.md.\n\nSyntax \n\nThis section has been moved to doc/README.md."
  }
]